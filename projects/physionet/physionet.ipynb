{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "physionet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swWt6ngbZbAQ",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "PhysioNet provides access to an open dataset of physiological signals, and each year they host a challenge. Challenges in previous years include detecting sleep apnea using ECG, predicting hypotensive episodes and classifying heart sounds. For a full list, see https://physionet.org/challenge/. This year's challenge is to predict sepsis before it occurs based on demographic information, vital signs and lab reports. You will be attempting a simplified version of this project, where we have made some of the design decisions for you.\n",
        "\n",
        "## What to Submit\n",
        "\n",
        "Please go through the notebook and complete any code where asked (marked `# TODO: `). Make sure you can answer the questions in sections __Machine Learning 1: Basics__, __Machine Learning 2: Better cross validation__, and __Machine Learning 3: Adding in time__. If you like, you can open a new text cell and write your answers there (please keep them short, 1-2 sentences is plenty!).\n",
        "\n",
        "Finally, once you are sure everything works, run all the cells in the notebook (on Google Colaboratory, select `Runtime > Run all`, in Jupyter Lab, select `Run > Run All Cells`). Please save and submit the resulting `.ipynb` file (on Google Colaboratory, select `File > Download .ipynb`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm2-b5b4ZkbK",
        "colab_type": "text"
      },
      "source": [
        "# Step 0: Background concepts\n",
        "\n",
        "This assignment requires you to use machine learning models and methods. This is a lot of background material here, and this assignment will barely scratch the surface. I would highly recommend going through Roger Grosse’s course notes for the CSC321 course. [http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/).\n",
        "At the very least, you should go over the following lectures and try to understand the listed concepts.\n",
        "- [CSC411 Intro: What is machine learning, history of machine learning](www.cs.toronto.edu/~rgrosse/courses/csc411_f18/slides/lec01-slides.pdf)\n",
        "- [CSC321 Lecture 2 Slides: Fitting polynomials (useful visualization), Generalization](www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L02%20Linear%20Regression.pdf)\n",
        "- [CSC321 Lecture 2 Notes: Generalization (very important concept)](www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L02%20Linear%20Regression.pdf)\n",
        "- [CSC321 Lecture 3 Notes: Section 1, intro paragraph of 2.](www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L03%20Linear%20Classifiers.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT6kGuNEZ9Eo",
        "colab_type": "text"
      },
      "source": [
        "# Step 1: Familiarize yourself with the challenge\n",
        "\n",
        "Read the details of the challenge at https://physionet.org/challenge/2019/. Pay particular attention to what data are available in the dataset and try to build a good understanding of what the objective of the challenge is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcknqoGYaEkV",
        "colab_type": "text"
      },
      "source": [
        "# Step 2: Download required files\n",
        "\n",
        "Download `training_setA.zip` from the PhysioNet 2019 challenge website and extract it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH3gIwV-aJpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -nc https://archive.physionet.org/users/shared/challenge-2019/training_setA.zip\n",
        "!unzip -n training_setA.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yrJD8iiaLww",
        "colab_type": "text"
      },
      "source": [
        "You may also need to install some python packages by running the following cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SluQMiBvaNlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scikit-learn numpy pandas cache-em-all imbalanced-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rOsYfZOaUUS",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn, NumPy and Pandas are some of the most used python packages for data science. [Scikit-learn](https://scikit-learn.org/stable/index.html) is a tool for data mining, data analysis and machine learning. [NumPy](https://scikit-learn.org/stable/index.html) provides a fast implementation of arrays, matrices and common math operations. [Pandas](https://pandas.pydata.org/) is an extremely useful library for working with data, especially when your data can be organized into tables (e.g. a csv or excel file). Getting familiar with these packages would be a good idea for both this assignment and if you want to do any kind of data analysis in Python. [Imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn) contains functions to help you work with imbalanced data.\n",
        "\n",
        "The final package, [cache-em-all](https://pypi.org/project/cache-em-all/), allows for saving the result of a function. In this assignment, some functions can take 5 or more minutes to run. So, cache-em-all allows you to save the result of the function so that it only takes 5 minutes the first time the function is called. Whenever the function is called again, it only takes a second or two to run. Full disclosure, cache-em-all was created by one of the TAs.\n",
        "\n",
        "The following cell just imports the required modules and defines some helper function we wrote for you, and you do not need to do anything besides run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTDD1_yCGerU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from cache_em_all import Cachable\n",
        "\n",
        "import os\n",
        "\n",
        "DATA_DIR = \"training\" # Path to the data\n",
        "\n",
        "# Names of all columns in the data that contain physiological data\n",
        "physiological_cols = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2',\n",
        "       'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST', 'BUN',\n",
        "       'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct',\n",
        "       'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium',\n",
        "       'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC',\n",
        "       'Fibrinogen', 'Platelets']\n",
        "\n",
        "# Names of all columns in the data that contain demographic data\n",
        "demographic_cols = ['Age', 'Gender', 'Unit1', 'Unit2', 'HospAdmTime', 'ICULOS']\n",
        "\n",
        "# The combination of physiological and demographic data is what we will use as features in our model\n",
        "feature_cols = physiological_cols + demographic_cols\n",
        "\n",
        "# The name of the column that contains the value we are trying to predic\n",
        "label_col = \"SepsisLabel\"\n",
        "\n",
        "# Pre-calculated means and standard deviation of all physiological and demographic columns. We will use this to normalize\n",
        "# data using their z-score. This isn't as important for simpler models such as random forests and decision trees,\n",
        "# but can result in significant improvements when using neural networks\n",
        "physiological_mean = np.array([\n",
        "        83.8996, 97.0520,  36.8055,  126.2240, 86.2907,\n",
        "        66.2070, 18.7280,  33.7373,  -3.1923,  22.5352,\n",
        "        0.4597,  7.3889,   39.5049,  96.8883,  103.4265,\n",
        "        22.4952, 87.5214,  7.7210,   106.1982, 1.5961,\n",
        "        0.6943,  131.5327, 2.0262,   2.0509,   3.5130,\n",
        "        4.0541,  1.3423,   5.2734,   32.1134,  10.5383,\n",
        "        38.9974, 10.5585,  286.5404, 198.6777])\n",
        "physiological_std = np.array([\n",
        "        17.6494, 3.0163,  0.6895,   24.2988, 16.6459,\n",
        "        14.0771, 4.7035,  11.0158,  3.7845,  3.1567,\n",
        "        6.2684,  0.0710,  9.1087,   3.3971,  430.3638,\n",
        "        19.0690, 81.7152, 2.3992,   4.9761,  2.0648,\n",
        "        1.9926,  45.4816, 1.6008,   0.3793,  1.3092,\n",
        "        0.5844,  2.5511,  20.4142,  6.4362,  2.2302,\n",
        "        29.8928, 7.0606,  137.3886, 96.8997])\n",
        "demographic_mean = np.array([60.8711, 0.5435, 0.0615, 0.0727, -59.6769, 28.4551])\n",
        "demographic_std = np.array([16.1887, 0.4981, 0.7968, 0.8029, 160.8846, 29.5367])\n",
        "\n",
        "@Cachable(\"flattened.csv\")\n",
        "def flatten(in_df, hours=4):\n",
        "    res = []\n",
        "\n",
        "    new_cols = []\n",
        "    for i in range(hours):\n",
        "        new_cols.append([c + \"_\" + str(i) for c in feature_cols])\n",
        "\n",
        "\n",
        "    df = in_df.sort_values(\"hour\")\n",
        "    for patient, _df in df.groupby(\"patient\"):\n",
        "        n = int(len(_df) / hours)\n",
        "\n",
        "        for i in range(n):\n",
        "            window = _df.iloc[i*hours:(i+1)*hours]\n",
        "            window_dict = {}\n",
        "\n",
        "            for j in range(hours):\n",
        "                for c in physiological_cols:\n",
        "                    window_dict[c + \"_\" + str(j)] = window[c].iloc[j]\n",
        "\n",
        "            for c in demographic_cols:\n",
        "                window_dict[c] = window[c].iloc[0]\n",
        "\n",
        "            window_dict[label_col] = window[label_col].mean()\n",
        "            window_dict[\"patient\"] = patient\n",
        "\n",
        "            res.append(window_dict)\n",
        "\n",
        "    res = pd.DataFrame(res)\n",
        "\n",
        "    res = res[res[label_col] <= 1 / hours]\n",
        "    res[label_col] = res[label_col].apply(lambda x: 1 if x else 0)\n",
        "\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JqQs1EObfOb",
        "colab_type": "text"
      },
      "source": [
        "# Helper function: load_single_file\n",
        "\n",
        "First, let's create a helper function that will load a single file from the dataset. If you look at the data in the training folder, you will see files like `\"p000001.psv\"`. This is a pipe separated file, similar to a comma-separated file, except each column is separated by a `\"|\"` instead of a comma. This helper function will take as input the path to one of these files (e.g. `\"training.p000001.psv\"`), use pandas to load the contents of the file, append some extra data and return the result.\n",
        "\n",
        "First, you should look up the pandas `read_csv` function. It is a very simple way to read a csv file. For example,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gHeaoT6bkza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"training/p000001.psv\", sep=\"|\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksFs6JObcZd7",
        "colab_type": "text"
      },
      "source": [
        "Will read `\"training/p000001.psv\"` and load it into the variable df. Pandas uses what's called a `DataFrame`, which you can think of as a table, to represent data. If you want to see what a `DataFrame` looks like, try running the above code in your notebook and printing `df`.\n",
        "\n",
        "This helper function also adds some extra information to the dataframe that we will be using\n",
        "later on. With a `DataFrame`, you can access a specific column using the square brackets (similar to a `dict`). So for example, if you read in one of the training files, you can run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOW0SldScdLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"HR\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6P2CulFclFR",
        "colab_type": "text"
      },
      "source": [
        "To view just the heart rate’s for this file. You can also use this to assign values to a column. So for example,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk8AzDelcoO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"Hbg\"] = df[\"Hgb\"] * 10\n",
        "df[\"feeling\"] = \"Happy\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JtknFhmcxSN",
        "colab_type": "text"
      },
      "source": [
        "Line 1 will multiply all hemoglobin values by 10 (convert from g/dL to g/L). Line 2 will create a new column called `\"feeling\"` and fill all rows with the value `\"happy\"`.\n",
        "\n",
        "Now back to the helper function. Once you've read in the file, let's create a new column called `\"patient\"` that contains the name of the file this data came from. We will use the file name as a sort of patient ID to figure out later on which data belongs to which patient. Secondly, let's also create a column called `\"hour\"`. If you remember from the PhysioNet data description, each row in the file represents an hour. This is fine for when our data is saved in a file, however as we're working with the data, we want to shuffle rows around a bit. Therefore, we would like the explicitly store the hour in a separate column. This can be achieved with the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JtPZCMqczhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"hour\"] = df.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtKCqkIhc4K3",
        "colab_type": "text"
      },
      "source": [
        "In Pandas, the index is a very important concept. However, it can also get somewhat complicated. Essentially, the index is used to identify and look up rows in the data frame. When we read in the data using the read_csv function, pandas created an index for us where the index value of each row was its row number in the csv file (i.e. the first row had an index value of 0, the second row had an index value of 1, etc.). So now, when we say assign the value of the index to the `“hour”` column, we are setting the hour to also be the row number. So by now, your load_single_file function should read the csv into a `DataFrame` and create new columns for `“patient”` and `“hour”`. You can now return this `DataFrame`.\n",
        "\n",
        "With this background, you should now be able to implement `load_single_file` below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnRsq2xnGer9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_single_file(file_path):\n",
        "    # TODO: Implement helper function: load_single_file\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwYUi4EIjTic",
        "colab_type": "text"
      },
      "source": [
        "# Helper function: load_data\n",
        "\n",
        "Now let's work on another helper function called `load_data`. The first one read in a single file and created a `DataFrame`, now this helper function will read in all data files and put them into a single data frame.\n",
        "\n",
        "In pandas, adding rows to a `DataFrame` is possible. However, it is very slow. Instead, what we will do is read each file in our dataset and append it to a list and then use Pandas `concat` function to concatenate the list of `DataFrame` into a single `DataFrame`. You should use the `load_single_file` helper function you wrote earlier.\n",
        "\n",
        "Then we will use the Pandas `concat` function to concatenate the list of `DataFrame`s into a single `DataFrame` (Note: only do this when you know that each `DataFrame` has the same columns. With this physionet data, we do know this to be the case).\n",
        "\n",
        "To help you with this, we have provided a function called `get_data_files` that will give you a list of all the files in the dataset. Once you have this list, you can iterate over it and use your `load_single_file` function to load each file. Append the result of this to a list, and after the for loop, use the `concat`.\n",
        "\n",
        "Once you have the concatenated `DataFrame`, use the following code to clean up the data. This\n",
        "calls a function we provided that normalizes data removes `NaN`s and cleans up the index a bit.\n",
        "\n",
        "```\n",
        "df = clean_data(df)\n",
        "```\n",
        "\n",
        "At the end of the load_data function, return the concatenated and cleaned `DataFrame`.\n",
        "\n",
        "Once you are sure this function is working correctly, you can un-comment the `@Cachable(\"data.csv\")` line above the function. The next time you run the function, it's result will be saved. Subsequent calls to the function will load the saved data. If you realize there was a mistake in the function implementation and you have to fix it, you will also need to delete the `\"cache\"` folder. Otherwise, your fixed code will not be called, and instead, the old saved data will be returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB0COxqnGesE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_files():\n",
        "    return [os.path.join(DATA_DIR, x) for x in sorted(os.listdir(DATA_DIR)) if int(x[1:-4]) % 5 > 0]\n",
        "\n",
        "def clean_data(data):\n",
        "    data.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # Normalizes physiological and demographic data using z-score.\n",
        "    data[physiological_cols] = (data[physiological_cols] - physiological_mean) / physiological_std\n",
        "    data[demographic_cols] = (data[demographic_cols] - demographic_mean) / demographic_std\n",
        "\n",
        "    # Maps invalid numbers (NaN, inf, -inf) to numbers (0, really large number, really small number)\n",
        "    data[feature_cols] = np.nan_to_num(data[feature_cols])\n",
        "\n",
        "    return data\n",
        "\n",
        "# @Cachable(\"data.csv\")\n",
        "def load_data():\n",
        "    # TODO: Implement helper function: load_data\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsuBjukFkgsH",
        "colab_type": "text"
      },
      "source": [
        "# Data exploration\n",
        "\n",
        "Use the helper functions you implemented to load the data. Try to understand what data is\n",
        "available. What are the different columns in the dataframe? How many rows? How many rows with where `\"SepsisLabel\"` is 0, how many where it is 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GbE8cmwkoum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = load_data()\n",
        "print(df.columns)\n",
        "print(len(df))\n",
        "print(df[\"SepsisLabel\"].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMKPmPgPlDj_",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning 1: Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6cFrftykqlJ",
        "colab_type": "text"
      },
      "source": [
        "In this section, we implement the function `train_simple`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g5dNnqVfY00",
        "colab_type": "text"
      },
      "source": [
        "Now that we can load our data, lets try some machine learning. Because we are working with data that has labels (i.e. we know which rows are associated with sepsis), we will be using a type of learning called _supervised learning_. More specifically, given an input X, we are trying to predict an output y and we have examples of X, y pairs that we can use to train our system. In contrast, with unsupervised learning we would only have X and we would be trying to learn something about X.\n",
        "\n",
        "Within supervised learning, there are two types of tasks: 1) classification and 2) regression. With classification, y is discrete (e.g. sepsis or not sepsis, picture is of cat, dog or bunny). With regression, y is continuous (e.g. white blood cell count, price of a house).\n",
        "\n",
        "Remember the concept of generalization? We want the model we train to be able to predict accurately on data it has not seen before. One way we can do this is by splitting our data into two sets. One set, which we call the train set, will be used to train the model. The other, called the test set, will only be used to evaluate the model.\n",
        "`Scikit-learn` provides a function that splits data into train and test. The test size is the proportion of data that should be used for the test set. In this case, 80% of the data is used for training, and 20% is used for testing. Here is the function you need to call in order to make this split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tBK0Tpsf_Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df , test_df = train_test_split(df , test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StALNPv1g4zJ",
        "colab_type": "text"
      },
      "source": [
        "Now, the `train_df` and `test_df` contain data that should be used for training and testing,\n",
        "respectively. However, they both contain both X and y values. So lets further split these\n",
        "into train_X, train_y, test_X, test_y. As a hint, remember that you can access specific\n",
        "columns of a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44F8s9RXg7Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df[\"col_a\"]\n",
        "\n",
        "col_list = [\"col_b\" , \"col_c\"]\n",
        "df2 = df[col_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed1C6q9phNDU",
        "colab_type": "text"
      },
      "source": [
        "This will create `df1` with just `col1` and `col2` from df. Also, for your convenience, we defined variables called `feature_cols` and `label_col` that contain the column names for X and y, respectively.\n",
        "\n",
        "Once you have X and y for both train and test sets, lets create a classifier. Here we are creating a decision tree classifier as an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb2toa9ihfCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = DecisionTreeClassifier(class_weight=\"balanced\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90qbapcqhkt-",
        "colab_type": "text"
      },
      "source": [
        "There are many different types of classifiers available in Scikit-learn. We have already imported\n",
        "some of them and you can experiment with which ones work well.\n",
        "After creating the classifier, we need to fit/train it on our data. Lets call the fit function with our training data.\n",
        "\n",
        "Once the model is fitted, we can evaluate how well it worked. To do this we can use the\n",
        "following code. What this does is it uses our model to predict what it thinks the y value should be for a given X. Then, using the models predicted y value and the actual y value, we\n",
        "compute evaluation accuracy, precision and recall using the provided helper function called\n",
        "evaluate.\n",
        "\n",
        "First, lets see how well the model performs on the train set. Remember, this is the same data that was used to fit/train the model. We can expect to get fairly high performance on this\n",
        "data. Next, lets see how well the model does on unseen data (i.e. is the model generalizing). This can be done similar to the code below, but just by using the test X and y values. If\n",
        "the model performs significantly worse on the test set than the train set, we can say that the model is overfitting to the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZdVegVqhtvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "evaluate(y_train, y_pred, \" Train\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqT2M9HTiw2p",
        "colab_type": "text"
      },
      "source": [
        "What happens if you run this code multiple times? Are the results consistent? Inconsistent\n",
        "results make it hard to replicate our work, therefore we would like a way to make our code\n",
        "produce the same result every time it is run. One way to do this is to set whats called a\n",
        "random seed.\n",
        "We can do this by adding the following lines right after our import statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2CL2tzci0iL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 9001\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vJno-_i7dB",
        "colab_type": "text"
      },
      "source": [
        "we also need to pass this seed to our the train test split function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kASsyn_ni-Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df , test_df = train_test_split(df ,test_size=0.2, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTD_nQwYjJF3",
        "colab_type": "text"
      },
      "source": [
        "In this case, I set a random seed of `9001`, but you can set it to whatever you like. But for a given seed, the program should produce the same result. So now your results should be\n",
        "consistent.\n",
        "\n",
        "Now, how does the model perform? Is the performance on the training set significantly higher than the test set? If so, this indicates overfitting. You can think of this as the model\n",
        "is memorizing the answers rather than learning the concepts. One way around this is to limit the complexity of the model. For example, we can create our model with the parameters\n",
        "shown below. This how deep and wide the decision tree can be, which effectively limits it’s ability to memorize answers. The selection of these parameters is also a complex task and\n",
        "you should be careful about how you go about selecting these parameters because you can introduce another form of overfitting. You can end up basically manually optimizing these\n",
        "parameters so that the model performs well on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etgX-4tHjOV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = DecisionTreeClassifier(class_weight=\" balanced \" ,max_depth=20, max_leaf_nodes=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff_CPgaAjTO0",
        "colab_type": "text"
      },
      "source": [
        "Do you remember how many rows we had where the SepsisLabel was 1 (positive examples)\n",
        "and how many there were with 0 (negative examples)? When this proportion is heavily\n",
        "skewed to one side, we call this an imbalance. With a strong imbalance, classifiers may not\n",
        "learn as well because there are too few examples of a particular class (e.g. 0 or 1, cat or dog or bunny). The class_weight=\"balance\" is one way to deal with a class imbalance. It tells the classifier to weigh classes relative to how many examples there are in the training data. For example, if there are twice as many negative examples than positive examples, the classifier will consider getting a positive examples wrong twice as bad as a negative examples.\n",
        "Another way to deal with class imbalance is undersampling. This can be achieved with the\n",
        "code shown below. You can read about it in more detail here (https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html). The ratio 0.5 tells the sampler\n",
        "to have a 1:1 ratio of positive and negative examples. A ratio of 0.1 would result in a 1:10\n",
        "ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS3CIeuBj3mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus = RandomUnderSampler(0.5 , random_state=seed )\n",
        "X_train , y_train = rus.fit_resample(X_train , y_train )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "levV-h5sparp"
      },
      "source": [
        "Now, use what you've learned above to implement the `train_simple` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wq5emY6kpgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(actual, predicted, prefix=\"\"):\n",
        "    precision = precision_score(actual, predicted)\n",
        "    recall = recall_score(actual, predicted)\n",
        "    accuracy = accuracy_score(actual, predicted)\n",
        "\n",
        "    print(\"%s Precision: %.3f%%, Recall: %.3f%%, Accuracy: %.3f%%\" % (prefix, precision * 100, recall * 100, accuracy * 100))\n",
        "\n",
        "def train_simple(data, feature_cols, label_col):\n",
        "    # TODO: Implement helper function: train_simple\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7FZFpbllII5",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning 2: Better cross validation\n",
        "\n",
        "In the first machine learning section, we talked about splitting data into train and test sets and how that helps with generalizability. We attempted to split data into independent train and test sets. However, it turns out our sets weren’t so independent. We were deciding whether each row should go into the training or testing set. However, multiple rows belong to a single patient. If we have data from patient _P_ in the training set, isn’t it kind of like cheating to have data from patient _P_ in the test set as well? What we would really like is if the model worked on patients that it had never seen before. To do this, we can use a `GroupKFold`. K-Fold is form of cross-validation and the simple train/test split we did earlier can be considered a special case of k-fold cross validation. You can read more [here](https://machinelearningmastery.com/k-fold-cross-validation/).\n",
        "\n",
        "With a `GroupKFold`, we ensure that each occurrence of the group variable occurs in only one of the train or test set. So if patient _P_ occurs is assigned to the train set, their data will not be in the test set and vice versa.\n",
        "\n",
        "We have implemented the skeleton for this `GroupKFold` cross validation in the function called `train_stratified`. Your job is to implement a classifier inside this function. Do not be surprised if the performance of your classifier drops. Not cheating on a test may result in a worse grade, but that grade is more reflective of how much you know. Experiment with different classifiers, parameters and sampling ratios to find what you think is the best classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfKfUvyzGesW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_stratified(data, feature_cols, label_col, stratify_col):\n",
        "    X = data[feature_cols]\n",
        "    y = data[label_col]\n",
        "    group = data[stratify_col]\n",
        "\n",
        "    train_pred = []\n",
        "    train_actual = []\n",
        "\n",
        "    test_pred = []\n",
        "    test_actual = []\n",
        "\n",
        "    kf = GroupKFold(n_splits=5)\n",
        "    for train_idx, test_idx in kf.split(X, y, group):\n",
        "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
        "\n",
        "        # TODO: Implement your classifier here\n",
        "\n",
        "        train_pred.extend(clf.predict(X_train))\n",
        "        train_actual.extend(y_train)\n",
        "\n",
        "        test_pred.extend(clf.predict(X_test))\n",
        "        test_actual.extend(y_test)\n",
        "\n",
        "    evaluate(train_actual, train_pred, \"Train\")\n",
        "    evaluate(test_actual, test_pred, \"Test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crXvo8MqlNNJ",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning 3: Adding in time\n",
        "\n",
        "So far, we have made prediction on individual rows (i.e. given data at time t, will this patient have sepsis at time t + 6). However, we would expect that if we have multiple time points of data, we can make better predictions. For example, having 4 hours of data will give us 4 data points for HR, Hgb, RR, etc. and if we can detect trends across these variables, we may be able to make better predictions. While there are more complex models out there that have a better notion of time baked in, one way to incorporate more data is to just flatten 4 hours of data into a single vector. So now instead of having HR, you would have HR_0, HR_1, HR_2 and HR_3. We have implemented a function that does exactly this transformation.\n",
        "\n",
        "You can call like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4D0uG8ll5P7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flat_df = flatten(df, hours=4)\n",
        "flattened_feat_cols = [x for x in flat_df.columns if x not in [label_col , \"patient\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3SGL17qmFo4",
        "colab_type": "text"
      },
      "source": [
        "Try experimenting with this flattened data. Does using multiple hours of data improve your prediction accuracy?"
      ]
    }
  ]
}