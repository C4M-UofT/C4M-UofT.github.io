{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["DekAnzq1b82b","xP4sP_ekZxqW","dF7vGVljaKp7","ryaahUysaOHD","gmMs1P1tabse","2ip_I8PZtMxy","TUuv3wRJaVer","A-AcODJ3aYx6","BLBy4qpPpQ6r","srXIpMFCpYxR","-fpM5JCVcS_c","hT0cnOFdafEF"],"authorship_tag":"ABX9TyMzjq+FdN12bSvgiR7dMzS5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this notebook, we're going to revisit the classification model we trained to distinguish between benign and malignant skin lesions in the hopes of improving its test accuracy. Most of the code in this notebook is copied from the previous session, but we will use the following techniques to boost the model's performance:\n","* Dataset balancing (SMOTE)\n","* Hyperparameter search for the random forest classifier"],"metadata":{"id":"BDaO9cuMWe83"}},{"cell_type":"markdown","source":["# Important: Run this code cell each time you start a new session!"],"metadata":{"id":"DekAnzq1b82b"}},{"cell_type":"code","source":["!pip install numpy\n","!pip install pandas\n","!pip install matplotlib\n","!pip install os\n","!pip install opencv-python\n","!pip install scikit-learn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import sklearn"],"metadata":{"id":"jrO0X1ZMxMN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget -Ncnp https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_Data.zip"],"metadata":{"id":"bASFcdZNZuGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -n ISBI2016_ISIC_Part3_Training_Data.zip"],"metadata":{"id":"2hP2UnPrcEdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget -Ncnp https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_GroundTruth.csv"],"metadata":{"id":"hIBYeAPogvAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget -Ncnp https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part1_Training_GroundTruth.zip"],"metadata":{"id":"b3tho-Kt9q_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -n ISBI2016_ISIC_Part1_Training_GroundTruth.zip"],"metadata":{"id":"Ebi3lsSm-poa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 1: Define the Problem You Are Trying to Solve"],"metadata":{"id":"xP4sP_ekZxqW"}},{"cell_type":"markdown","source":["As before, our overarching goal is to generate a binary classification model that predicts whether the skin lesion in a photograph is benign or malignant."],"metadata":{"id":"BBrNiJenan-A"}},{"cell_type":"code","source":["# The relevant folders and files associated with this dataset\n","# (we will talk about some of them later)\n","image_folder = 'ISBI2016_ISIC_Part3_Training_Data'\n","segmentation_folder = 'ISBI2016_ISIC_Part1_Training_GroundTruth'\n","label_filename = 'ISBI2016_ISIC_Part3_Training_GroundTruth.csv'"],"metadata":{"id":"tjO9wyZHMg9O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load two pre-selected image files to show what they look like\n","benign_filename = 'ISIC_0000000.jpg'\n","malignant_filename = 'ISIC_0000002.jpg'\n","benign_img = cv2.imread(os.path.join(image_folder, benign_filename))\n","benign_img = cv2.cvtColor(benign_img, cv2.COLOR_BGR2RGB)\n","malignant_img = cv2.imread(os.path.join(image_folder, malignant_filename))\n","malignant_img = cv2.cvtColor(malignant_img, cv2.COLOR_BGR2RGB)\n","\n","# Show the images and their labels\n","plt.figure(figsize=(6, 3))\n","plt.subplot(1, 2, 1), plt.imshow(benign_img), plt.title('Benign')\n","plt.subplot(1, 2, 2), plt.imshow(malignant_img), plt.title('Malignant')\n","plt.show()"],"metadata":{"id":"nlbjx-aIDbFe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 2: Create Your Features and Labels"],"metadata":{"id":"dF7vGVljaKp7"}},{"cell_type":"markdown","source":["We are going to keep our labels and features the same as before. All of the code from the previous session is copied below, so refer to that notebook if you need a reminder of how we came up with these code blocks."],"metadata":{"id":"1O2DKWI7lpOk"}},{"cell_type":"code","source":["# Helper function for extracting contour from annotation\n","def extract_contour(seg_img):\n","    \"\"\"\n","    Extracts the lone contour from the image annotation\n","    seg_img: a binary image representing an annotation\n","    \"\"\"\n","    cnts, hierarchy = cv2.findContours(seg_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    return cnts[0]"],"metadata":{"id":"TgSG8O2e-aY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","from scipy.spatial.distance import cdist\n","\n","def flip_point(point, line_point, angle_deg):\n","    \"\"\"\n","    A helper function that reflects a point over an arbitrary line\n","    This requires a lot of math, so don't worry if it's not intuitive\n","    point: the point we are trying to reflect\n","    line_point: a point that goes through the line\n","    angle_deg: the angle that defines the slope of the line\n","    \"\"\"\n","    # Convert the angle from degrees to radians\n","    angle_rad = math.radians(angle_deg)\n","\n","    # Calculate the coordinates of the point in a coordinate system where line_point is the origin\n","    translated_point = (point[0] - line_point[0], point[1] - line_point[1])\n","\n","    # Rotate this coordinate system by an angle of -angle_rad\n","    rotated_point = (translated_point[0]*math.cos(angle_rad) + translated_point[1]*math.sin(angle_rad),\n","                     -translated_point[0]*math.sin(angle_rad) + translated_point[1]*math.cos(angle_rad))\n","\n","    # Reflect the point over the x-axis\n","    reflected_point = (rotated_point[0], -rotated_point[1])\n","\n","    # Rotate the coordinate system back by an angle of angle_rad and translate back to original coordinate system\n","    reflected_translated_back = (reflected_point[0]*math.cos(-angle_rad) + reflected_point[1]*math.sin(-angle_rad),\n","                                 -reflected_point[0]*math.sin(-angle_rad) + reflected_point[1]*math.cos(-angle_rad))\n","    final_point = (reflected_translated_back[0] + line_point[0], reflected_translated_back[1] + line_point[1])\n","\n","    return final_point\n","\n","def flip_contour(contour, center, angle_deg):\n","    \"\"\"\n","    A helper function reflects an entire contour over its major axis\n","    contour: the contour\n","    center: the center of the contour's fit ellipse\n","    angle_deg: the angle of the contour's fit ellipse\n","    \"\"\"\n","    # Create an array to store the flipped contour points\n","    flipped_contour = []\n","\n","    # Flip each point in the contour\n","    for point in contour:\n","        flipped_point = flip_point(point[0], center, angle_deg)\n","        flipped_contour.append(flipped_point)\n","\n","    # Convert the flipped contour list to an array\n","    flipped_contour = np.array(flipped_contour, dtype=np.int32).reshape(-1, 1, 2)\n","\n","    return flipped_contour\n","\n","def get_hausdorff_distance(cnt1, cnt2):\n","    \"\"\"\n","    A helper function to compute the Haussdorf distance between two contours\n","    cnt2: the first contour\n","    cnt2: the second contour\n","    \"\"\"\n","    pts1 = np.array(cnt1).squeeze()\n","    pts2 = np.array(cnt2).squeeze()\n","    distances = cdist(pts1, pts2)\n","    return np.max(np.min(distances, axis=0))"],"metadata":{"id":"0im5cSDimPmL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_asymmetry(img, cnt):\n","    \"\"\"\n","    Compute the asymmetry of the skin lesion by comparing the contour with a\n","    reflected version of itself\n","    img: the image of the skin lesion\n","    cnt: the contour of the skin lesion\n","    \"\"\"\n","    # Get the min enclosing ellipse\n","    center, axes, angle = cv2.fitEllipse(cnt)\n","\n","    # Flip the contour of the ellipse's major axis\n","    flipped_cnt = flip_contour(cnt, center, -angle)\n","\n","    # Measure the difference between the two contours as the Haussdorff distance\n","    distance = get_hausdorff_distance(cnt, flipped_cnt)\n","\n","    # Scale the distance according to the diameter for fair comparison\n","    _, r = cv2.minEnclosingCircle(cnt)\n","    d = 2*r\n","    return distance / d"],"metadata":{"id":"FNZOOcXsdISL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_border(cnt):\n","    \"\"\"\n","    Compute the jaggedness of the skin lesion's border by comparing the\n","    perimeter of the actual border to the perimeter of the convex hull\n","    cnt: the contour of the skin lesion\n","    \"\"\"\n","    # Compute the perimeter\n","    perimeter = cv2.arcLength(cnt, True)\n","\n","    # Approximate the contour as a convex hull\n","    hull = cv2.convexHull(cnt)\n","\n","    # Compute the perimeter of the convex hull\n","    simplified_perimeter = cv2.arcLength(hull, True)\n","\n","    # Return the ratio between the two\n","    return simplified_perimeter / perimeter"],"metadata":{"id":"nt_TUVgzc4gp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_color(img, cnt):\n","    \"\"\"\n","    Compute the color standard deviation of the skin lesion within the contour\n","    img: the image of the skin lesion\n","    cnt: the contour of the skin lesion\n","    \"\"\"\n","    # Convert the image to HSV\n","    hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n","\n","    # Recreate the binary mask using the contour\n","    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n","    cv2.drawContours(mask, [cnt], -1, (255), thickness=-1)\n","\n","    # Apply the mask to the image\n","    masked_img = cv2.bitwise_and(img, img, mask=mask)\n","\n","    # Compute the variation in HSV color\n","    mean, stdev = cv2.meanStdDev(masked_img, mask=mask)\n","    return tuple(stdev.flatten())"],"metadata":{"id":"ZOp5ASdMCMUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_diameter(cnt):\n","    \"\"\"\n","    Compute the radius of the skin lesion according to the min enclosing circle\n","    cnt: the contour of the skin lesion\n","    \"\"\"\n","    _, r = cv2.minEnclosingCircle(cnt)\n","    return 2*r"],"metadata":{"id":"Bnz0KZtDCQ_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_img(filename):\n","    \"\"\"\n","    Process a skin lesion image and produce all of the features according to\n","    the ABCD(E) rule as a dictionary (one value per key)\n","    filename: the name of the skin lesion image without the file extension\n","    \"\"\"\n","    # Get the contour filename\n","    rgb_filename = filename + '.jpg'\n","    seg_filename = filename + '_Segmentation.png'\n","\n","    # Get both of the images (RGB and segmentation annotation)\n","    img = cv2.imread(os.path.join(image_folder, rgb_filename))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    seg_img = cv2.imread(os.path.join(segmentation_folder, seg_filename))\n","    seg_img = cv2.cvtColor(seg_img, cv2.COLOR_BGR2GRAY)\n","\n","    # Get the contour\n","    cnt = extract_contour(seg_img)\n","\n","    # Extract features from the image\n","    asymmetry = compute_asymmetry(img, cnt)\n","    border = compute_border(cnt)\n","    color = compute_color(img, cnt)\n","    diameter = compute_diameter(cnt)\n","\n","    # Combine everything into a feature vector\n","    feature_dict = {'Asymmetry': asymmetry,\n","                    'Border': border,\n","                    'Color Stdev (H)': color[0],\n","                    'Color Stdev (S)': color[1],\n","                    'Color Stdev (V)': color[2],\n","                    'Diameter': diameter}\n","    return feature_dict"],"metadata":{"id":"-6Egsxlfil5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get all the filenames but remove the extension\n","img_filenames = os.listdir(image_folder)\n","img_filenames = sorted([f[:-4] for f in img_filenames])\n","\n","# Iterate through the filenames\n","features_df = pd.DataFrame()\n","for img_filename in img_filenames:\n","    # Generate the features\n","    feature_dict = process_img(img_filename)\n","\n","    # Add the image name\n","    feature_dict['Image Name'] = img_filename\n","    feature_df = pd.DataFrame([feature_dict])\n","    features_df = pd.concat([features_df, feature_df], axis=0)\n","\n","# Set the index to the image name\n","features_df.set_index(['Image Name'], inplace=True)\n","features_df"],"metadata":{"id":"vyt2LUL7i1BC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate labels\n","labels_df = pd.read_csv(label_filename, header=None)\n","labels_df.rename(columns={0: 'Image Name', 1: 'Label'}, inplace=True)\n","labels_df.set_index(['Image Name'], inplace=True)\n","labels_df['Label'].replace({'benign': 0, 'malignant': 1}, inplace=True)\n","labels_df"],"metadata":{"id":"Xkr2TodFhmN8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = features_df.merge(labels_df, left_index=True, right_index=True)\n","df"],"metadata":{"id":"IGmw_RJsZXqg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 3: Decide How the Data Should Be Split for Training and Testing"],"metadata":{"id":"ryaahUysaOHD"}},{"cell_type":"markdown","source":["Similar to before, we are going to use an 80%-20% split to train and test our model. All of the code from the previous session is copied below, so refer to that notebook if you need a reminder of how we came up with these code blocks."],"metadata":{"id":"k6EWrUy4BBbp"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Set the random seed to an arbitrary number of your choosing\n","np.random.seed(42)\n","\n","# Split off 20% of the data for model testing\n","train_df, test_df = train_test_split(df, test_size=0.2)\n","\n","# Show the results\n","print(f'Number of samples in train data: {len(train_df)}')\n","print(f'Number of samples in test data: {len(test_df)}')"],"metadata":{"id":"dtSB2DFjA_FG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train = train_df.drop('Label', axis=1).values\n","y_train = train_df['Label'].values\n","x_test = test_df.drop('Label', axis=1).values\n","y_test = test_df['Label'].values"],"metadata":{"id":"rECmsVbb8305"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: (Optional) Add Feature Selection"],"metadata":{"id":"gmMs1P1tabse"}},{"cell_type":"markdown","source":["Given that we only have a few features and they are informed by domain expertise, we are going to skip this step and assume that we have a reasonable set of features."],"metadata":{"id":"Qv-3GvQCcG1_"}},{"cell_type":"markdown","source":["# Step 5: (Optional) Balance Your Dataset"],"metadata":{"id":"2ip_I8PZtMxy"}},{"cell_type":"markdown","source":["Recall that our dataset was fairly imbalanced:"],"metadata":{"id":"xVdSPKfVtWbl"}},{"cell_type":"code","source":["def print_label_dist(y):\n","    \"\"\"\n","    Prints out the balance between positive and negative samples\n","    y: a 1D array of labels\n","    \"\"\"\n","    num_neg = np.count_nonzero(y == 0)\n","    num_pos = np.count_nonzero(y == 1)\n","    print(f'Number of benign samples: {num_neg}')\n","    print(f'Number of malignant samples: {num_pos}')\n","    print(f'Fraction of positive samples: {num_pos/(num_pos+num_neg):0.2f}')"],"metadata":{"id":"_xp6oTLPtYLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_label_dist(df['Label'].values)"],"metadata":{"id":"LstJADupSo_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_label_dist(y_train)"],"metadata":{"id":"vxOOZ0CiSVSN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_label_dist(y_test)"],"metadata":{"id":"CDn6T1l8SaYK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The [`imbalanced-learn`](https://imbalanced-learn.org/stable/)  library provides support for various techniques that address dataset imbalance."],"metadata":{"id":"-bmeLSFEtih3"}},{"cell_type":"markdown","source":["To increase the number of malignant samples in our dataset, we are going to use the Synthetic Minority Oversampling Technique (SMOTE)."],"metadata":{"id":"wtuQYlDb490m"}},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","sm = SMOTE()\n","x_train_balanced, y_train_balanced = sm.fit_resample(x_train, y_train)"],"metadata":{"id":"vCy3W-5n3Svo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_label_dist(y_train_balanced)"],"metadata":{"id":"tgBvCWhkT4-R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that we only apply SMOTE on our training data. This is because we should **never** be touching our test data until we are ready to evaluate our model, and it wouldn't make sense to evaluate our model on synthetic data."],"metadata":{"id":"VJbKn6rs4AAJ"}},{"cell_type":"markdown","source":["# Step 6: Select an Appropriate Model"],"metadata":{"id":"TUuv3wRJaVer"}},{"cell_type":"markdown","source":["We will use the same random forest classifier as before."],"metadata":{"id":"IiS0RhPymQwg"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier()"],"metadata":{"id":"-ZejuzTe8X3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 7: (Optional) Select Your Hyperparameters"],"metadata":{"id":"A-AcODJ3aYx6"}},{"cell_type":"markdown","source":["## Deciding Which Hyperparameters to Optimize"],"metadata":{"id":"BLBy4qpPpQ6r"}},{"cell_type":"markdown","source":["The random forest classifier has many different hyperparameters we can tune. We can find the full list of hyperparameters along with their current settings (currently the defaults) using the `.get_params()` method:"],"metadata":{"id":"h2_4cIHqJ0rc"}},{"cell_type":"code","source":["clf.get_params()"],"metadata":{"id":"W4RJehCKcSwJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not all of these dictionary entries are hyperparameters. For example, the `verbose` parameter is simply a flag that indicates whether you want Python to print out more information whenever you are calling a method on your model."],"metadata":{"id":"5J5xnuJkC3Lp"}},{"cell_type":"markdown","source":["Trying to optimize every single hyperparameter requires a lot of time with marginal benefits. Rather, it generally makes sense to become familiar with the classifier you are using and deciding for yourself which hyperparameters you think are going to have the most impact on model performance. For our classifier, we are just going to optimize three hyperparameters:\n","1. ***n_estimators:*** The number of trees in the random forest\n","2. ***criterion:*** The metric used to measure the quality of a decision tree split\n","3. ***max_depth:*** The maximum number of decisions that can be made in a given decision tree\n","\n","Why did we pick these three? They happen to be the first three in `scikit-learn`'s [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), which is usually a good indicator that these are going to be interesting ones to optimize."],"metadata":{"id":"_9vPdWTQecv7"}},{"cell_type":"markdown","source":["It's also difficult to try an infinite number of settings for each hyperparameter. Therefore, we are going to create a dictionary that specifies the settings that we want to try out:"],"metadata":{"id":"Lar-E9fZgw2q"}},{"cell_type":"code","source":["param_grid = {'n_estimators': [25, 50, 100, 200],\n","              'criterion' : ['gini', 'entropy', 'log_loss'],\n","              'max_depth': [2, 5, 10, None]}"],"metadata":{"id":"ZSf1ZS3-d-8I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How did we know which settings to try? It requires a combination of familiarity with the model and reading documentation. Here is the intuition behind the options laid out above:\n","1. ***n_estimators:*** The default is 100, so it makes sense to try some configurations with fewer estimators and some configurations with more estimators.\n","2. ***criterion:*** Only a few possible settings are allowed since this hyperparameter refers to names of metrics for measuring the impurity of decision tree splits.\n","3. ***max_depth:*** The default is `None`, which implies that a given decision tree can split as many times as needed until all of the leaves (i.e., the ends of the decision tree) are relatively pure. However, this can enable very convoluted and specific decision criteria that lead to overfitting, so we can try some settings to restrict the depth of our trees to see if that helps.\n","\n","This still explains neither the number of settings we plan to try out nor the spacing between the settings. Unfortunately, that comes with experience and trial-and-error. Bear in mind that while trying more options can make you more confident in the outcome, it means that our code will take longer to run."],"metadata":{"id":"ixOJTrRfhTsl"}},{"cell_type":"markdown","source":["## Optimizing the Hyperparameters"],"metadata":{"id":"srXIpMFCpYxR"}},{"cell_type":"markdown","source":["We are going to use 5-fold cross-validation on our training set to optimize our hyperparameters. This will give us access to far more data for hyperparameter tuning than if we separated out a distinct chunk of data from our training set to serve as our validation set.\n","\n","As a reminder, k-fold cross-validation involves splitting data into $k$ distinct chunks and then training $k$ different models such that each chunk serves as the test set once."],"metadata":{"id":"M_Mwd7x1oA4m"}},{"cell_type":"markdown","source":["Many hyperparameters are often intertwined. For example, increasing the number of estimators in your model may mean that your trees can be shallow. Therefore, it makes sense to optimize all of the hyperparameters at the same time rather than one at a time."],"metadata":{"id":"BWQwZxxteAjq"}},{"cell_type":"markdown","source":["We are going to use a ***grid search*** to systematically try all possible combinations of hyperparameter settings that we've laid out above. The pseudocode below outlines how this procedure will look for our example:\n","\n","```\n","for n_estimators in [25, 50, 100, 200]:\n","    for criterion in ['gini', 'entropy', 'log_loss']:\n","        for max_depth in [2, 5, 10, None]:\n","            train the model using these settings\n","            if model better than any model from before:\n","                save the settings\n","```"],"metadata":{"id":"W3ps3PA5tCVF"}},{"cell_type":"markdown","source":["`scikit-learn` provides a `GridSearchCV` class that will do this procedure for us. We are going to specify four parameters in this class:\n","1. ***estimator:*** A model architecture\n","2. ***param_grid:*** A dictionary specifying the hyperparameters and settings that we'd like to test\n","3. ***cv:*** The number of folds in our k-fold cross-validation scheme\n","4. ***verbose:*** Setting this flag to `3` will print detailed information about what is happening in the function. We will only do this for the sake of verifying what is happening in our code.\n","\n","As a small detail, we will need to set the random seed for the model we use for this procedure in particular since it is separate from the random seed used by `numpy`. This detail can be ignored once you are ready to truly test your pipeline."],"metadata":{"id":"bwY69-cjt__m"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","grid_search_model = RandomForestClassifier(random_state=42)\n","grid_search = GridSearchCV(grid_search_model, param_grid,\n","                           cv=5, verbose=3)\n","\n","grid_result = grid_search.fit(x_train, y_train)\n","best_params = grid_result.best_params_\n","print(f'Best Params: {grid_result.best_params_}')\n","print(f'Best Score: {grid_result.best_score_}')"],"metadata":{"id":"SDk0Sqdccga4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As it turns out, our optimal hyperparameters for our task are all different from `scikit-learn`'s default settings. It seems that limiting the depth of our trees and reducing the number of estimators led to better performance. These settings translate to a model with less flexibility, which may lead to better generalization down the road."],"metadata":{"id":"d6J1WJMt9JCZ"}},{"cell_type":"markdown","source":["Once we have identified the optimal hyperparameters, we will need to set those values in the model that we plan to actually train and test. We can do this by creating a new model as follows:"],"metadata":{"id":"ZoOM4dIZqAXM"}},{"cell_type":"code","source":["clf = RandomForestClassifier(**best_params)"],"metadata":{"id":"2-md62nYqAD3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that `**best_params` unpacks the dictionary of best parameters into keyword arguments for the model's constructor."],"metadata":{"id":"zkZwY5tj6Fp2"}},{"cell_type":"markdown","source":["# Step 8: Train and Test Your Model"],"metadata":{"id":"-fpM5JCVcS_c"}},{"cell_type":"markdown","source":["We will train and test our model as we have in the past, but with two key differences:\n","1. We have optimized the hyperparameters of the random forest\n","2. We will train our data using the balanced dataset provided by SMOTE"],"metadata":{"id":"fndMkb0u7_-p"}},{"cell_type":"code","source":["clf.fit(x_train_balanced, y_train_balanced)"],"metadata":{"id":"i4DkQNLRDH3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train_pred = clf.predict(x_train)\n","y_test_pred = clf.predict(x_test)"],"metadata":{"id":"LZBGUjVnDbce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_binary = clf.predict(x_train[:10, :])\n","y_prob = clf.predict_proba(x_train[:10, :])"],"metadata":{"id":"a7ZExfdEfvjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train_pred_prob = clf.predict_proba(x_train)[:, 1]\n","y_test_pred_prob = clf.predict_proba(x_test)[:, 1]"],"metadata":{"id":"tOESgRqUi8ge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 9: Use an Appropriate Method for Interpreting Results"],"metadata":{"id":"hT0cnOFdafEF"}},{"cell_type":"markdown","source":["We are going to use the same function we created earlier to view the classification accuracy of our model."],"metadata":{"id":"zj0apvZTbn5d"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n","\n","classes = ['benign', 'malignant']\n","def classification_evaluation(y_true, y_pred, y_pred_prob):\n","    \"\"\"\n","    Generate a series of graphs that will help us determine the performance of\n","    a binary classifier model\n","    y_true: the target binary labels\n","    y_pred: the predicted binary labels\n","    y_pred_prob: the predicted likelihood scores for a positive label\n","    \"\"\"\n","    # Calculate f1 score, sensitivity, and specificity\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn = cm[0][0]\n","    fp = cm[0][1]\n","    fn = cm[1][0]\n","    tp = cm[1][1]\n","    f1 = f1_score(y_true, y_pred)\n","    sens = tp / (tp+fn)\n","    spec = tn / (tn+fp)\n","\n","    # Generate the confusion matrix\n","    cm_title = f'Confusion Matrix \\n(Sensitivity: {sens:0.2f}, Specificity: {spec:0.2f})'\n","    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=classes)\n","    plt.title(cm_title)\n","    plt.show()\n","\n","    # Display the ROC curve\n","    roc_title = f'ROC Curve (F1 score: {f1:0.2f})'\n","    RocCurveDisplay.from_predictions(y_true, y_pred_prob)\n","    plt.title(roc_title)\n","    plt.show()"],"metadata":{"id":"b1LxWw82eXkj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Without dataset balancing and hyperparameter tuning, we achieved perfect accuracy on our training data; however, we only achieved an F1 score of 0.10 on our test data. Let's see what happens when we evaluate our new model (note that we evaluating training accuracy only on real data before SMOTE was applied):"],"metadata":{"id":"PeRsPC3cjgwu"}},{"cell_type":"code","source":["classification_evaluation(y_train, y_train_pred, y_train_pred_prob)"],"metadata":{"id":"A-eEyZjjjl_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Because we chose hyperparameters that made our model less flexible, we actually see a slight drop in training accuracy. Nevertheless, it seems like the model is still learning from the features in order to provide useful predictions on the training set."],"metadata":{"id":"r6K5RI-nBTqE"}},{"cell_type":"code","source":["classification_evaluation(y_test, y_test_pred, y_test_pred_prob)"],"metadata":{"id":"DnvfgKLRjrCO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Although the model performance isn't stellar, we can see that our changes led to an improvement in our test accuracy. While we only achieved an F1 score of 0.10 before, we have now bumped the F1 score to 0.36! Since we made multiple changes to our pipeline at the same time, we can't say for certain which change led to the bigger improvement, but clearly we're heading in the right direction."],"metadata":{"id":"73HEAeGwqoEr"}},{"cell_type":"markdown","source":["So what are some things we could do to push the performance further? Here are some ideas:\n","* **Refine our current features:** We made some assumptions about the best way to represent various characteristics of the skin lesion. For example, we decided that the HSV color space would be the best way to represent color, but perhaps another color space may have yielded better results.\n","* **Examine more features:** All of our features were inspired by the ABCDE rule of dermatology. While it is strongly recommended that you ground your features in domain expertise, data science gives you the opportunity to push the limits of what humans can do. For example, maybe the ABCDE rule hasn't included anything about speckling patterns within skin lesions because it's hard to perceive such patterns with the naked eye. Using image processing, we can explore for ourselves whether we can extract such information and whether it can be useful for prediction.\n","* **More data:** Roughly 20% of our dataset included malignant skin lesions. Although we used SMOTE to balance our training split, remember that the synthetic examples can only be made from existing data. If possible, it would have been better if we could have intentionally collected more images of malignant skin lesions to round out our dataset."],"metadata":{"id":"rzB4bc88ueDr"}}]}