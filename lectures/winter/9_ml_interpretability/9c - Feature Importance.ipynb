{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DekAnzq1b82b",
        "K6PhsvK1NuDg",
        "y2buTYo2jNIF",
        "qaN8HJdVjQTN",
        "IX2o4Qs4jYoo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to talk about some of the ways we can inspect the internals of a machine learning model to learn more about how it makes predictions."
      ],
      "metadata": {
        "id": "BDaO9cuMWe83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important: Run this code cell each time you start a new session!"
      ],
      "metadata": {
        "id": "DekAnzq1b82b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install os\n",
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "jrO0X1ZMxMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "diabetes_dataset = datasets.load_diabetes(as_frame=True)\n",
        "df = diabetes_dataset.frame\n",
        "\n",
        "# Rename the features for clarity\n",
        "df = df.rename(columns={'s1': 'total serum cholesterol',\n",
        "                        's2': 'low-density lipoproteins',\n",
        "                        's3': 'high-density lipoproteins',\n",
        "                        's4': 'total cholesterol',\n",
        "                        's5': 'log of serum triglycerides',\n",
        "                        's6': 'blood sugar'})"
      ],
      "metadata": {
        "id": "bkHkGrOMm2KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_regressor(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a regression model on the input DataFrame, returning the\n",
        "    regressor, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(orig_df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    regr = LinearRegression()\n",
        "    regr.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = regr.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return regr, feature_names, data"
      ],
      "metadata": {
        "id": "dckQxUtmXekZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_classifier(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a classification model on the input DataFrame, returning the\n",
        "    classifier, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame for regression\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Copy the DataFrame since we will be modifying it\n",
        "    df = orig_df.copy()\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Turn the label into a binary variable\n",
        "    df['target'] = df['target'] > 150\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    clf = DecisionTreeClassifier()\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = clf.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return clf, feature_names, data"
      ],
      "metadata": {
        "id": "NdmtZ4niHtVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Is Feature Importance?"
      ],
      "metadata": {
        "id": "K6PhsvK1NuDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Feature importance*** refers to the idea of quantifying the relevance of features that are used in a predictive model. Determining the importance of each feature can help us understand which features are most influential in determining the final predictions made by the model. By identifying important features, we can gain insights into the underlying patterns and relationships that drive the model's predictions, which can in turn help with feature selection, model interpretability, and improvements in the model's performance.  \n",
        "\n"
      ],
      "metadata": {
        "id": "C2XXNkrVKe_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each category of machine learning model architectures (e.g., linear models, tree-based models) have its own mathematical underpinnings. Therefore, each category may require a different technique to inspect its internals. We will go over relevant techniques for the two models we built on our Diabetes Dataset — a linear regression model and a decision tree classifier — but these methods will generalize to many other model architectures."
      ],
      "metadata": {
        "id": "afSy3_y4K9dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Weights and Coefficients"
      ],
      "metadata": {
        "id": "y2buTYo2jNIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear models like linear regression and logistic regression use weighted sums of input features to make predictions. Therefore, the magnitude and direction of these coefficients can give us a rough idea about feature importance as follows:\n",
        "\n",
        "| Magnitude | Sign | Interpretation |\n",
        "|-----------|------|----------------|\n",
        "| Large | Positive | The feature is important and leads to larger ouptuts |\n",
        "| Small | Positive | The feature is less important, but still leads to larger ouptuts |\n",
        "| Large | Negative | The feature is important and leads to smaller ouptuts |\n",
        "| Small | Negative | The feature is less important, but still leads to larger ouptuts |"
      ],
      "metadata": {
        "id": "OAePmUkgM3lo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important:** The table above assumes that all of the model's features have a similar scale. Imagine that we had two features: $F_1$, which ranges between 0 and 1; and $F_2$, which ranges between 0 and 100. Since the scale of $F_2$ is much larger than that of $F_1$, even small changes in $F_2$ can lead to larger changes in the prediction. As a result, $F_2$'s coefficient after model training is likely to be larger compared to $F_1$'s coefficient. This does not mean that $F_2$'s coefficient, but is rather a matter of adjusting scale.\n",
        "\n",
        "Once you normalize the scale of your features, you can start answering questions like the following: \"When I change this feature value by half a standard deviation, how does it influence the model's prediction?\""
      ],
      "metadata": {
        "id": "11C580E4NbxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the coefficients of our linear regression model:"
      ],
      "metadata": {
        "id": "xuwJalDQPne5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regr, feature_names, _ = generate_regressor(df)\n",
        "importances = regr.coef_\n",
        "for feature_name, importance in zip(feature_names, importances):\n",
        "    print(f'Feature: {feature_name}, Score: {importance}')"
      ],
      "metadata": {
        "id": "MbG_l_NzN-m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot these coefficients in a bar chart to visually compare them. Let's look at both the signed and unsigned value of the coefficients so that we can compare both their magnitude and direction:"
      ],
      "metadata": {
        "id": "oeSPzT7LQEcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(feature_names, importances)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Signed Coefficients')\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Linear Model Coefficient')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(feature_names, np.abs(importances))\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Unsigned Coefficients')\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Linear Model Coefficient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ch2MUYlINI0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to these graphs, we can see that total serum cholestorol has the strongest impact on the predictions from our linear regression model. Because it is negative, higher total serum cholesterol values will lead to lower predictions.\n",
        "\n",
        "Log of serum triglycerides, BMI, and low-density lipoproteins seem to be the next most significant features, and they are all positive. Therefore, higher values for those features will lead to higher predictions."
      ],
      "metadata": {
        "id": "Reh-tq_OQ2M0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Branches"
      ],
      "metadata": {
        "id": "qaN8HJdVjQTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree-based models like decision trees and random forests do not have coefficients. Instead, they use a series of learned rules to make predictions on input data. We can actually inspect these rules using the `plot_tree()` function in\n",
        "the `tree` module of `scikit-learn`. Note that we are saving this figure to as an image file rather than showing it in this notebook. That is because we need to make the image quite large in order to see all of the rules:"
      ],
      "metadata": {
        "id": "-cFgrjwsR3_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "clf, feature_names, _ = generate_classifier(df)\n",
        "plt.figure(figsize=(50, 50))\n",
        "tree.plot_tree(clf, feature_names=feature_names, filled=True, fontsize=10)\n",
        "plt.savefig('tree.jpg')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "GXLbxpPSN9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rows within a node of the decision tree can be interpreted as follows:\n",
        "1. **Rule:** The decision rule that dictates whether an input sample should go to the left or right.\n",
        "2. **Gini:** We will cover this number in a bit, but this is a measure of the rule's importance within the tree according to our training set.\n",
        "3. **Samples:** The number of training samples that made it to this point in the tree.\n",
        "4. **Value:** The number of training samples that went to the left and to the right after this rule, respectively."
      ],
      "metadata": {
        "id": "9-5XeOxeVje4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While visualizing the tree gives us definitive information about how the model is making decsions, it can be difficult to keep track of which feature values are relevant for a given input.\n",
        "\n",
        "Furthermore, looking at the tree doesn't necessarily give us a clear idea of which features are most important to the overall model. Just because a node is higher in the decision tree does not mean that the corresponding feature is automatically more important."
      ],
      "metadata": {
        "id": "sVfjnFpeVdqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purity-Based Importance Scores"
      ],
      "metadata": {
        "id": "IX2o4Qs4jYoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way we can rate the quality of a feature in a tree-based model is by seeing how effective it is at separating training samples belonging to different classes. Imagine we have a decision tree with three nodes:\n",
        "* $N_{root}$: the node that makes a decision based on a rule\n",
        "* $N_{yes}$: the node where samples that satisfy the rule land\n",
        "* $N_{no}$: the node where samples that do not satisfy the rule land\n",
        "\n",
        "If $N_{root}$ only has training samples from a single class, we would call it \"pure\". If $N_{root}$ has an equal mix of positive and negative samples, we would say that the node is \"impure\". There are lots of ways of measuring impurity, but one of the more common metrics that we will see is the ***Gini impurity***."
      ],
      "metadata": {
        "id": "1NARkW5mftUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rule in $N_{root}$ is effective if the nodes that follow it, $N_{yes}$ and $N_{no}$, both have higher purity than $N_{root}$. This mean that the rule is sorting through the disorder of the class distributions and separating training samples belonging to different classes."
      ],
      "metadata": {
        "id": "5xEr18fIic-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can measure the overall ***Gini importance score*** for a feature by averaging the total reduction in the Gini impurity across all the nodes in the tree that use the feature for splitting. Features that result in a significant reduction in the impurity are considered more important, as they contribute more to the overall purity and separation of the classes in the decision tree."
      ],
      "metadata": {
        "id": "Re8XgIo8SQTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini importance scores range from 0 to 1, with higher values indicating greater importance. Unlike with the linear model coefficients, we cannot look at the Gini importance scores to determine whether features have a positive or negative impact on predictions. This is because there may be multiple nodes with conflicting rules based on how the data gets separated within lower levels of the tree."
      ],
      "metadata": {
        "id": "tq8g6AucjnMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the Gini importance scores look like for our decision tree classifier:"
      ],
      "metadata": {
        "id": "rV6QJ-wiSujr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = clf.feature_importances_\n",
        "for feature_name, importance in zip(feature_names, importances):\n",
        "    print(f'Feature: {feature_name}, Score: {importance}')"
      ],
      "metadata": {
        "id": "H-L_9-_uSxhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.title('Gini Importance Scores')\n",
        "plt.bar(feature_names, importances)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Gini Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wHhmongPjeNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to this graph, we can see that the log of serum triglycerides is the most important feature in our model, followed by BMI and blood pressure."
      ],
      "metadata": {
        "id": "hEXWSVOvjcCc"
      }
    }
  ]
}