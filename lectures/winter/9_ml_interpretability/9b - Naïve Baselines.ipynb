{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DekAnzq1b82b",
        "xP4sP_ekZxqW",
        "mTlaD47Pp-Ud",
        "rj4LyAPbryfw",
        "8HicRm76rCmB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to talk about the benefits of creating a naïve baseline model to establish whether a machine learning model is taking advantage of the features to make more accurate predictions."
      ],
      "metadata": {
        "id": "BDaO9cuMWe83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important: Run this code cell each time you start a new session!"
      ],
      "metadata": {
        "id": "DekAnzq1b82b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install os\n",
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "jrO0X1ZMxMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "diabetes_dataset = datasets.load_diabetes(as_frame=True)\n",
        "df = diabetes_dataset.frame\n",
        "\n",
        "# Rename the features for clarity\n",
        "df = df.rename(columns={'s1': 'total serum cholesterol',\n",
        "                        's2': 'low-density lipoproteins',\n",
        "                        's3': 'high-density lipoproteins',\n",
        "                        's4': 'total cholesterol',\n",
        "                        's5': 'log of serum triglycerides',\n",
        "                        's6': 'blood sugar'})"
      ],
      "metadata": {
        "id": "bkHkGrOMm2KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_regressor(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a regression model on the input DataFrame, returning the\n",
        "    regressor, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(orig_df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    regr = LinearRegression()\n",
        "    regr.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = regr.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return regr, feature_names, data"
      ],
      "metadata": {
        "id": "dckQxUtmXekZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_classifier(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a classification model on the input DataFrame, returning the\n",
        "    classifier, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame for regression\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Copy the DataFrame since we will be modifying it\n",
        "    df = orig_df.copy()\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Turn the label into a binary variable\n",
        "    df['target'] = df['target'] > 150\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    clf = DecisionTreeClassifier()\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = clf.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return clf, feature_names, data"
      ],
      "metadata": {
        "id": "NdmtZ4niHtVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Do We Know Our Model Is Good?"
      ],
      "metadata": {
        "id": "xP4sP_ekZxqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some of our previous lectures, we evaluated the performance of our models using a variety of metrics: accuracy and F1 score for classification, mean-squared error for regression, etc. We saw that these numbers improved as we made adjustments to our machine learning pipeline. However, we didn't really get into what could be considered a \"good model\" versus a \"bad model\"."
      ],
      "metadata": {
        "id": "La5h-Y85nLZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're working in a specific problem domain, then you might have a target accuracy you are trying to achieve. For example, the American Heart Association recommends that any device or algorithm designed to estimate blood pressure should have an average accuracy of ±3 mmHg for systolic blood pressure and ±2 mmHg for diastolic blood pressure."
      ],
      "metadata": {
        "id": "J9OJliKcB3XH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While it is nice to have a target that we are trying to reach, it is equally important to have a starting point to know where we are coming from before we even start building our model."
      ],
      "metadata": {
        "id": "Ix9gelEjDVfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say that we wanted to predict the outcome of a fair coin flip. We could compare two different approaches: (1) a human making the guess, and (2) a machine learning model that tries to predict the outcome of the coin flip once the person flips the coin in the air.\n",
        "* **Human guesser:** If we were to predict the result at random chance, we would be accurate 1/2 = 50% of the time.\n",
        "* **Machine learning model:** If the model guesses the coin's outcome correctly 30% of the time, then it would actually be doing worse than random chance."
      ],
      "metadata": {
        "id": "z1z1Mt8upE2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we were to go through the same thought exercise with an unfair coin that we know lands on heads 80% of the time?\n",
        "* **Human guesser:** Using our knowledge about the coin, always predicting heads would make us correct 80% of the time.\n",
        "* **Machine learning model:** If the model guesses the coin's outcome correctly 30% of the time, then our model would be considered terrible."
      ],
      "metadata": {
        "id": "uWjJ0uX7-tTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's change the situation one more time and imagine we're rolling a fair dice instead of flipping a coin.\n",
        "* **Human guesser:** If we were to predict the result at random chance, we would be accurate 1/6 = 17% of the time.\n",
        "* **Machine learning model:** If the model guesses the dice's outcome correctly 30% of the time, it doesn't *sound* like it's working that well. However, considering that we were getting 17% with random guessing, 30% doesn't sound so bad all of the sudden!"
      ],
      "metadata": {
        "id": "tc5HrDAwM1W6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we always said that the accuracy of the model was 30% in these three scenarios, the way we interpreted that number varied each time."
      ],
      "metadata": {
        "id": "RAtrOjflWIga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the best ways of contextualizing the performance of your model is by establishing a ***naïve baseline***. Some people refer to a naïve baseline as a ***dummy model*** because it follows a predefined rule or strategy rather than making any intelligent or informed predictions."
      ],
      "metadata": {
        "id": "Yw7vzvzwyETV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naïve Baselines for Regression"
      ],
      "metadata": {
        "id": "mTlaD47Pp-Ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we generate regression baselines for our toy diabetes dataset, let's first remind ourselves of how well our regression model worked. We could look at any number of performance metrics, but we will focus on the mean-absolute error (MAE)."
      ],
      "metadata": {
        "id": "daCy9tLlxNdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, data = generate_regressor(df)\n",
        "y_test = data['test']['y']\n",
        "y_pred = data['pred']['y']\n",
        "model_mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'Mean absolute error: {model_mae:0.2f}')"
      ],
      "metadata": {
        "id": "85ItYs_7gYDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`scikit-learn` provides a `DummyRegressor` object with a similar interface to the other machine learning model objects we have created so far. In other words, we can call `.fit()` and `.predict()` on it just like any other model. However, this regression model determines its predictions solely on the distribution of the labels it sees during training."
      ],
      "metadata": {
        "id": "DcdBtZukxicP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyRegressor"
      ],
      "metadata": {
        "id": "y8nQWfkKFC7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important parameter in this object is the `strategy` parameter, which determines how the `DummyRegressor` makes predictions. It can take one of the following values:\n",
        "* **“mean”:** Always predicts the mean of the training set\n",
        "* **“median”:** Always predicts the median of the training set\n",
        "* **“quantile”:** Always predicts a specified quantile of the training set according to the `quantile` parameter (i.e., `quantile = 0.5` is the same as `median`)\n",
        "* **“constant”:** Always predicts a constant value according to the `constant` parameter"
      ],
      "metadata": {
        "id": "Cd_79aVqzOAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how well we can predict the disease progression score using all of these naïve methods:"
      ],
      "metadata": {
        "id": "Ok0MFrayzfTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "const = 175 #@param {type:\"slider\", min:0, max:350, step:25}\n",
        "\n",
        "# Get the required parts of the dataset\n",
        "x_train = data['train']['x']\n",
        "y_train = data['train']['y']\n",
        "x_test = data['test']['x']\n",
        "y_test = data['test']['y']\n",
        "\n",
        "# Print the model's performance\n",
        "print(f\"Model's mean absolute error: {model_mae:0.2f}\")\n",
        "\n",
        "# Try all strategies\n",
        "for strat in ['mean', 'median', 'quantile', 'constant']:\n",
        "    # Train the dummy regressor on the training data\n",
        "    dumb_regr = DummyRegressor(strategy=strat, quantile=quant, constant=const)\n",
        "    dumb_regr.fit(x_train, y_train)\n",
        "\n",
        "    # Test the dummy regressor on the test data\n",
        "    dumb_y_pred = dumb_regr.predict(x_test)\n",
        "\n",
        "    # Evaluate MSE for the dummy regressor\n",
        "    dummy_mae = mean_absolute_error(y_test, dumb_y_pred)\n",
        "    print(f'Mean absolute error for {strat}: {dummy_mae:0.2f}')"
      ],
      "metadata": {
        "id": "gn1miDCR0_Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although it seemed like the MSE of our model was very high, comparing to these naïve baselines reveals that our model is actually learning quite a bit of useful information from the features."
      ],
      "metadata": {
        "id": "k5Tk3I4zxJG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naïve Baselines for Classification"
      ],
      "metadata": {
        "id": "rj4LyAPbryfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compare the accuracy of our classifier to some naïve baselines using the `DummyClassifier` object."
      ],
      "metadata": {
        "id": "-F1PYxXqHsKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, data = generate_classifier(df)\n",
        "y_test = data['test']['y']\n",
        "y_pred = data['pred']['y']\n",
        "model_acc = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {model_acc:0.2f}')"
      ],
      "metadata": {
        "id": "UC-3zjPQHsKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier"
      ],
      "metadata": {
        "id": "jBtPhisgHsKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we describe the strategies that `DummyClassifier` can take, we will first need to introduce one new piece of terminology. ***One-hot encoding*** is a popular technique used to represent categorical variables as binary vectors. Let's say we have three possible classes: negative (0), neutral (1), and positive (2). Using one-hot encoding, we could convert these labels to binary vectors as follows:\n",
        "\n",
        "| Label | One-hot Encoded Vector |\n",
        "|-------|------------------------|\n",
        "| Negative | `[1, 0, 0]` |\n",
        "| Neutral | `[0, 1, 0]` |\n",
        "| Positive | `[0, 0, 1]` |\n"
      ],
      "metadata": {
        "id": "PRlnKZ-nc5qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might recall that classifiers have a method called `.predict_proba()` that returns the probability that the given sample belongs in a specific class. This is in contrast to `.predict()`, which simply reports the class to which the sample most likely belongs.\n",
        "\n",
        "When the model gets trained, it's actually getting trained on one-hot encoded vector labels. When you call `.predict()`, the model is calling `.predict_proba()` to predict a vector of probabilities and then picking the class with the highest probability."
      ],
      "metadata": {
        "id": "izs-MgUZeRhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this detail in mind, let's talk about the prediction strategies supported by the `DummyClassifier` object:\n",
        "* **“most_frequent”:** Always returns the most frequent class label in the training data. If you call `.predict_proba()`, the model will return the matching one-hot encoded vector.\n",
        "* **“prior”:** The output of `.predict()` will be the same as if you were using `\"most_frequent\"`. However, the output of `.predict_proba()` will be continuous probabilities according to the distribution of the training set rather than one-hot encoded vectors.\n",
        "* **“stratified”:** The output of `.predict_proba()` randomly samples one-hot vectors from the distribution of the training set. The `.predict()` method returns the class label with the highest probability within each vector.\n",
        "* **“uniform”:** Generates predictions at random with equal probability from the list of unique classes observed in the training set\n",
        "* **“constant”:** Always predicts a constant label according to the `constant` parameter"
      ],
      "metadata": {
        "id": "EIjFM1d4HsKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how well we can predict the disease progression severity level using all of these naïve methods:"
      ],
      "metadata": {
        "id": "CeY9eA5BHsKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "const = 1 #@param {type:\"slider\", min:0, max:1, step:1}\n",
        "\n",
        "# Get the required parts of the dataset\n",
        "x_train = data['train']['x']\n",
        "y_train = data['train']['y']\n",
        "x_test = data['test']['x']\n",
        "y_test = data['test']['y']\n",
        "\n",
        "# Print the model's performance\n",
        "print(f\"Model's accuracy: {model_acc:0.2f}\")\n",
        "\n",
        "# Try all strategies\n",
        "for strat in ['most_frequent', 'prior', 'stratified', 'uniform', 'constant']:\n",
        "    # Train the dummy classifier on the training data\n",
        "    dumb_clf = DummyClassifier(strategy=strat, constant=const)\n",
        "    dumb_clf.fit(x_train, y_train)\n",
        "\n",
        "    # Test the dummry classifier on the test data\n",
        "    dumb_y_pred = dumb_clf.predict(x_test)\n",
        "\n",
        "    # Evaluate accuracy for the dummy classifier\n",
        "    dummy_acc = accuracy_score(y_test, dumb_y_pred)\n",
        "    print(f'Accuracy for {strat}: {dummy_acc:0.2f}')"
      ],
      "metadata": {
        "id": "s6TR9wooHsKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we see that our model is outperforming our naïve baselines with respect to these metrics. As a sanity check, let's re-run the same code but look at F1 score rather than accuracy:"
      ],
      "metadata": {
        "id": "JPx_cJ6tb_1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "const = 0 #@param {type:\"slider\", min:0, max:1, step:1}\n",
        "\n",
        "# Get the required parts of the dataset\n",
        "x_train = data['train']['x']\n",
        "y_train = data['train']['y']\n",
        "x_test = data['test']['x']\n",
        "y_test = data['test']['y']\n",
        "\n",
        "# Print the model's performance\n",
        "model_f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Model's F1 score: {model_f1:0.2f}\")\n",
        "\n",
        "# Try all strategies\n",
        "for strat in ['most_frequent', 'prior', 'stratified', 'uniform', 'constant']:\n",
        "    # Train the dummy classifier on the training data\n",
        "    dumb_clf = DummyClassifier(strategy=strat, constant=const)\n",
        "    dumb_clf.fit(x_train, y_train)\n",
        "\n",
        "    # Test the dummry classifier on the test data\n",
        "    dumb_y_pred = dumb_clf.predict(x_test)\n",
        "\n",
        "    # Evaluate accuracy for the dummy classifier\n",
        "    dummy_f1 = f1_score(y_test, dumb_y_pred)\n",
        "    print(f'F1 score for {strat}: {dummy_f1:0.2f}')"
      ],
      "metadata": {
        "id": "vQdJ5bgShoyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that some of our naïve baselines get an F1 score of 0. This is because F1 score is the harmonic mean of precision and recall. When a naïve classifier always predicts negative, it gets a perfect precision because it never gets a false positive; however, it also gets zero recall because it never gets a true positive. This shows that it is important to think carefully about which metrics you use to evaluate both your own model and potential baselines."
      ],
      "metadata": {
        "id": "vPOFjABAmKRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Baselines"
      ],
      "metadata": {
        "id": "8HicRm76rCmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baselines we've discussed so far are naïve because they rely solely on the distribution of the labels or predetermined rules to make decisions. That doesn't mean that these are the only kinds of baselines you can use as a frame of reference. Here are some examples of other kinds of \"baselines\" you might consider:\n",
        "* **Model architecture:** If you want to show that a random forest classifier is the right model for your problem, your baseline can come in the form of a decision tree trained on the same set of data.\n",
        "* **Features:** If you want to show that you need all of your features to generate the best model possible, your baselines can come in the form of models that are trained on subets of your features. With our toy Diabetes Dataset, for example, the baselines could be a model trained on just the demographic variables and a model trained on just the blood serum test results."
      ],
      "metadata": {
        "id": "BfSb8B7_rE3x"
      }
    }
  ]
}