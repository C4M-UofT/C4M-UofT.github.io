{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["DekAnzq1b82b","xP4sP_ekZxqW","dF7vGVljaKp7","1O2DKWI7lpOk","buga415hls_2","OGuqI2yadkun","2gLiPr2p6ymF","SY3s63jSnDGb","ryaahUysaOHD","gmMs1P1tabse","90Fwfakplyu8","A-AcODJ3aYx6","-fpM5JCVcS_c","hT0cnOFdafEF","WwE9unOpIbm2","e87qpxdPIsga","iikCuJ_eeHUj","uvtSJXQxeJE9"],"authorship_tag":"ABX9TyMDAXqbTo02yBoECBHNkmB7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this notebook, we're going to build a classification model that discriminates between malignant and benign skin lesions. The data will come from a machine learning challenge published by the [International Skin Imaging Collaboration (ISIC)](https://challenge.isic-archive.com/data/) in 2016."],"metadata":{"id":"BDaO9cuMWe83"}},{"cell_type":"markdown","source":["This will also be our first encounter with [`scikit-learn`](https://scikit-learn.org/stable/index.html) — the most popular Python library for building and evaluating machine learning models."],"metadata":{"id":"ya0DHpuKh1He"}},{"cell_type":"code","source":["!pip install scikit-learn\n","import sklearn"],"metadata":{"id":"AUlGQ9EpiGkH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Important: Run this code cell each time you start a new session!"],"metadata":{"id":"DekAnzq1b82b"}},{"cell_type":"code","source":["!pip install numpy\n","!pip install pandas\n","!pip install matplotlib\n","!pip install os\n","!pip install opencv-python\n","!pip install scikit-learn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import sklearn"],"metadata":{"id":"jrO0X1ZMxMN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget -Ncnp https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_Data.zip"],"metadata":{"id":"bASFcdZNZuGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -n ISBI2016_ISIC_Part3_Training_Data.zip"],"metadata":{"id":"2hP2UnPrcEdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget -Ncnp https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_GroundTruth.csv"],"metadata":{"id":"hIBYeAPogvAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget -Ncnp https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part1_Training_GroundTruth.zip"],"metadata":{"id":"b3tho-Kt9q_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -n ISBI2016_ISIC_Part1_Training_GroundTruth.zip"],"metadata":{"id":"Ebi3lsSm-poa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 1: Define the Problem You Are Trying to Solve"],"metadata":{"id":"xP4sP_ekZxqW"}},{"cell_type":"markdown","source":["The overarching goal of the ISIC 2016 Challenge was to develop image analysis tools that automatically diagnose of melanoma from dermoscopic images. The organizers of the challenge provided collections of 1022 (w) $\\times$ 767 (h) px images gathered from distinct patients. Each image was determined to be benign or malignant based on the judgment of a clinician."],"metadata":{"id":"BBrNiJenan-A"}},{"cell_type":"code","source":["# The relevant folders and files associated with this dataset\n","# (we will talk about some of them later)\n","image_folder = 'ISBI2016_ISIC_Part3_Training_Data'\n","segementation_folder = 'ISBI2016_ISIC_Part1_Training_GroundTruth'\n","label_filename = 'ISBI2016_ISIC_Part3_Training_GroundTruth.csv'"],"metadata":{"id":"tjO9wyZHMg9O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load two pre-selected image files to show what they look like\n","benign_filename = 'ISIC_0000000.jpg'\n","malignant_filename = 'ISIC_0000002.jpg'\n","benign_img = cv2.imread(os.path.join(image_folder, benign_filename))\n","benign_img = cv2.cvtColor(benign_img, cv2.COLOR_BGR2RGB)\n","malignant_img = cv2.imread(os.path.join(image_folder, malignant_filename))\n","malignant_img = cv2.cvtColor(malignant_img, cv2.COLOR_BGR2RGB)\n","\n","# Show the images and their labels\n","plt.figure(figsize=(6, 3))\n","plt.subplot(1, 2, 1), plt.imshow(benign_img), plt.title('Benign')\n","plt.subplot(1, 2, 2), plt.imshow(malignant_img), plt.title('Malignant')\n","plt.show()"],"metadata":{"id":"nlbjx-aIDbFe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Because we are deciding between distinct outcome categories, we will want to create a ***classification model***. We have two possible labels: 'benign' and 'malignant'. For simplicity, we will call these labels 'negative' (`0`) and 'positive' (`1`) respectively."],"metadata":{"id":"e8tQO6exj_sr"}},{"cell_type":"markdown","source":["# Step 2: Create Your Features and Labels"],"metadata":{"id":"dF7vGVljaKp7"}},{"cell_type":"markdown","source":["## Creating the Labels"],"metadata":{"id":"1O2DKWI7lpOk"}},{"cell_type":"markdown","source":["The labels for our dataset are provided in a `.csv` file. Let's load it and see what it looks like:"],"metadata":{"id":"CvWsGcLPkXvy"}},{"cell_type":"code","source":["labels_df = pd.read_csv(label_filename, header=None)\n","labels_df"],"metadata":{"id":"Xkr2TodFhmN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The first column provides the name of an image file (without the extension), and the second column provides the label that should be associated with that image."],"metadata":{"id":"X9WHluLPkeFL"}},{"cell_type":"markdown","source":["To make this more usable, we are going to do two things:\n","1. To make it easier to merge our features with our labels, we will set the index of the `DataFrame` to be the image name.\n","2. Most libraries prefer that labels are represented with numbers rather than strings of text. Therefore, we are going to map the text `'benign'` to `0` and `'malignant'` to `1`."],"metadata":{"id":"nCIOkgRulnca"}},{"cell_type":"code","source":["labels_df.rename(columns={0: 'Image Name', 1: 'Label'}, inplace=True)\n","labels_df.set_index(['Image Name'], inplace=True)\n","labels_df['Label'].replace({'benign': 0, 'malignant': 1}, inplace=True)\n","labels_df"],"metadata":{"id":"2e_PIVlqk6CT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plan for Creating the Features"],"metadata":{"id":"buga415hls_2"}},{"cell_type":"markdown","source":["Creating the features is not going to be as straightforward. We could theoretically use the color of every single pixel as its own feature, but would result in an extremely complex and rigid feature space that would result in overfitting. Deep learning can handle such complex data, but not traditional machine learning models."],"metadata":{"id":"t9huu6aTluCV"}},{"cell_type":"markdown","source":["Instead, we are going to use the image processing techniques we discussed earlier to summarize each image as a series of numerical features. More specifically, we are going to locate the skin lesion within the broader image and then quantify aspects of the lesion that we think will be relevant for prediction."],"metadata":{"id":"a0HUa0T_mErN"}},{"cell_type":"markdown","source":["We could generate hundreds of random features and hope that the machine learning model can figure out which ones are most important. We could also generate hundreds of random features and hope that a feature selection process can determine which features are more important before model training."],"metadata":{"id":"DLp7XtV7Z0rD"}},{"cell_type":"markdown","source":["However, having domain expertise about the problem we are trying to solve can save us significant time and effort while possibly leading to a more accurate model. In this particular case, we can use the ABCDE rule of dermatology. This rule is a handy tool that helps people visually identify potential signs of melanoma. It stands for:\n","\n","* **Asymmetry (A):** Melanomas are often asymmetric, meaning one half of the mole or lesion does not mirror the other half.\n","* **Border (B):** Melanomas typically have irregular, ragged, or blurred borders, rather than smooth and well-defined edges.\n","* **Color (C):** Melanomas often exhibit a variety of colors within the same lesion, such as different shades of brown, black, red, or blue.\n","* **Diameter (D):** Melanomas tend to be larger in size compared to benign moles. Although the exact threshold may vary, a diameter greater than 6 millimeters is often considered a warning sign.\n","* **Evolution (E):** Any significant change in size, shape, color, or texture of a mole or lesion over time should be closely monitored.\n","\n","We will only be looking a single image of each skin lesion, we will only be able to extract features representing the first four components of the rule."],"metadata":{"id":"txTEh8fNYCB5"}},{"cell_type":"markdown","source":["Below is a description of the specific values we are going to calculate for each rule component along with a brief summary of their intuition. There are multiple ways we could have formulated these features (e.g., average RGB color instead of HSV color), and some of these calculations are more advanced than others.\n","\n","Knowing how to translate human-interpretable rules to features is a difficult skill that comes with practice, exposure to a diverse toolbox of techniques, and a healthy amount of internet searching for code examples and academic papers. For now, the main takeaway should simply be that each feature was informed by a combination of domain expertise and knowledge about how to work with image data.\n","\n","| Rule Component | Feature | Explanation |\n","|------|:-----|:-----|\n","| Asymmetry (A) | The Hausdorff distance between the contour and a flipped version of it | The Hausdorff distance is a measure of similarity between two polygons |\n","| Border (B) | The ratio between the perimeter of the skin lesion and the perimeter of a smoothed version of the skin lesion's contour | The more jagged the contour, the more different it will be from the smoothed contour |\n","| Color (C) | The color of the skin lesion in HSV | Using HSV since it will extract more intuitive color components |\n","| Diameter (D) | The diameter of the minimum enclosing circle | We care about the widest line through the skin lesion |"],"metadata":{"id":"lg40ZwfVbTwj"}},{"cell_type":"markdown","source":["## Extracting the Features from a Single Image"],"metadata":{"id":"OGuqI2yadkun"}},{"cell_type":"markdown","source":["Properly segmenting a skin lesion from a image is difficult for multiple reasons:\n","* Skin lesions can be red, brown, black, or purple, so a single color filter won't suffice\n","* People can have different skin tones, so a dynamic brightness threshold wouldn't work either\n","* Hair can cover skin lesions and make it more difficult to accurately detect edges"],"metadata":{"id":"3AuuR-B8-RJm"}},{"cell_type":"markdown","source":["We are going to skip this step and rely on image annotations provided by the ISIC challenge organizers. These annotations indicate where the skin lesion is according to a binary image where white pixels belong to the skin lesion and black pixels correspond to everything else."],"metadata":{"id":"bKxZ3hG3BExA"}},{"cell_type":"code","source":["filename = 'ISIC_0000001'\n","\n","# Load an image and its corresponding annotation\n","rgb_filename = filename + '.jpg'\n","seg_filename = filename + '_Segmentation.png'\n","img = cv2.imread(os.path.join(image_folder, rgb_filename))\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","seg_img = cv2.imread(os.path.join(segementation_folder, seg_filename))\n","seg_img = cv2.cvtColor(seg_img, cv2.COLOR_BGR2GRAY)\n","\n","# Show the image with its annotation\n","plt.figure(figsize=(6, 3))\n","plt.subplot(1, 2, 1), plt.imshow(img), plt.title('RGB Image')\n","plt.subplot(1, 2, 2), plt.imshow(seg_img, cmap='gray'), plt.title('Annotation')\n","plt.show()"],"metadata":{"id":"sBJV1uzcBcOr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To extract the contour from the image on the right, all we need to do is call `cv2.findContours()` and return the first (and only) contour from the list:"],"metadata":{"id":"7oUh3HrCBdpn"}},{"cell_type":"code","source":["def extract_contour(seg_img):\n","    \"\"\"\n","    Extracts the lone contour from the image annotation\n","    seg_img: a binary image representing an annotation\n","    \"\"\"\n","    cnts, hierarchy = cv2.findContours(seg_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    return cnts[0]"],"metadata":{"id":"TgSG8O2e-aY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once we have a contour that defines where the skin lesion is within the correspnding color image, we can use a series of helper functions to extract our features. We will create one helper function for each component of the ABCD(E) rule. The features related strictly to the shape of the skin lesion only require the contour, while the features related to color require both the contour and the original color image."],"metadata":{"id":"spqXg4xfFH4P"}},{"cell_type":"markdown","source":["Again, you do not need to understand the exact mechanics of these calculations, as some are more complicated than you might expect. You should simply appreciate the fact that many of these helper functions use image processing techniques that we discussed earlier in the school year.\n","\n","If you get lost looking at these helper functions, jump straight to the next header."],"metadata":{"id":"TdWz_cT-1a-t"}},{"cell_type":"code","source":["from scipy.spatial.distance import cdist\n","\n","# Helper functions for calculating our custom asymmetry score\n","def rotate_contour(cnt, angle, center):\n","    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n","    rotated_contour = cv2.transform(cnt.reshape(-1, 1, 2), rotation_matrix).reshape(-1, 2).astype(int)\n","    return rotated_contour\n","\n","def flip_contour_horiz(cnt, width):\n","    return np.array([[[width - point[0][0], point[0][1]] for point in cnt]], dtype=np.int32)\n","\n","def get_hausdorff_distance(cnt1, cnt2):\n","    pts1 = np.array(cnt1).squeeze()\n","    pts2 = np.array(cnt2).squeeze()\n","    distances = cdist(pts1, pts2)\n","    return np.max(np.min(distances, axis=0))"],"metadata":{"id":"HzeNjrKl5umC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_asymmetry(img, cnt):\n","    \"\"\"\n","    Compute the asymmetry of the skin lesion by comparing the contour with a\n","    reflected version of itself\n","    img: the image of the skin lesion\n","    cnt: the contour of the skin lesion\n","    \"\"\"\n","    # Get the min enclosing ellipse\n","    center, axes, angle = cv2.fitEllipse(cnt)\n","\n","    # Rotate the contour so that it is upright\n","    rotated_cnt = rotate_contour(cnt, angle, center)\n","\n","    # Flip the contour horizontally\n","    flipped_rotated_cnt = flip_contour_horiz(cnt, img.shape[1])\n","\n","    distance = get_hausdorff_distance(rotated_cnt, flipped_rotated_cnt)\n","\n","    # Compute the diameter of the contour\n","    _, r = cv2.minEnclosingCircle(cnt)\n","    d = 2*r\n","\n","    # Compute the symmetry score\n","    return distance / d"],"metadata":{"id":"FNZOOcXsdISL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_border(cnt):\n","    \"\"\"\n","    Compute the jaggedness of the skin lesion's border by comparing the\n","    perimeter of the actual border to the perimeter of a smoothed version of it\n","    cnt: the contour of the skin lesion\n","    \"\"\"\n","    # Compute the perimeter\n","    perimeter = cv2.arcLength(cnt, True)\n","\n","    # Approximate the contour as a convex hull\n","    epsilon = 0.01 * perimeter\n","    approx = cv2.approxPolyDP(cnt, epsilon, True)\n","\n","    # Compute the perimeter of the convex hull\n","    simplified_perimeter = cv2.arcLength(approx, True)\n","\n","    # Return the ratio between the two\n","    return simplified_perimeter / perimeter"],"metadata":{"id":"nt_TUVgzc4gp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_color(img, cnt):\n","    \"\"\"\n","    Compute the average color of the skin lesion within the contour\n","    img: the image of the skin lesion\n","    cnt: the contour of the skin lesion\n","    \"\"\"\n","    # Convert the image to HSV\n","    hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n","\n","    # Recreate the binary mask using the contour\n","    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n","    cv2.drawContours(mask, [cnt], -1, (255), thickness=-1)\n","\n","    # Apply the mask to the image\n","    # TODO: see if this changes things and remove otherwise???\n","    masked_img = cv2.bitwise_and(img, img, mask=mask)\n","\n","    # Compute the average HSV color\n","    return cv2.mean(masked_img, mask=mask)[:3]"],"metadata":{"id":"ZOp5ASdMCMUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_diameter(cnt):\n","    \"\"\"\n","    Compute the radius of the skin lesion according to the min enclosing circle\n","    cnt: the contour of the skin lesion\n","    \"\"\"\n","    _, r = cv2.minEnclosingCircle(cnt)\n","    return 2*r"],"metadata":{"id":"Bnz0KZtDCQ_0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extract the Features from All Images"],"metadata":{"id":"2gLiPr2p6ymF"}},{"cell_type":"markdown","source":["Now that we have helper functions to calculate our features, let's put everything together into a single function. This function will take a single image as input and return all of the features calculated for that image as a `dict`."],"metadata":{"id":"jnI4rsL6YeAm"}},{"cell_type":"code","source":["def process_img(filename):\n","    \"\"\"\n","    Process a skin lesion image and produce all of the features according to\n","    the ABCD(E) rule as a dictionary (one value per key)\n","    filename: the name of the skin lesion image without the file extension\n","    \"\"\"\n","    # Get the contour filename\n","    rgb_filename = filename + '.jpg'\n","    seg_filename = filename + '_Segmentation.png'\n","\n","    # Get both of the images (RGB and segmentation annotation)\n","    img = cv2.imread(os.path.join(image_folder, rgb_filename))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    seg_img = cv2.imread(os.path.join(segementation_folder, seg_filename))\n","    seg_img = cv2.cvtColor(seg_img, cv2.COLOR_BGR2GRAY)\n","\n","    # Get the contour\n","    cnt = extract_contour(seg_img)\n","\n","    # Extract features from the image\n","    asymmetry = compute_asymmetry(img, cnt)\n","    border = compute_border(cnt)\n","    color = compute_color(img, cnt)\n","    diameter = compute_diameter(cnt)\n","\n","    # Combine everything into a feature vector\n","    feature_dict = {'Asymmetry': asymmetry,\n","                    'Border': border,\n","                    'Color (H)': color[0],\n","                    'Color (S)': color[1],\n","                    'Color (V)': color[2],\n","                    'Diameter': diameter}\n","    return feature_dict"],"metadata":{"id":"-6Egsxlfil5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test our function\n","filename = 'ISIC_0000000'\n","process_img(filename)"],"metadata":{"id":"6YynXU1C-J7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To get the features for all of the images in our dataset, we will iterate through all of the files and call our `process_image()` function on each image. We will gather the results in a single `DataFrame` that will hold all of our features. Running this will take some time since we have lots of images."],"metadata":{"id":"wXntyzHBC-tQ"}},{"cell_type":"code","source":["# Get all the filenames but remove the extension\n","img_filenames = os.listdir(image_folder)\n","img_filenames = sorted([f[:-4] for f in img_filenames])\n","\n","# Iterate through the filenames\n","features_df = pd.DataFrame()\n","for img_filename in img_filenames:\n","    # Generate the features\n","    feature_dict = process_img(img_filename)\n","\n","    # Add the image name\n","    feature_dict['Image Name'] = img_filename\n","    feature_df = pd.DataFrame([feature_dict])\n","    features_df = pd.concat([features_df, feature_df], axis=0)\n","\n","# Set the index to the image name\n","features_df.set_index(['Image Name'], inplace=True)\n","features_df"],"metadata":{"id":"vyt2LUL7i1BC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Grouping Features and Labels Together"],"metadata":{"id":"SY3s63jSnDGb"}},{"cell_type":"markdown","source":["To finalize our features and labels, we will combine `labels_df` and `features_df` into a single `DataFrame`. While this is an optional step, it will yield a few benefits:\n","1. We can export this `DataFrame` as a `.csv` to colleagues so that they can analyze the processed data for themselves\n","2. We can save this processed data for later so that we don't have to generate features from scratch\n","2. We can make sure the features and the labels are properly matched as we do our dataset splitting"],"metadata":{"id":"LWNCheijnJZS"}},{"cell_type":"markdown","source":["To ensure that everything lines up properly, we will combine `labels_df` and `features_df` so that the rows line up according to their index (the image name)."],"metadata":{"id":"sy_edmVyi-0x"}},{"cell_type":"code","source":["df = features_df.merge(labels_df, left_index=True, right_index=True)\n","df"],"metadata":{"id":"IGmw_RJsZXqg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 3: Decide How the Data Should Be Split for Training and Testing"],"metadata":{"id":"ryaahUysaOHD"}},{"cell_type":"markdown","source":["The organizers of the ISIC challenge technically provided separate datasets for model training and testing. However, we are going to make our own splits for the sake of practice."],"metadata":{"id":"TQA7ZLmsCAXb"}},{"cell_type":"markdown","source":["Since we only have one image per person, we can treat the images independently and do not have to worry about splitting our dataset in any fancy way. We are going to do a simple 80%-20% split where 80% of the images will be used for model training and the rest will be used for model testing. We can do this by using the `train_test_split()` function, which takes two input parameters:\n","1. **arrays:** A list, `numpy` array, or `DataFrame` containing our data\n","2. **test_size:** The fraction of the data that will be assigned to the test split"],"metadata":{"id":"7OyleQcjfJba"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_df, test_df = train_test_split(df, test_size=0.2)\n","print(f'Number of samples in train data: {len(train_df)}')\n","print(train_df.head())\n","print(f'Number of samples in test data: {len(test_df)}')\n","print(test_df.head())"],"metadata":{"id":"uFqD0FMLmobM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The splits are randomly decided, which means that we will get a different split each time we call this function. Randomization is important since we want to avoid fine-tuning our pipeline for a very specific configuration. However, randomization can also make it more difficult to debug our code since we won't be able to tell whether we are getting different results because of the randomness or because of changes we made."],"metadata":{"id":"jASpe_r7-1i1"}},{"cell_type":"markdown","source":["One way to avoid this issue is by setting a ***random seed*** — an arbitrarily selected number that controls how random numbers are generated. As we are building our pipeline, we can set the random seed so that we get the same results every time. Once we are confident that everything is working properly, we can \"turn off\" the random seed by setting it to `None`. We will keep the random seed set in this notebook so that everyone gets the same results when they get to the end."],"metadata":{"id":"k6EWrUy4BBbp"}},{"cell_type":"code","source":["# Set the random seed to an arbitrary number of your choosing\n","np.random.seed(42)\n","\n","# Rerunning this code will always have the same outcome\n","train_df, test_df = train_test_split(df, test_size=0.2)\n","print(f'Number of samples in train data: {len(train_df)}')\n","print(train_df.head())\n","print(f'Number of samples in test data: {len(test_df)}')\n","print(test_df.head())"],"metadata":{"id":"dtSB2DFjA_FG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once we have our train and test splits, we will separate our data back into features and labels since `scikit-learn` models require `numpy` arrays as input."],"metadata":{"id":"5nqH3xU-bH32"}},{"cell_type":"code","source":["x_train = train_df.drop('Label', axis=1).values\n","y_train = train_df['Label'].values\n","x_test = test_df.drop('Label', axis=1).values\n","y_test = test_df['Label'].values"],"metadata":{"id":"rECmsVbb8305"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: (Optional) Add Feature Selection"],"metadata":{"id":"gmMs1P1tabse"}},{"cell_type":"markdown","source":["Given that we only have a few features and they are informed by domain expertise, we are going to skip this step and assume that we have a reasonable set of features."],"metadata":{"id":"Qv-3GvQCcG1_"}},{"cell_type":"markdown","source":["# Step 5: (Optional) Balance Your Dataset"],"metadata":{"id":"90Fwfakplyu8"}},{"cell_type":"markdown","source":["We can check the balance of our dataset by looking at the frequency of values in our labels column. We can do this by checking either the `DataFrames` or the `numpy` arrays:"],"metadata":{"id":"2a3BVu0lm6wD"}},{"cell_type":"code","source":["def print_label_dist(y):\n","    \"\"\"\n","    Prints out the balance between positive and negative samples\n","    y: a 1D array of labels\n","    \"\"\"\n","    num_neg = np.count_nonzero(y == 0)\n","    num_pos = np.count_nonzero(y == 1)\n","    print(f'Number of benign samples: {num_neg}')\n","    print(f'Number of malignant samples: {num_pos}')\n","    print(f'Fraction of positive samples: {num_pos/(num_pos+num_neg):0.2f}')"],"metadata":{"id":"9GiFmcuIsMWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_label_dist(df['Label'].values)"],"metadata":{"id":"kEgoSbL7q83R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that we have many more benign cases than we do malingant ones in our overall dataset. This imbalance trickles down once we split our data into train and test sets."],"metadata":{"id":"CqHeGBAeryyN"}},{"cell_type":"code","source":["print_label_dist(y_train)"],"metadata":{"id":"1VMYNfIAr-dt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_label_dist(y_test)"],"metadata":{"id":"4sAHQmA-su9d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For now, we are going to leave this undisturbed, but we will revisit this issue the next time we look at this model."],"metadata":{"id":"SHfP4CDpl5RH"}},{"cell_type":"markdown","source":["# Step 6: Select an Appropriate Model"],"metadata":{"id":"TUuv3wRJaVer"}},{"cell_type":"markdown","source":["`scikit-learn` provides numerous classification model architectures with their own advantages and disadvantages. For now, we are going to stick with a ***random forest classifier***, which uses a collection of decision trees to make a decision."],"metadata":{"id":"IiS0RhPymQwg"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier()"],"metadata":{"id":"-ZejuzTe8X3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 7: (Optional) Select Your Hyperparameters"],"metadata":{"id":"A-AcODJ3aYx6"}},{"cell_type":"markdown","source":["For now, we are going to stick with the default hyperparameters for our model."],"metadata":{"id":"5J5xnuJkC3Lp"}},{"cell_type":"markdown","source":["# Step 8: Train and Test Your Model"],"metadata":{"id":"-fpM5JCVcS_c"}},{"cell_type":"markdown","source":["We are finally ready to train our machine learning model! All we need to do is call the `.fit()` method while providing the features and labels from our training dataset. Underneath the hood, the model will adjust its underlying parameters and decision boundaries in order to optimize its performance on that data."],"metadata":{"id":"fndMkb0u7_-p"}},{"cell_type":"code","source":["clf.fit(x_train, y_train)"],"metadata":{"id":"i4DkQNLRDH3r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once we've trained the model, we can see how the model would classify a set of features by calling the `.predict()` method. People often forgo generating predictions for the training dataset since it will not give us a real indication of how the model will perform on previously unseen data. However, we will generate predictions for both our training and test data so that we can compare the model accuracy on both sets."],"metadata":{"id":"7CqgEKuyDsYC"}},{"cell_type":"code","source":["y_train_pred = clf.predict(x_train)\n","y_test_pred = clf.predict(x_test)"],"metadata":{"id":"LZBGUjVnDbce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To plot an ROC curve, we actually need to generate our predictions a slightly different way. Rather than generating binary predictions (`0` or `1`), we will need to generate probabilistic predictions that indicate the likelihood that the given sample belongs to the positive class."],"metadata":{"id":"PKAqvdEyfS9o"}},{"cell_type":"markdown","source":["We can do this by calling `.predict_proba()` instead of `.predict()` on our model. This function produces an $n \\times c$ array. $n$ is the number of samples, and $c$ is the number of classes we have in our dataset. The number in row $n_i$ and column $c_j$ indicates the likelihood that sample $i$ belongs to class $j$ according to our model."],"metadata":{"id":"rhLsp_xfiaDB"}},{"cell_type":"markdown","source":["Let's compare the output of `.predict()` and `.predict_proba()` on a subset of our training dataset features:"],"metadata":{"id":"PuVEa-GRh3F7"}},{"cell_type":"code","source":["y_binary = clf.predict(x_train[:10, :])\n","y_prob = clf.predict_proba(x_train[:10, :])\n","print(y_binary)\n","print(y_prob)"],"metadata":{"id":"a7ZExfdEfvjg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that only the 7th row has a `1` in the output of `.predict()`, indicating that it was the only sample that was predicted to be malignant out of the 10 samples. The corresponding row in the output of `predict_proba()` is the only one where the value on the left is less than the value on the right, reflecting the fact that the model was more confident that the sample was malignant instead of benign."],"metadata":{"id":"IxTBl10ilXqH"}},{"cell_type":"markdown","source":["Since we only have two classes, we are simply going to save the rightmost column, which indicates the likelihood that each sample belongs to the positive class (`malignant`) according to our model."],"metadata":{"id":"QgADZEG9i1dn"}},{"cell_type":"code","source":["y_train_pred_prob = clf.predict_proba(x_train)[:, 1]\n","y_test_pred_prob = clf.predict_proba(x_test)[:, 1]"],"metadata":{"id":"tOESgRqUi8ge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 9: Use an Appropriate Method for Interpreting Results"],"metadata":{"id":"hT0cnOFdafEF"}},{"cell_type":"markdown","source":["Now that we have predictions, we will examine a variety of metrics to see how well our model performed. Most of the functions we will discuss in this section require two inputs:\n","1. **y_true:** The known ground-truth labels from our dataset\n","2. **y_pred:** The labels predicted from the model"],"metadata":{"id":"zj0apvZTbn5d"}},{"cell_type":"markdown","source":["We will start by examining how well our model worked on the training dataset, after which we will revisit them for our test dataset."],"metadata":{"id":"XNg-2m-LO0S0"}},{"cell_type":"markdown","source":["## Confusion Matrix"],"metadata":{"id":"WwE9unOpIbm2"}},{"cell_type":"markdown","source":["We can manually generate and save a confusion matrix using the `confusion_matrix()` function:"],"metadata":{"id":"SMYizHnQNcbl"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","# Generate the confusion matrix\n","cm = confusion_matrix(y_train, y_train_pred)\n","print(cm)\n","\n","# Split the confusion matrix according to decision outcomes\n","tn = cm[0][0]\n","fp = cm[0][1]\n","fn = cm[1][0]\n","tp = cm[1][1]\n","print(f'True positives: {tp}')\n","print(f'True negatives: {tn}')\n","print(f'False positives: {fp}')\n","print(f'False negatives: {fn}')"],"metadata":{"id":"6ttSfs4qNgg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, most people prefer to generate a figure that shows the visualization since that's what ends up getting put into a paper or report. `scikit-learn` provides a handy class called `ConfusionMatrixDisplay` for creating such a visualization."],"metadata":{"id":"6wDANVl5LyQW"}},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay\n","classes = ['benign', 'malignant']\n","ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n","                                        display_labels=classes)\n","plt.show()"],"metadata":{"id":"f7nOnbhUMBZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Classification Accuracy Rates"],"metadata":{"id":"e87qpxdPIsga"}},{"cell_type":"markdown","source":["The `metrics` module provides numerous functions you can call to calcluate various scores. A few examples are provided below:"],"metadata":{"id":"zvZqiWh9bU7m"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","print(f'Accuracy: {accuracy_score(y_train, y_train_pred)}')\n","print(f'F1 Score: {f1_score(y_train, y_train_pred)}')\n","print(f'Precision: {precision_score(y_train, y_train_pred)}')\n","print(f'Recall: {recall_score(y_train, y_train_pred)}')"],"metadata":{"id":"U4JQn0MdbmdZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfortunately, it does not provide functions for calculating sensitivity and specificity, which are commonly used in medical applications. However, we can calculate these ourselves using the entries of the confusion matrix:"],"metadata":{"id":"_hXQscbbbBCb"}},{"cell_type":"code","source":["# Get the confusion matrix\n","cm = confusion_matrix(y_train, y_train_pred)\n","tn = cm[0][0]\n","fp = cm[0][1]\n","fn = cm[1][0]\n","tp = cm[1][1]\n","\n","# Calculate sensitivity and specificity\n","sens = tp / (tp+fn)\n","spec = tn / (tn+fp)\n","print(f'Sensitivity: {sens}')\n","print(f'Specificity: {spec}')"],"metadata":{"id":"kV6Ft-Q5cQWp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`classification_report()` provides a quick printout of commonly used classification accuracy metrics. This table breaks down the performance across the individual classes, which can be useful for determining whether your model is performing better for one class versus another. This table breaks down what each of the numbers represents:\n","\n","| |Precision|Recall|F1-Score|Support|\n","|-----|-----|-----|-----|-----|\n","| **Benign** | Precision if it considered `benign` to be the positive label | Recall if it considered `benign` to be considered the positive label | F1-score if it considered `benign` to be considered the positive label | The number of examples that were labeled `benign` |\n","| **Malignant** | Precision if it considered `malignant` to be the positive label | Recall if it considered `malignant` to be considered the positive label | F1-score if it considered `malignant` to be considered the positive label | The number of examples that were labeled `malignant` |\n","| **Accuracy** | | | The F1-score across the entire dataset | The nubmer of examples in the entire dataset |\n","| **Macro avg** | The unweighted average precision across both classes | The unweighted average recall across both classes | The unweighted average F1-score across both classes | The nubmer of examples in the entire dataset |\n","| **Weighted avg** | The weighted average precision across both classes | The weighted average recall across both classes | The weighted average F1-score across both classes | The nubmer of examples in the entire dataset |\n"],"metadata":{"id":"ZMtDXPjuJT89"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","print(classification_report(y_train, y_train_pred, target_names=classes))"],"metadata":{"id":"4CqItIfgdVfa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ROC Curve"],"metadata":{"id":"iikCuJ_eeHUj"}},{"cell_type":"markdown","source":["We can generate the raw data for an ROC curve using `roc_curve()` and the area under the curve (AUC) using `auc_roc_score()`. Instead of providing the predicted binary labels from our model, we will need to provide the predicted likelihood scores that were output by `.predict_proba()`."],"metadata":{"id":"mnTIpaN-f6To"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, roc_auc_score\n","fpr, tpr, thresholds = roc_curve(y_train, y_train_pred_prob)\n","auc = roc_auc_score(y_train, y_train_pred_prob)\n","print(f'AUC: {auc}')"],"metadata":{"id":"MqZFsJvff_l8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Similar to what we said about confusion matrices, however, `scikit-learn` provides a handy function for generating an ROC curve visualization for us."],"metadata":{"id":"6BmYqGIStnB5"}},{"cell_type":"code","source":["from sklearn.metrics import RocCurveDisplay\n","RocCurveDisplay.from_predictions(y_train, y_train_pred_prob)\n","plt.show()"],"metadata":{"id":"CpqrSRthoqyM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Comparing Performance on Train and Test Data"],"metadata":{"id":"uvtSJXQxeJE9"}},{"cell_type":"markdown","source":["Let's create a function that will generate a detailed classification accuracy report combining a subset of the aforementioned metrics and visualizations:"],"metadata":{"id":"dOvMA31Y8EYv"}},{"cell_type":"code","source":["def classification_evaluation(y_true, y_pred, y_pred_prob):\n","    \"\"\"\n","    Generate a series of graphs that will help us determine the performance of\n","    a binary classifier model\n","    y_true: the target binary labels\n","    y_pred: the predicted binary labels\n","    y_pred_prob: the predicted likelihood scores for a positive label\n","    \"\"\"\n","    # Calculate f1 score, sensitivity, and specificity\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn = cm[0][0]\n","    fp = cm[0][1]\n","    fn = cm[1][0]\n","    tp = cm[1][1]\n","    f1 = f1_score(y_true, y_pred)\n","    sens = tp / (tp+fn)\n","    spec = tn / (tn+fp)\n","\n","    # Generate the confusion matrix\n","    cm_title = f'Confusion Matrix \\n(Sensitivity: {sens:0.2f}, Specificity: {spec:0.2f})'\n","    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=classes)\n","    plt.title(cm_title)\n","    plt.show()\n","\n","    # Display the ROC curve\n","    roc_title = f'ROC Curve (F1 score: {f1:0.2f})'\n","    RocCurveDisplay.from_predictions(y_true, y_pred_prob)\n","    plt.title(roc_title)\n","    plt.show()"],"metadata":{"id":"b1LxWw82eXkj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's run this function on both our train and test predictions to see the disparity in performance:"],"metadata":{"id":"PeRsPC3cjgwu"}},{"cell_type":"code","source":["classification_evaluation(y_train, y_train_pred, y_train_pred_prob)"],"metadata":{"id":"A-eEyZjjjl_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classification_evaluation(y_test, y_test_pred, y_test_pred_prob)"],"metadata":{"id":"DnvfgKLRjrCO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So what can we learn from all of these results:\n","* As we saw earlier, our dataset has many more benign cases than it does malignant cases. This imbalance could bias our model to assume that skin lesions are benign.\n","* Our model achieved perfect accuracy on our training dataset, which means that the model was able to learn something useful from the features we provided.\n","* However, our model did not perform so well on the test dataset, achieving a poor sensitivity and F1 score. Although we achieved a high specificity score, but that was because almost all of the test cases were predicted to be benign. This confirms our suspicion that the model may be biased."],"metadata":{"id":"73HEAeGwqoEr"}},{"cell_type":"markdown","source":["To summarize, our model was able to learn from the features we provided, but it did not generalize to the unseen test dataset. We are going to revisit this machine learning pipeline in a later session to see how we can improve its ability to generalize to new data."],"metadata":{"id":"Wkt4YdgS_PJK"}}]}