{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DekAnzq1b82b",
        "xP4sP_ekZxqW",
        "dF7vGVljaKp7",
        "gE3z6tk2wHIa",
        "ryaahUysaOHD",
        "Ft8ymn0J0Io4",
        "AUL_8Ns90M6f",
        "TUuv3wRJaVer",
        "A-AcODJ3aYx6",
        "gmMs1P1tabse",
        "hT0cnOFdafEF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to revisit the regression model we trained to predict the UPDRS score of patients with Parkinson's disease from their gait. Most of the code in this notebook is copied from the previous session, but we will use the following techniques to boost the model's performance:\n",
        "* Imputation of missing data\n",
        "* Normalizing the feature distributions\n",
        "* Feature selection\n",
        "\n"
      ],
      "metadata": {
        "id": "BDaO9cuMWe83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important: Run this code cell each time you start a new session!"
      ],
      "metadata": {
        "id": "DekAnzq1b82b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install os\n",
        "!pip install librosa\n",
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import librosa\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "jrO0X1ZMxMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -rNcnp https://physionet.org/files/gaitpdb/1.0.0/"
      ],
      "metadata": {
        "id": "bASFcdZNZuGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Define the Problem You Are Trying to Solve"
      ],
      "metadata": {
        "id": "xP4sP_ekZxqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, our overarching goal is to generate a regression model that predicts a person's UPDRSM score based on characteristics of their gait."
      ],
      "metadata": {
        "id": "RAtrOjflWIga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The relevant folders and files associated with this dataset\n",
        "base_folder = os.path.join('physionet.org', 'files', 'gaitpdb', '1.0.0')\n",
        "label_filename = os.path.join(base_folder, 'demographics.xls')"
      ],
      "metadata": {
        "id": "85ItYs_7gYDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The names of the columns in the recordings\n",
        "column_names = ['Time']\n",
        "for i in range(1, 9):\n",
        "    column_names.append(f'Left Sensor {i}')\n",
        "for i in range(1, 9):\n",
        "    column_names.append(f'Right Sensor {i}')\n",
        "column_names.append('Left Foot')\n",
        "column_names.append('Right Foot')"
      ],
      "metadata": {
        "id": "tjO9wyZHMg9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the structure of one of the files\n",
        "example_filename = 'GaCo01_01.txt'\n",
        "example_df = pd.read_csv(os.path.join(base_folder, example_filename),\n",
        "                         sep=\"\\t\", header=None, names=column_names)\n",
        "example_df"
      ],
      "metadata": {
        "id": "nlbjx-aIDbFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the data\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(example_df['Time'], example_df['Left Foot'], 'k-', label='Left')\n",
        "plt.xlabel('Time (s)'), plt.ylabel('VGRF (N)'), plt.title('Entire Recording, Single Foot')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(example_df['Time'], example_df['Left Foot'], 'k-', label='Left')\n",
        "plt.plot(example_df['Time'], example_df['Right Foot'], 'r-', label='Right')\n",
        "plt.xlabel('Time (s)'), plt.ylabel('VGRF (N)'), plt.title('Short Snippet, Both Feet')\n",
        "plt.xlim(0, 5)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SdmFJoR5ko5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create Your Features and Labels"
      ],
      "metadata": {
        "id": "dF7vGVljaKp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to keep our labels and features the same as before. However, we are going to dive deeper into the availability and the distribution of our data to address some of its limitations."
      ],
      "metadata": {
        "id": "1O2DKWI7lpOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the code from the previous session is copied below, so refer to that notebook if you need a reminder of how we came up with these code blocks."
      ],
      "metadata": {
        "id": "YpETvdIBS1hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_arbitrary_time_domain_metrics(times, values, fs=100):\n",
        "    \"\"\"\n",
        "    Calculates generic time-domain statistics on the signal\n",
        "    times: the times associated with the VGRF data\n",
        "    values: the VGRF data\n",
        "    fs: the sampling rate\n",
        "    \"\"\"\n",
        "    return {'average': np.mean(values),\n",
        "            'stdev': np.std(values),\n",
        "            '95th percentile': np.percentile(values, 95),\n",
        "            'rms': np.sqrt(np.mean(values**2))}"
      ],
      "metadata": {
        "id": "gtMjoTpsulKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.fft import fftfreq\n",
        "from scipy.fftpack import fft\n",
        "from scipy import signal\n",
        "def compute_arbitrary_freq_domain_metrics(times, values, fs=100):\n",
        "    \"\"\"\n",
        "    Calculates generic frequency-domain statistics on the signal\n",
        "    times: the times associated with the VGRF data\n",
        "    values: the VGRF data\n",
        "    fs: the sampling rate\n",
        "    \"\"\"\n",
        "    # Calculate the FFT\n",
        "    values_centered = values - values.mean()\n",
        "    fft_mag = np.abs(fft(values_centered))\n",
        "    fft_freqs = fftfreq(len(values_centered), 1/fs)\n",
        "\n",
        "    # Calculate the indices relevant to our frequency bands of interest\n",
        "    low_indices = np.where((fft_freqs >= 0) & (fft_freqs <= 3))\n",
        "    high_indices = np.where((fft_freqs >= 3) & (fft_freqs <= 8))\n",
        "\n",
        "    # Calculate the power at the low and high frequencies\n",
        "    low_power = np.sum(fft_mag[low_indices]**2)\n",
        "    high_power = np.sum(fft_mag[high_indices]**2)\n",
        "\n",
        "    # Calculate the power within the frequency range\n",
        "    high_to_low_ratio = 10*np.log10(high_power / low_power)\n",
        "    return {'power at low freqs': low_power,\n",
        "            'power at high freqs': high_power,\n",
        "            'high-to-low power ratio': high_to_low_ratio}"
      ],
      "metadata": {
        "id": "sy1KLrzRvBzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_amplitude_metrics(times, values, fs=100):\n",
        "    \"\"\"\n",
        "    Calculate metrics related to the transient amplitude of the signal over time\n",
        "    using a 5-second window with 0% overlap\n",
        "    times: the times associated with the VGRF data\n",
        "    values: the VGRF data\n",
        "    fs: the sampling rate\n",
        "    \"\"\"\n",
        "    # Set the sliding window parameters\n",
        "    window_width = 5\n",
        "    start_time = 0\n",
        "    end_time = window_width\n",
        "    sample_period = 1/fs\n",
        "    middle_idx = int((window_width / sample_period) // 2)\n",
        "\n",
        "    # Stop generating windows it would go past the end of the signal\n",
        "    window_amplitudes = []\n",
        "    while end_time < times.max():\n",
        "        # Grab the current window by filtering indexes according to time\n",
        "        window_idxs = (times >= start_time) & (times <= end_time)\n",
        "        window_values = values[window_idxs]\n",
        "\n",
        "        # Calculate the amplitude\n",
        "        window_rms = np.sqrt(np.mean(window_values**2))\n",
        "        window_amplitudes.append(window_rms)\n",
        "\n",
        "        # Move the window over by a stride\n",
        "        start_time += window_width\n",
        "        end_time += window_width\n",
        "\n",
        "    # Summarize the amplitude over time\n",
        "    return {'average amplitude': np.mean(window_amplitudes),\n",
        "            'stdev amplitude': np.std(window_amplitudes)}"
      ],
      "metadata": {
        "id": "FNZOOcXsdISL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cadence_metrics(times, values, fs=100):\n",
        "    \"\"\"\n",
        "    Calculate metrics related to the transient peak frequency of the signal\n",
        "    over time\n",
        "    times: the times associated with the VGRF data\n",
        "    values: the VGRF data\n",
        "    fs: the sampling rate\n",
        "    \"\"\"\n",
        "    # Calculate the spectrogram\n",
        "    values_centered = values - values.mean()\n",
        "    spec_freqs, spec_times, spectro = signal.spectrogram(values_centered, fs)\n",
        "\n",
        "    # Find the largest bin along the frequency dimension\n",
        "    dominant_bins = np.argmax(spectro, axis=0)\n",
        "\n",
        "    # Map those bin indeces to frequencies\n",
        "    peak_freqs = spec_freqs[dominant_bins]\n",
        "\n",
        "    # Summarize the step rate over time\n",
        "    return {'average cadence': np.mean(peak_freqs),\n",
        "            'stdev cadence': np.std(peak_freqs)}"
      ],
      "metadata": {
        "id": "XXS1hhmcq7MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_differences(left, right):\n",
        "    \"\"\"\n",
        "    Compares corresponding metrics across two feet\n",
        "    left: the dictionary of metrics from the left side\n",
        "    right: the dictionary of metrics from the right side\n",
        "    \"\"\"\n",
        "    diffs_dict = {}\n",
        "    for key in left:\n",
        "        diffs_dict[key] = np.abs(left[key] - right[key])\n",
        "    return diffs_dict"
      ],
      "metadata": {
        "id": "3Hicu3U1rD28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_recording(filename):\n",
        "    \"\"\"\n",
        "    Process a VGRF recording and produce all of the features as a dictionary\n",
        "    (one value per key)\n",
        "    filename: the name of the recording file\n",
        "    \"\"\"\n",
        "    # Get the useful columns\n",
        "    df = pd.read_csv(os.path.join(base_folder, filename),\n",
        "                     sep=\"\\t\", header=None, names=column_names)\n",
        "    time = df['Time'].values\n",
        "    left_values = df['Left Foot'].values\n",
        "    right_values = df['Right Foot'].values\n",
        "\n",
        "    # Extract metrics from the left side\n",
        "    left_time = compute_arbitrary_time_domain_metrics(time, left_values)\n",
        "    left_freq = compute_arbitrary_freq_domain_metrics(time, left_values)\n",
        "    left_amplitude = compute_amplitude_metrics(time, left_values)\n",
        "    left_cadence = compute_cadence_metrics(time, left_values)\n",
        "\n",
        "    # Extract metrics from the right side\n",
        "    right_time = compute_arbitrary_time_domain_metrics(time, right_values)\n",
        "    right_freq = compute_arbitrary_freq_domain_metrics(time, right_values)\n",
        "    right_amplitude = compute_amplitude_metrics(time, right_values)\n",
        "    right_cadence = compute_cadence_metrics(time, right_values)\n",
        "\n",
        "    # Extract difference metrics\n",
        "    diff_time = compute_differences(left_time, right_time)\n",
        "    diff_freq = compute_differences(left_freq, right_freq)\n",
        "    diff_amplitude = compute_differences(left_amplitude, right_amplitude)\n",
        "    diff_cadence = compute_differences(left_cadence, right_cadence)\n",
        "\n",
        "    # Combine everything into a dictionary\n",
        "    feature_dict = {}\n",
        "    for left_dict in [left_time, left_freq, left_amplitude, left_cadence]:\n",
        "        for key in left_dict:\n",
        "            feature_dict['Single foot ' + key] = left_dict[key]\n",
        "    for diff_dict in [diff_time, diff_freq, diff_amplitude, diff_cadence]:\n",
        "        for key in diff_dict:\n",
        "            feature_dict['Difference ' + key] = diff_dict[key]\n",
        "    return feature_dict"
      ],
      "metadata": {
        "id": "-6Egsxlfil5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_filenames = os.listdir(base_folder)\n",
        "\n",
        "# Iterate through the filenames\n",
        "features_df = pd.DataFrame()\n",
        "for data_filename in data_filenames:\n",
        "    # Skip the file if we want to ignore it\n",
        "    patient_name = data_filename[0:6]\n",
        "    patient_type = data_filename[2:4]\n",
        "    trial_id = data_filename[7:9]\n",
        "    if (patient_type == 'Co') or (trial_id == '10') or not ('_' in data_filename):\n",
        "        continue\n",
        "\n",
        "    # Generate the features\n",
        "    feature_dict = process_recording(data_filename)\n",
        "\n",
        "    # Add the patient's name as the identifier\n",
        "    feature_dict['ID'] = patient_name\n",
        "    feature_dict = pd.DataFrame([feature_dict])\n",
        "    features_df = pd.concat([features_df, feature_dict], axis=0)\n",
        "\n",
        "# Set the index to the image name\n",
        "features_df.set_index(['ID'], inplace=True)\n",
        "features_df"
      ],
      "metadata": {
        "id": "vyt2LUL7i1BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_df = pd.read_excel(label_filename, index_col='ID')\n",
        "labels_df"
      ],
      "metadata": {
        "id": "Xkr2TodFhmN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only patient data\n",
        "labels_df = labels_df[labels_df['Group'] == 'PD']\n",
        "\n",
        "# Get rid of unnecessary columns\n",
        "labels_df = labels_df[['Gender', 'Age', 'Height (meters)',\n",
        "                       'Weight (kg)', 'UPDRSM']]\n",
        "\n",
        "# Rename the columns\n",
        "labels_df.rename(columns={'Gender': 'Sex', 'Height (meters)': 'Height',\n",
        "                          'Weight (kg)': 'Weight', 'UPDRSM': 'Label'}, inplace=True)\n",
        "labels_df"
      ],
      "metadata": {
        "id": "2e_PIVlqk6CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert gender to a binary sex variable\n",
        "labels_df['Sex'] = labels_df['Sex'].replace({'male': 0, 'female': 1})\n",
        "labels_df"
      ],
      "metadata": {
        "id": "QYP272-ll4Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the Ju rows so that the height is in meters\n",
        "bad_height_rows = labels_df.index.str.startswith('Ju')\n",
        "labels_df.loc[bad_height_rows, 'Height'] /= 100\n",
        "labels_df"
      ],
      "metadata": {
        "id": "t5ByBmhOlYJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.merge(labels_df, features_df, how='right', left_index=True, right_index=True)\n",
        "df"
      ],
      "metadata": {
        "id": "IGmw_RJsZXqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Feature Values"
      ],
      "metadata": {
        "id": "VCYt6n4TwFmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we have missing entries for some people's UPDRS scores and demographics. We can identify how many rows have missing values in these columns by looking at the \"count\" row that results from calling the `.describe()` method:"
      ],
      "metadata": {
        "id": "JEgofpKUxXeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "TAm0UxbZspIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will still exclude the patients who did not complete the UPDRS because there is no use in trying to guess what their scores would have been had they completed the assessment."
      ],
      "metadata": {
        "id": "gZqST4if9N3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with missing data\n",
        "df = df[~pd.isna(df['Label'])]\n",
        "df"
      ],
      "metadata": {
        "id": "otwoDkcJSan7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will fill in the missing demographic variables with reasonable guesses so that we have access to more data during model training. `scikit-learn` provides a class called `SimpleImputer` that generates reasonable guesses based on the distribution of known values according to a `strategy` like mean or mode. For example, we could fill in missing weights by calculating the average of the weights reported in our dataset."
      ],
      "metadata": {
        "id": "ihXo3knt9xYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
      ],
      "metadata": {
        "id": "BDoRT_rtB-Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because this imputer requires learning from our dataset, it is important that we treat similarly to our machine learning model. In other words, we should only determine the distribution of known values in our training dataset; otherwise, we would be cheating by inspecting our test data before the final evaluation step. Since we are doing k-fold cross-validation, we will add this imputer in Step 8 of this notebook."
      ],
      "metadata": {
        "id": "JI7sPf7EDQkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling Feature Values"
      ],
      "metadata": {
        "id": "gE3z6tk2wHIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look at the output of the `.describe()` method one more time, you will notice that each column has a very different range."
      ],
      "metadata": {
        "id": "DLsif3JWskng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "5_XVtAFwCjx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some models are inherently robust to feature scaling, such as decision trees and random forests. These models make decisions based on threshold rules, so the scale of one feature does not impact the threshold that is optimal for another feature. Models that rely on distance metrics (e.g., k-nearest neighbors) or linear combinations of features (e.g., linear regression, SVM) can be more sensitive to feature scaling, as features with larger scales may dominate the calculations underlying these models."
      ],
      "metadata": {
        "id": "hQTsPtu5uE8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While there are ways of making these models account for features with varied scaling, normalizing the scale of our features can prevent this issue and allow models to give appropriate weight to each feature. We will use the `StandardScaler` from `scikit-learn`. This class scales each feature independently such that they look like standard normally distributed data (i.e., Gaussian with 0 mean and unit variance)."
      ],
      "metadata": {
        "id": "RUg4xwVpwMFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "lSIvuqLNwF5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like our data-driven imputer for filling in missing values, this feature scaling step depends on the distribution of known values in our training dataset. Therefore, we will incorporate this scaler in Step 8 of this notebook."
      ],
      "metadata": {
        "id": "F0ToTh4TtEfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Decide How the Data Should Be Split for Training and Testing"
      ],
      "metadata": {
        "id": "ryaahUysaOHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train our model as before, using `GroupKFold` to split our data into 5 folds in a way that keeps samples associated with the same `ID` in the same fold."
      ],
      "metadata": {
        "id": "n0hb8m7jfSpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: (Optional) Add Feature Selection"
      ],
      "metadata": {
        "id": "Ft8ymn0J0Io4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We intentionally created some features at random and some features inspired by domain expertise. While the number of features we have is still significantly smaller than the number of recordings, fewer features could prevent overfitting and lead to better test accuracy."
      ],
      "metadata": {
        "id": "TI3mp5UWzItT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can select features in a variety of ways. We can remove features that are redundant, we can pick features that have a certain amount variance, or we could even pick features according to a second machine learning model."
      ],
      "metadata": {
        "id": "8miBICLvyZMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, we will use the `SelectKBest` class from `scikit-learn`. This feature selector removes all but the top-`k` best features according to a scoring function (`score_func`). `scikit-learn` provides multiple scoring functions for classification and regression tasks, each with their own mathematical underpininngs. It is important to note that the default scoring function is for classification tasks, so we will need to specify a different one. We will use the `f_regression` score, which determines the utility of each feature according to the F-statistic of univariate linear regression tests."
      ],
      "metadata": {
        "id": "JUYBncY9mjMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this particular feature selector, we also need to carefully consider the number of features we keep. We could treat this number like a model hyperparameter and try different settings to identify the optimal one. For now, however, we will fix this hyperparameter to `k=5`."
      ],
      "metadata": {
        "id": "aOINNbQel3y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "feat_select = SelectKBest(f_regression, k=5)"
      ],
      "metadata": {
        "id": "dzZESz7Yt00h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with the other feature pre-processing steps we have added, feature selection depends on the distribution of known values in our training dataset. Therefore, we will incorporate this feature selector in Step 8 of this notebook."
      ],
      "metadata": {
        "id": "jaEJ_fm3uapf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: (Optional) Balance Your Dataset"
      ],
      "metadata": {
        "id": "AUL_8Ns90M6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw that the distribution in UPDRSM scores was fairly normally distributed, so we won't worry about trying to balance our dataset."
      ],
      "metadata": {
        "id": "EECBO7dX1nMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Select an Appropriate Model"
      ],
      "metadata": {
        "id": "TUuv3wRJaVer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the same k-nearest neighbors regressor as before."
      ],
      "metadata": {
        "id": "IiS0RhPymQwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "reg = KNeighborsRegressor()"
      ],
      "metadata": {
        "id": "-ZejuzTe8X3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: (Optional) Select Your Hyperparameters"
      ],
      "metadata": {
        "id": "A-AcODJ3aYx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could optimize the hyperparameter `n_neighbors` in our regressor; however, we will skip that step since we already have already made many other changes in our pipeline."
      ],
      "metadata": {
        "id": "u5UTDgY31cQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Train and Test Your Model"
      ],
      "metadata": {
        "id": "gmMs1P1tabse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train and test our model as we have in the past, but with three key differences:\n",
        "1. We will fill in missing demographic values\n",
        "2. We will scale our features to the same distribution\n",
        "3. We will apply feature selection to identify the most informative features"
      ],
      "metadata": {
        "id": "nZLFduix1Dgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logic of putting imputation first is that the other steps are easiest to work with when the dataset is complete. Since we don't have too much missing data, it is also unlikely that filling in missing values will completely change the scale or utility of those features.\n",
        "\n",
        "The logic of putting feature scaling before feature selection is that the model will be trained on scaled features rather unscaled ones. Therefore, we should pick which scaled features are most useful.\n",
        "\n",
        "Depending on how your model pipeline is configured and the type of machine learning model you are using, you might find that a different order may make more sense."
      ],
      "metadata": {
        "id": "mTK1WGmfvwj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# Get the features, labels, and grouping variables\n",
        "x = df.drop(['Label'], axis=1).values\n",
        "y = df['Label'].values\n",
        "groups = df.index.values\n",
        "\n",
        "# Initialize a data structure to save our final results,\n",
        "# assuming all of the predictions are 0 to start\n",
        "y_pred = np.zeros(y.shape)\n",
        "\n",
        "# Split the data into folds\n",
        "group_kfold = GroupKFold(n_splits=5)\n",
        "for train_idxs, test_idxs in group_kfold.split(x, y, groups):\n",
        "    # Split the data into train and test\n",
        "    x_train = x[train_idxs]\n",
        "    y_train = y[train_idxs]\n",
        "    x_test = x[test_idxs]\n",
        "    y_test = y[test_idxs]\n",
        "\n",
        "    # Impute the missing features\n",
        "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    imputer.fit(x_train)\n",
        "    x_train = imputer.transform(x_train)\n",
        "    x_test = imputer.transform(x_test)\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(x_train)\n",
        "    x_train = scaler.transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    # Select the most useful features\n",
        "    feat_select = SelectKBest(f_regression, k=5)\n",
        "    feat_select.fit(x_train, y_train)\n",
        "    x_train = feat_select.transform(x_train)\n",
        "    x_test = feat_select.transform(x_test)\n",
        "\n",
        "    # Train a model on the transformed training data\n",
        "    reg = KNeighborsRegressor(n_neighbors=5)\n",
        "    reg.fit(x_train, y_train)\n",
        "\n",
        "    # Predict on the transformed test data\n",
        "    y_test_pred = reg.predict(x_test)\n",
        "    y_pred[test_idxs] = y_test_pred"
      ],
      "metadata": {
        "id": "zUO20hcDKr2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an aside, recall that using cross-validation means that we are training distinct models for each fold of our dataset. Because we are doing feature selection within each fold, we could easily end up selecting different features for each model. This could have implications for how we communicate our results, but since we are focusing on optimizing performance, we will leave things as they are."
      ],
      "metadata": {
        "id": "uUHIs_mcwV_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Use an Appropriate Method for Interpreting Results"
      ],
      "metadata": {
        "id": "hT0cnOFdafEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the same function we created earlier to view the regression accuracy of our model."
      ],
      "metadata": {
        "id": "zj0apvZTbn5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "def regression_evaluation(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Generate a series of graphs that will help us determine the performance of\n",
        "    a regression model\n",
        "    y_true: the target labels\n",
        "    y_pred: the predicted labels\n",
        "    \"\"\"\n",
        "    # Calculate the distance metrics and Pearson's correlation\n",
        "    mean_error = np.mean(y_pred-y)\n",
        "    std_error = np.std(y_pred-y)\n",
        "    mean_absolute_error = np.mean(np.abs(y_pred-y))\n",
        "    corr, pval = pearsonr(y, y_pred)\n",
        "\n",
        "    # Set up the graphs\n",
        "    fig_bounds = [y.min()-1, y.max()+1]\n",
        "    corr_title = f'Correlation = {corr:0.2f}'\n",
        "    corr_title += ', p<.05' if pval <.05 else ', n.s.'\n",
        "    ba_title = f'Mean Error = {mean_error:0.2f} ± {std_error:0.2f}'\n",
        "\n",
        "    # Generate a correlation plot with the scores in the title\n",
        "    plt.figure(figsize=(9, 3))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(y, y_pred, '*')\n",
        "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--')\n",
        "    plt.grid()\n",
        "    plt.xlim(fig_bounds), plt.ylim(fig_bounds)\n",
        "    plt.xlabel('Actual Score'), plt.ylabel('Predicted Score')\n",
        "    plt.title(corr_title)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(y, y_pred-y, '*')\n",
        "    plt.axhline(y=mean_error, color='k', linestyle='--')\n",
        "    plt.axhline(y=mean_error+std_error, color='r', linestyle='--')\n",
        "    plt.axhline(y=mean_error-std_error, color='r', linestyle='--')\n",
        "    plt.xlim(fig_bounds)\n",
        "    plt.xlabel('Actual Score'), plt.ylabel('Error')\n",
        "    plt.title(ba_title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "b1LxWw82eXkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_evaluation(y, y_pred)"
      ],
      "metadata": {
        "id": "A-eEyZjjjl_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So after all of that work, we ended up with worse accuracy...what gives? In this case, it would be good to re-run the code block from Step 8 while trying different combinations of the changes we made to identify where the issues may lie. If you were to do that, you would find out that doing imputation is the main culprit."
      ],
      "metadata": {
        "id": "73HEAeGwqoEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From there, it would make sense to see if the imputed values are reasonable and/or whether a different imputation method leads to vastly different results. If you were to do that, you would see that using different imputation methods generally lead to comparable and reasonable results."
      ],
      "metadata": {
        "id": "A-VzsVWU9vMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a step back and look at the data that was missing demographic information in the first place:"
      ],
      "metadata": {
        "id": "X3CxlxOn7ZqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.merge(labels_df, features_df, how='right', left_index=True, right_index=True)\n",
        "df = df[pd.isna(df['Weight']) | pd.isna(df['Height'])]\n",
        "df"
      ],
      "metadata": {
        "id": "i7gaQ2vIxktK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are a few observations:\n",
        "* All of the participants came from the same cohort: `Ga`.\n",
        "* The average age across the entire dataset was 68.2 ± 8.7, yet almost half of these patients are over 75.\n",
        "* The average UPDRSM score across the entire dataset was 17.9 ± 7.1, yet almost half of these patients had scores above 25."
      ],
      "metadata": {
        "id": "0ZZmEDTg7swc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what does this mean? It could be that by doing imputation and increasing the size of our dataset, we just so happen to be adding anomalous examples back into our training and test folds. Furthermore, these samples are significantly deflating all of our performance metrics because they are coming from individuals with high UPDRSM scores.\n",
        "\n",
        "Although we would ideally not want to exclude patients from our analyses, let's see what happens when we re-run our supposedly improved pipeline without imputation:"
      ],
      "metadata": {
        "id": "dJ08pf9K-80W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# Generate the original dataset while removing patients with missing features or labels\n",
        "# (same as in the previous notebook)\n",
        "df = pd.merge(labels_df, features_df, how='right', left_index=True, right_index=True)\n",
        "df = df.dropna(how='any')\n",
        "\n",
        "# Get the features, labels, and grouping variables\n",
        "x = df.drop(['Label'], axis=1).values\n",
        "y = df['Label'].values\n",
        "groups = df.index.values\n",
        "\n",
        "# Initialize a data structure to save our final results,\n",
        "# assuming all of the predictions are 0 to start\n",
        "y_pred = np.zeros(y.shape)\n",
        "\n",
        "# Split the data into folds\n",
        "group_kfold = GroupKFold(n_splits=5)\n",
        "for train_idxs, test_idxs in group_kfold.split(x, y, groups):\n",
        "    # Split the data into train and test\n",
        "    x_train = x[train_idxs]\n",
        "    y_train = y[train_idxs]\n",
        "    x_test = x[test_idxs]\n",
        "    y_test = y[test_idxs]\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(x_train)\n",
        "    x_train = scaler.transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    # Select the most useful features\n",
        "    feat_select = SelectKBest(f_regression, k=5)\n",
        "    feat_select.fit(x_train, y_train)\n",
        "    x_train = feat_select.transform(x_train)\n",
        "    x_test = feat_select.transform(x_test)\n",
        "\n",
        "    # Train a model on the transformed training data\n",
        "    reg = KNeighborsRegressor(n_neighbors=5)\n",
        "    reg.fit(x_train, y_train)\n",
        "\n",
        "    # Predict on the transformed test data\n",
        "    y_test_pred = reg.predict(x_test)\n",
        "    y_pred[test_idxs] = y_test_pred"
      ],
      "metadata": {
        "id": "fzomK2kD8XQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_evaluation(y, y_pred)"
      ],
      "metadata": {
        "id": "55gjw7LK82Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It turns out that by ignoring the patients with missing data and just focusing on our other pipeline modifications (feature scaling and selection), the model goes back to a positive correlation. However, we still aren't doing as well as our original model.\n",
        "\n",
        "While this may be disappointing, it shows that there is a lot of trial and error that happens in machine learning. There is no guarantee that every tool in your arsenal is going to improve model performance. In this particularly case, our feature selection might be removing too much useful information."
      ],
      "metadata": {
        "id": "jtT4kecFAdm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what are some things we could do instead to improve model performance? Here are some ideas:\n",
        "* **Hyperparameter search:** Both our model architecture and the steps that we added had hyperparameters, namely the `k` for the `SelectKBest` feature selector and the number of neighbors for the `KNearestRegressor` model. We kept these values fixed throughout this notebook, but tuning these numbers could have led to better results.\n",
        "* **Examine outliers:** We identified outliers in our dataset that worsened the performance of our model. It could be worth our time investigating these individuals more closely and seeing how their features compare to the rest of our dataset. It might also be worth considering if these individuals should be even more represented in our dataset (either by resampling or more data collection).\n",
        "* **Reconsider the task:** Admittedly, this is not an easy dataset to work with. The UPDRSM examines motor symptoms throughout the body, asking patients to perform tasks ranging from finger tapping and hand movements to leg agility and speech evaluation. We are attempting to predict these comprehensive scores solely using what was measured from the soles of patients' feet while they walked. This doesn't even give us a full picture of their gait, as we're missing out on the length of their stride and the angles of their joints. Perhaps we need more signals to get a meaningful model? Perhaps we should be trying classification (high vs. low score) instead of regression?"
      ],
      "metadata": {
        "id": "5I8SfNNuAzSm"
      }
    }
  ]
}