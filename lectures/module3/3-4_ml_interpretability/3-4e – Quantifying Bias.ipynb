{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DekAnzq1b82b",
        "qfIR7WvGil-k",
        "YW34UABn_y9E",
        "KIY9ErJAKTVJ",
        "psJmG43ELjQp",
        "u7HolXKlLzi4",
        "0n0qVrGLGlcl",
        "XbboAs5JbfbW",
        "1IK279p7byVT",
        "BXVD_fh9HjjF",
        "QZVo8uusHjjO",
        "VdNbqQ9rHjjO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to talk about how we can use performance metrics and feature importance values to identify potential cases of bias in a machine learning model."
      ],
      "metadata": {
        "id": "BDaO9cuMWe83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important: Run this code cell each time you start a new session!"
      ],
      "metadata": {
        "id": "DekAnzq1b82b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install os\n",
        "!pip install scikit-learn\n",
        "!pip install shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sklearn\n",
        "import shap"
      ],
      "metadata": {
        "id": "jrO0X1ZMxMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "diabetes_dataset = datasets.load_diabetes(as_frame=True)\n",
        "df = diabetes_dataset.frame\n",
        "\n",
        "# Rename the features for clarity\n",
        "df = df.rename(columns={'s1': 'total serum cholesterol',\n",
        "                        's2': 'low-density lipoproteins',\n",
        "                        's3': 'high-density lipoproteins',\n",
        "                        's4': 'total cholesterol',\n",
        "                        's5': 'log of serum triglycerides',\n",
        "                        's6': 'blood sugar'})"
      ],
      "metadata": {
        "id": "bkHkGrOMm2KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_regressor(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a regression model on the input DataFrame, returning the\n",
        "    regressor, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(orig_df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    regr = LinearRegression()\n",
        "    regr.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = regr.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return regr, feature_names, data"
      ],
      "metadata": {
        "id": "dckQxUtmXekZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_classifier(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a classification model on the input DataFrame, returning the\n",
        "    classifier, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame for regression\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Copy the DataFrame since we will be modifying it\n",
        "    df = orig_df.copy()\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Turn the label into a binary variable\n",
        "    df['target'] = df['target'] > 150\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    clf = DecisionTreeClassifier()\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = clf.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return clf, feature_names, data"
      ],
      "metadata": {
        "id": "NdmtZ4niHtVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Is Bias?"
      ],
      "metadata": {
        "id": "qfIR7WvGil-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of machine learning, ***bias*** refers to the phenomenon when a machine learning model is predisposed to make unfair or skewed predictions. Bias can manifest in many forms. Some examples are described below:\n",
        "* **Loan Approvals:** Machine learning algorithms used in credit scoring can exhibit bias in loan approvals. For example, people who identify as a visible minority are more likely to be turned down for loans despite having similar records and qualifications.\n",
        "* **Facial Recognition Technology:** Some facial recognition algorithms produce higher error rates for women and people with darker skin tones, leading to difficulties when such individuals use technologies like Apple's Face ID.\n",
        "* **Skin Cancer Detection:** Machine learning algorithms used for automated skin cancer detection have been found to exhibit bias based on skin tone. Studies have shown that these algorithms may have higher accuracy rates for lighter skin tones while performing less accurately on darker skin tones."
      ],
      "metadata": {
        "id": "W8MgPgaQi_Jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although these examples primarily involve demographic variables like race and ethnicity, a model can be biased according to any and all variables. For example, a model for predicting the sale value of a home may be biased to inflate the price of homes with a pool. In the end, the features that go into models are simply a series of numbers, and models typically don't have any social context of the meaning behind these numbers."
      ],
      "metadata": {
        "id": "WH9rbe5Q-VdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Where Does Bias Come From?"
      ],
      "metadata": {
        "id": "YW34UABn_y9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is paramount that we not only reflect on the symptoms of bias but also its potential causes. There are certainly cases when data scientists intentionally create, encode, or perpetuate biases in their models. However, even engineers with the best intentions can create biased models."
      ],
      "metadata": {
        "id": "2rmJ5aDe8u50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To talk about some potential culprits of bias, we will discuss some examples around a problem that is discussed in a highly-cited paper by Obermeyer et al. (2019) titled [\"Dissecting racial bias in an algorithm used to manage the health of populations\"](https://www.science.org/doi/full/10.1126/science.aax2342).\n",
        "\n",
        "The paper examined a widely used algorithm that helped hospitals identify high-risk patients according to a \"risk score\". The goal of the algorithm was to help hospitals triage patients and deliver proper care to those with complex health needs. However, Obermeyer et al. found that African-American patients who were scored according to the algorithm were typically sicker than Caucasian patients with the same risk score. This meant that there was a disproportionate number of African-American patients who were not properly identified as being high-risk."
      ],
      "metadata": {
        "id": "pCa9N9-v9TwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tables below describe some examples of how intentional and unintentional biases could have led to this bias:"
      ],
      "metadata": {
        "id": "0DOB-TMu-eTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intentional Bias**\n",
        "\n",
        "| Category | Description |\n",
        "|:----------|:-------------|\n",
        "| **Discriminatory Data Collection** | A data scientist can create or manipulate data to reflect discriminatory practices or biases |\n",
        "| | Example: Limiting dataset representation of African-American patients who are obviously low- and high-risk |\n",
        "| **Prejudiced Feature Selection** | A data scientist can include or exclude features that are known to disproportionately impact certain kinds of inputs |\n",
        "| | Example: Ignoring illnesses that are more common among African-American patients |\n",
        "| **Manipulating Weights and Parameters** | A data scientist can manually manipulate the weights or rules within a machine learning model to favor specific inputs |\n",
        "| | Example: Increasing the importance of illnesses that are more prevalent in Caucasian patients towards a high risk score |"
      ],
      "metadata": {
        "id": "5KCo8zwGin7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unintentional Bias**\n",
        "\n",
        "| Category | Description |\n",
        "|:----------|:-------------|\n",
        "| **Non-Representative Dataset** | A data scientist can collect / be given data that does not adequately represent the population |\n",
        "| | Example: Data was collected from a clinic that only saw African-American patients who were obviously low- and high-risk |\n",
        "| **Unbalanced Training** | A data scientist may split their dataset in a way that leads to biased model training |\n",
        "| | Example: The training dataset only contains African-American patients who are obviously low- and high-risk, leaving all of the borderline cases in the test dataset |\n",
        "| **Proxy Variables** | A data scientist may use a proxy variable to avoid using sensitive information, but that proxy may correlate with a bias |\n",
        "| | Example: Postal code is used as a proxy for socio-economic status, but neighborhoods are often grouped by racial or ethnic groups |\n",
        "| **Model Design** | A data scientist can pick a model architecture that happens to favor features or relationships between features that benefit a particular group |\n",
        "| | Example: A simpler model might learn that a feature is statistically beneficial for overall prediction accuracy but happens to be more important to Caucasian patients |"
      ],
      "metadata": {
        "id": "O-YoqAApDp_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of Obermeyer et al.'s study, the authors found that the model was biased because the algorithm was trained to predict health costs as a proxy for health needs. Less money is spent on African-American patients who have the same level of need, and the algorithm thus falsely concluded that African-American patients were healthier than equally sick Caucasian patients."
      ],
      "metadata": {
        "id": "osNtHx48-jWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretically, the dataset we have used and the models we have constructed for our toy dataset do not have any intentional biases. However, we will walk through some potential strategies that can be used to see if there biases that have crept in our final products."
      ],
      "metadata": {
        "id": "XTxl-FzbGdPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching for Potential Bias: Biological Sex in our Regression Model"
      ],
      "metadata": {
        "id": "KIY9ErJAKTVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are going to see if our regression model is biased according to patients' biological sex."
      ],
      "metadata": {
        "id": "AQwbhHglh-bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regr, feature_names, data = generate_regressor(df)\n",
        "x_test = data['test']['x']\n",
        "y_test = data['test']['y']\n",
        "y_pred = data['pred']['y']"
      ],
      "metadata": {
        "id": "1fjLdyAtGTYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we do that, let's first inspect the distribution of sexes reported in our dataset:"
      ],
      "metadata": {
        "id": "mRfQGdI4iNIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['sex'].value_counts()"
      ],
      "metadata": {
        "id": "4OGbxZ5NiawD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that the dataset was normalized in advance, so instead of having something like `male = 0` or `female = 1`, the features were normalized so that they have a mean of 0 and a variance of 1."
      ],
      "metadata": {
        "id": "6dH399lXiYoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is a toy dataset, we actually don't know which feature value corresponds to male or female. We will assume that the negative values are males and the positive values are females."
      ],
      "metadata": {
        "id": "r_AQ0TLliTYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Feature Importance"
      ],
      "metadata": {
        "id": "psJmG43ELjQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start off by revisiting the importance of the features in our model. While this isn't necessarily going to help us identify bias, it will give us a clue into whether the model is overweighting in the importance of biological sex. If sex is more important than all of the other features in our model (e.g., total cholesterol, blood sugar), then our domain expertise would lead us to believe that there might be something wrong."
      ],
      "metadata": {
        "id": "aTpEhImkDsxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the coefficients of the linear model\n",
        "importances = regr.coef_\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(feature_names, np.abs(importances))\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Unsigned Coefficients')\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Linear Model Coefficient')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(feature_names, importances)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Signed Coefficients')\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Linear Model Coefficient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y5xU_f30gg7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the SHAP explainer\n",
        "explainer = shap.Explainer(regr, x_test, feature_names=feature_names)\n",
        "\n",
        "# Apply the SHAP explainer on our test set\n",
        "shap_values = explainer(x_test)\n",
        "\n",
        "# Plot the distribution of SHAP values\n",
        "shap.summary_plot(shap_values, features=x_test, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "bcorUa1VoMoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we were to rank all of the features according to their importance, we would find sex in the middle of the pack â€” it's not the most important feature, and it's not the least important feature. Therefore, everything seems okay so far."
      ],
      "metadata": {
        "id": "EaAS9S587ZR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Performance Metrics"
      ],
      "metadata": {
        "id": "u7HolXKlLzi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further help us identify bias with respect to a categorical variable like sex, we are going to split our test dataset into two groups: (1) the males in the test dataset and (2) the females in the test dataset."
      ],
      "metadata": {
        "id": "he7yHRvMnYen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the rows in the test data corresponding to male and female patients\n",
        "feature_idx = feature_names.index('sex')\n",
        "sexes = x_test[:, feature_idx]\n",
        "male_rows = sexes < 0\n",
        "female_rows = sexes > 0\n",
        "print(f'Number of males in the test set: {male_rows.sum():d}')\n",
        "print(f'Number of females in the test set: {female_rows.sum():d}')"
      ],
      "metadata": {
        "id": "WxuvCVrLo_zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all data associated with male patients in the test data\n",
        "y_test_male = y_test[male_rows]\n",
        "y_pred_male = y_pred[male_rows]\n",
        "\n",
        "# Find all data associated with female patients in the test data\n",
        "y_test_female = y_test[female_rows]\n",
        "y_pred_female = y_pred[female_rows]"
      ],
      "metadata": {
        "id": "cPKO0-oVhpIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way we can identify if there is a bias between these two groups is by examining if the model is better at predicting the outcomes in one group versus the other. We can do that by using the same performance metric we used to evaluate the overall performance of the model, but computing it separately for the males and females in the test dataset."
      ],
      "metadata": {
        "id": "AUNNVshyk53l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we are going to look at the mean absolute error (MAE) because it is directly comparable across two differently sized samples and can be easily compared to the overall range of possible disease progression scores (0 to 350)."
      ],
      "metadata": {
        "id": "I0LP5SAz-wIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae_male = mean_absolute_error(y_test_male, y_pred_male)\n",
        "mae_female = mean_absolute_error(y_test_female, y_pred_female)\n",
        "print(f'MAE for males: {mae_male:0.2f}')\n",
        "print(f'MAE for females: {mae_female:0.2f}')"
      ],
      "metadata": {
        "id": "VQnzNQfSk4Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is a difference between the two models, with the MAE being is lower for females than it is for males. But is this difference is meaningful? The difference between the two groups is 2 points out of a possible 350, so it may appear small in the grand scheme of the scoring system."
      ],
      "metadata": {
        "id": "_LSdOogCk3Ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still, we are only comparing two numbers that aggregate the errors across our entire test dataset. It is important for us to determine whether this difference is due to chance or actually represents a small, but statistically meaningful bias.\n",
        "\n",
        "We can do this by comparing two distributions of errors rather than two performance metrics."
      ],
      "metadata": {
        "id": "xxQZG55hgz0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the prediction error for each sample\n",
        "pred_errors_males = y_pred_male - y_test_male\n",
        "pred_errors_females = y_pred_female - y_test_female\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.boxplot([np.abs(pred_errors_males), np.abs(pred_errors_females)], labels=['Male', 'Female'])\n",
        "plt.title('Unsigned Error')\n",
        "plt.xlabel('Sex')\n",
        "plt.ylabel('Unsigned Error')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot([pred_errors_males, pred_errors_females], labels=['Male', 'Female'])\n",
        "plt.title('Signed Error')\n",
        "plt.xlabel('Sex')\n",
        "plt.ylabel('Signed Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YeGdx0SFlScN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "statistic, p_value = stats.ttest_ind(pred_errors_males, pred_errors_females)\n",
        "print(f'Result of t-test: {statistic}, p-value is {p_value:0.3f}')"
      ],
      "metadata": {
        "id": "3uNSKF5iCmNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test confirms what we probably could have figured out from the boxplots: the observed difference in model performance across the two biological sexes is not statistically significant."
      ],
      "metadata": {
        "id": "GnyetV3tByTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching for Potential Bias: Age in our Regression Model"
      ],
      "metadata": {
        "id": "0n0qVrGLGlcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's repeat the same analysis that we did before, but with a focus on a continuous variable like age."
      ],
      "metadata": {
        "id": "9XNP10jl986C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regr, feature_names, data = generate_regressor(df)\n",
        "x_test = data['test']['x']\n",
        "y_test = data['test']['y']\n",
        "y_pred = data['pred']['y']"
      ],
      "metadata": {
        "id": "k3uC6QNkguEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the distribution of ages reported in our dataset:"
      ],
      "metadata": {
        "id": "-YrSYPq5986E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['age'].hist()"
      ],
      "metadata": {
        "id": "LGR-UbUT986F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the sex feature, the age feature in our toy dataset is normalized to have a mean of 0 and a variance of 1. Still, we can tell that the distribution is fairly normal with a slight lean towards older patients."
      ],
      "metadata": {
        "id": "H1PANWYc986G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Feature Importance"
      ],
      "metadata": {
        "id": "XbboAs5JbfbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, let's examine importance of the features in our model to see if the model is overweighting in the importance of age:"
      ],
      "metadata": {
        "id": "UMU4plX-bkZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the coefficients of the linear model\n",
        "importances = regr.coef_\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(feature_names, np.abs(importances))\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Unsigned Coefficients')\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Linear Model Coefficient')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(feature_names, importances)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Signed Coefficients')\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Linear Model Coefficient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VlqcnuXSgYLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the SHAP explainer\n",
        "explainer = shap.Explainer(regr, x_test, feature_names=feature_names)\n",
        "\n",
        "# Apply the SHAP explainer on our test set\n",
        "shap_values = explainer(x_test)\n",
        "\n",
        "# Plot the distribution of SHAP values\n",
        "shap.summary_plot(shap_values, features=x_test, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "i02uCxPagbBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to both of these analyses, it seems like age is one of the least important features in this model. Still, let's look at the performance metrics to see if there is a bias."
      ],
      "metadata": {
        "id": "e0tzkqcnb4W7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Performance Metrics"
      ],
      "metadata": {
        "id": "1IK279p7byVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we don't have any predefined age groups for our dataset, we could create such groups ourselves and then evaluate the classification accuracy of the model within each group individually. For simplicity, we'll make two groups: young people and older people."
      ],
      "metadata": {
        "id": "Iq26-GjEcIpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the rows in the test data corresponding to different age groups that we arbitrarily define\n",
        "feature_idx = feature_names.index('age')\n",
        "ages = x_test[:, feature_idx]\n",
        "young_rows = ages < 0\n",
        "old_rows = ages >= 0\n",
        "print(f'Number of young patients the test set: {young_rows.sum():d}')\n",
        "print(f'Number of old patients in the test set: {old_rows.sum():d}')"
      ],
      "metadata": {
        "id": "SQCe6OHAe8nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all data associated with young patients in the test data\n",
        "y_test_young = y_test[young_rows]\n",
        "y_pred_young = y_pred[young_rows]\n",
        "\n",
        "# Find all data associated with old patients in the test data\n",
        "y_test_old = y_test[old_rows]\n",
        "y_pred_old = y_pred[old_rows]"
      ],
      "metadata": {
        "id": "lUJlZ57x_hF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_young = mean_absolute_error(y_test_young, y_pred_young)\n",
        "mae_old = mean_absolute_error(y_test_old, y_pred_old)\n",
        "print(f'MAE for young patients: {mae_young:0.2f}')\n",
        "print(f'MAE for old patients: {mae_old:0.2f}')"
      ],
      "metadata": {
        "id": "x1tPmZ8Y_zxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the prediction error for each sample\n",
        "pred_errors_young = y_pred_young - y_test_young\n",
        "pred_errors_old = y_pred_old - y_test_old\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.boxplot([np.abs(pred_errors_young), np.abs(pred_errors_old)], labels=['Young', 'Old'])\n",
        "plt.title('Unsigned Error')\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Unsigned Error')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot([pred_errors_young, pred_errors_old], labels=['Young', 'Old'])\n",
        "plt.title('Signed Error')\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Signed Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dERQ26HSEGE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "statistic, p_value = stats.ttest_ind(pred_errors_young, pred_errors_old)\n",
        "print(f'Result of t-test: {statistic}, p-value is {p_value:0.3f}')"
      ],
      "metadata": {
        "id": "bMLqDyzYIzFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis is fine, but the way that we defined the groups was somewhat arbitrary and therefore could be hiding an important trend. Instead, what we can do is calculate whether there is a correlation between age and prediction error for the individual samples in our dataset."
      ],
      "metadata": {
        "id": "xrsdYJQmI16J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the prediction error for each sample\n",
        "pred_errors = y_pred - y_test\n",
        "\n",
        "# Calculate the correlation between age and error\n",
        "pearson_r, pval = stats.pearsonr(ages, pred_errors)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(ages, pred_errors, 'k*')\n",
        "plt.title(f\"Pearson's r: {pearson_r:0.2f}, p-val = {pval:0.3f}\")\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Prediction Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xlY3HPxMJaSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look hard enough, we might argue that there is a slight upward trend in signed error as patient age increases; however, this trend is not statistically significant. Therefore, we might conclude that there is not a meaninful bias according to age in this model."
      ],
      "metadata": {
        "id": "474cCmAzKDv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching for Potential Bias: Biological Sex in our Classification Model"
      ],
      "metadata": {
        "id": "BXVD_fh9HjjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do very similar analyses for a classification model, although statistical tests for establishing the significance of bias can be a bit more complicated. For simplicity, we will just examine if this model is biased according to sex."
      ],
      "metadata": {
        "id": "RVCrJX7WHjjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf, feature_names, data = generate_classifier(df)\n",
        "x_test = data['test']['x']\n",
        "y_test = data['test']['y']\n",
        "y_pred = data['pred']['y']"
      ],
      "metadata": {
        "id": "rEdF0iUiHjjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Feature Importance"
      ],
      "metadata": {
        "id": "QZVo8uusHjjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we can assess the importance of features in a classification model using Gini importance scores:"
      ],
      "metadata": {
        "id": "0rZ4HC1jHjjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the Gini importance scores of the model\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title('Gini Importance Scores')\n",
        "plt.bar(feature_names, importances)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Gini Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x5pBsz-3HjjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the SHAP explainer\n",
        "explainer = shap.Explainer(clf, x_test, feature_names=feature_names)\n",
        "\n",
        "# Apply the SHAP explainer on our test set\n",
        "shap_values = explainer(x_test)\n",
        "\n",
        "# Plot the distribution of SHAP values\n",
        "target_class = 1\n",
        "shap.summary_plot(shap_values[:, :, target_class], features=x_test,\n",
        "                  feature_names=feature_names)"
      ],
      "metadata": {
        "id": "bFbyc-BbHjjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to both of these analyses, it seems like sex is one of the least important features in this model. Still, let's look at the performance metrics to see if there is a bias."
      ],
      "metadata": {
        "id": "5TAvJT_eHjjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Performance Metrics"
      ],
      "metadata": {
        "id": "VdNbqQ9rHjjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we can compare the classifier's performance across groups using an appropriate metric. In this case, we will look at F1 score."
      ],
      "metadata": {
        "id": "5GvUlke4HjjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the rows in the test data corresponding to male and female patients\n",
        "feature_idx = feature_names.index('sex')\n",
        "sexes = x_test[:, feature_idx]\n",
        "male_rows = sexes < 0\n",
        "female_rows = sexes > 0\n",
        "print(f'Number of males in the test set: {male_rows.sum():d}')\n",
        "print(f'Number of females in the test set: {female_rows.sum():d}')"
      ],
      "metadata": {
        "id": "VXSmeGRpMQbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all data associated with male patients in the test data\n",
        "y_test_male = y_test[male_rows]\n",
        "y_pred_male = y_pred[male_rows]\n",
        "\n",
        "# Find all data associated with female patients in the test data\n",
        "y_test_female = y_test[female_rows]\n",
        "y_pred_female = y_pred[female_rows]"
      ],
      "metadata": {
        "id": "wrYgubZpMQbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're digging into a classifer, we will quantify classification accuracy using F1 score."
      ],
      "metadata": {
        "id": "T6bMSThDHjjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_male = f1_score(y_test_male, y_pred_male)\n",
        "f1_female = f1_score(y_test_female, y_pred_female)\n",
        "print(f'F1 score for males: {f1_male:0.2f}')\n",
        "print(f'F1 score for females: {f1_female:0.2f}')"
      ],
      "metadata": {
        "id": "7nEsN57cHjjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the classifier didn't seem to put much importance into biological, it seems like there is a major difference in performance between the different sexes. However, we don't know for sure if these differences are statistically meaningful. One way we could establish this is by using a statistical test called the ***Cochran-Mantel-Haenszel test*** to compare the confusion matrices between two distinct groups. This kind of analysis isn't all too common in data science, but here is one way you can do it in case you are curious:"
      ],
      "metadata": {
        "id": "gjLk30zlHjjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels\n",
        "import statsmodels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from statsmodels.stats.contingency_tables import StratifiedTable"
      ],
      "metadata": {
        "id": "GAZDZ6qYHjjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate confusion matrices for the males\n",
        "cm_male = confusion_matrix(y_test_male, y_pred_male)\n",
        "classes = ['negative', 'positive']\n",
        "ConfusionMatrixDisplay.from_predictions(y_test_male, y_pred_male,\n",
        "                                        display_labels=classes)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HGGPfSwYOmQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate confusion matrices for the females\n",
        "cm_female = confusion_matrix(y_test_female, y_pred_female)\n",
        "classes = ['negative', 'positive']\n",
        "ConfusionMatrixDisplay.from_predictions(y_test_female, y_pred_female,\n",
        "                                        display_labels=classes)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jXv8vJAzQqQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the confusion matrices using the CMH test\n",
        "results = StratifiedTable([cm_male, cm_female])\n",
        "stat_test = results.test_equal_odds()\n",
        "print(f'Result of equal odds test: {stat_test.statistic}, p-value is {stat_test.pvalue:0.3f}')"
      ],
      "metadata": {
        "id": "CXQKlrePQn_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific test we just ran was to establish whether the confusion matrices had equal odds of making a correct decision. Since we see a high p-value, we can conclude that this test failed and that the two confusion matrices are significantly different. In other words, there seems to be a bias in model performance across the different genders! This could warrant further investigation to see if the issue lies in the composition of the dataset, the features that were used, or some other factor from those listed at the top of this notebook."
      ],
      "metadata": {
        "id": "i4M9vBUtRxnk"
      }
    }
  ]
}