{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HLZotiuNsvUK",
        "6jKuLWBaqEdK",
        "hh0b7dn3bG_g",
        "qvxYQvsQmZ2h"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to talk about ways of digging into the utility of a machine learning model beyond its accuracy."
      ],
      "metadata": {
        "id": "zKBYPS5L5yWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important: Run this code cell each time you start a new session!"
      ],
      "metadata": {
        "id": "HLZotiuNsvUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install os\n",
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "0cjlwPU5svUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Can We Trust Machine Learning Models?"
      ],
      "metadata": {
        "id": "6jKuLWBaqEdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantifying the performance of a machine learning model can only go so far in giving us confidence that the model is ready to be shared or deployed. When we don't know how a model is making its predictions, it can be challenging to trust the decision that it makes."
      ],
      "metadata": {
        "id": "ipW8kt-KRnpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding how a model is arriving at its decisions can bring about the following benefits:\n",
        "* **Transparency:** Transparency helps build trust and credibility in the model's results. Users can better understand the reasoning behind the predictions and have more confidence in the model's recommendations.\n",
        "* **Regulatory compliance:** In some industries, such as finance or healthcare, there are strict regulations and requirements for model transparency and interpretability. Interpretable models ensure compliance with these regulations, providing explanations and justifications for their predictions.\n",
        "* **Error debugging and improvement:** By understanding why certain predictions are incorrect, we can diagnose errors made by the model and make targeted improvements to enhance its accuracy and reliability.\n",
        "* **Domain expertise:** If we already have an idea of which features should be particularly informative for our target problem, seeing that the model also considers those features important could give us confidence that the model is learning useful information.\n",
        "* **Knowledge extraction:** By understanding how features are weighted, we can also generate new domain-specific insights for further investigation.\n",
        "* **Bias detection and fairness:** If the model puts undue importance on a particular feature, then it may be biased to make incorrect decisions on specific categories of data. Such biases can lead to discrimination or unfair treatment based on certain attributes, so identifying these issues is the first step towards improving the fairness of the model."
      ],
      "metadata": {
        "id": "qQhmEL-FbyU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretability vs. Explainability"
      ],
      "metadata": {
        "id": "hh0b7dn3bG_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two terms that many people use to talk about how a model arrives at its decisions: ***interpretability*** and ***explainability***. Some people use these terms interchangeably, and it is hard to find a definitive consensus across different fields. Here are definitions taken from [Cynthia Rudin](https://www.nature.com/articles/s42256-019-0048-x):\n",
        "\n",
        "* If a model is **interpretable**, then we can explain how it **generally makes decisions** by inspecting its inner workings. We can think of this as an **internal** or **a priori** form of understanding decision making.\n",
        "* If a model is **explainable**, then we can explain how it makes **decisions for individual inputs** either by inspecting its inner workings, by using statistical methods, or a second model altogether. We can think of this as an **external** or **a posteriori** form of understanding decision making."
      ],
      "metadata": {
        "id": "0lOkrsAkudel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most explainable models are inherently interpretable, but the two concepts are not always correlated. Let's talk about some examples:\n",
        "* Logistic regressions are very easy to interpret and explain because we can look at model coefficients and calculate predictions ourselves.\n",
        "* Decision trees are pretty easy to explain because we can trace a series of decisions along its branches. We can also interpret how a decision tree makes decisions in general by looking at the tree in its totality, but that can get cumbersome if the tree is quite large.\n",
        "* Deep learning models are not interpretable because of their mathematical complexity. However, significant research has gone into making models the models at least explainable."
      ],
      "metadata": {
        "id": "sZnYjOKLx5G0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might notice that these models are roughly listed in decreasing order of interpretability, increasing order of complexity, and increasing of potential accuracy. This is an inherent trade-off in data science, and has led many researchers to investigate other ways of deciphering machine learning models."
      ],
      "metadata": {
        "id": "xmm-uXjCy3gB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following notebooks, we will examine various techniques that give us more insight into the inner workings of the machine learning models we create."
      ],
      "metadata": {
        "id": "vw8XyCc7lTx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup for Today's Notebooks"
      ],
      "metadata": {
        "id": "qvxYQvsQmZ2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In all of the notebooks today, we are going to investigate the quality of machine learning models trained on a toy dataset provided by `scikit-learn` called the Diabetes Dataset. This fictitious dataset is intended to emulate electronic health record data collected from 442 diabetic patients. The dataset is already tabular, so we will not need to do nearly as much work to process the data as we have done in the past."
      ],
      "metadata": {
        "id": "Wk5ZPqpZnrTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ten features for our dataset are listed below:\n",
        "\n",
        "| Feature Name | Description |\n",
        "|--------------|-------------|\n",
        "| age | Age in years |\n",
        "| sex | Biological sex |\n",
        "| bmi | Body mass index |\n",
        "| bp | Average blood pressure |\n",
        "| s1 tc | Total serum cholesterol |\n",
        "| s2 ldl | Low-density lipoproteins |\n",
        "| s3 hdl | High-density lipoproteins |\n",
        "| s4 tch | Total cholesterol / HDL |\n",
        "| s5 ltg | Log of serum triglycerides |\n",
        "| s6 glu | Blood sugar |\n",
        "\n",
        "The creators of this dataset have already normalized all of the features such that they range between -0.2 and 0.2."
      ],
      "metadata": {
        "id": "HmetvTZuIjm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The label for our dataset is \"a quantitative measure of disease progression one year after baseline\". This value ranges between 0 to 350."
      ],
      "metadata": {
        "id": "EvYVehnBmnZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "diabetes_dataset = datasets.load_diabetes(as_frame=True)\n",
        "df = diabetes_dataset.frame\n",
        "\n",
        "# Rename the features for clarity\n",
        "df = df.rename(columns={'s1': 'total serum cholesterol',\n",
        "                        's2': 'low-density lipoproteins',\n",
        "                        's3': 'high-density lipoproteins',\n",
        "                        's4': 'total cholesterol',\n",
        "                        's5': 'log of serum triglycerides',\n",
        "                        's6': 'blood sugar'})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bkHkGrOMm2KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to examine two different models trained on this dataset:\n",
        "1. A linear regression model that predicts the disease progression score\n",
        "2. A decision tree classification model that predicts whether the disease progression score is above 150, which is roughly the mean score across the dataset.\n",
        "\n",
        "We will train both of these models using an 80%-20% train-test split."
      ],
      "metadata": {
        "id": "3HWxtonAW3Tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code blocks will be replicated in the upcoming notebooks so that we can examine models trained on these datasets."
      ],
      "metadata": {
        "id": "ZWdlVM87XS4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_regressor(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a regression model on the input DataFrame, returning the\n",
        "    regressor, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(orig_df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    regr = LinearRegression()\n",
        "    regr.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = regr.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return regr, feature_names, data"
      ],
      "metadata": {
        "id": "dckQxUtmXekZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, data = generate_regressor(df)\n",
        "y_test = data['test']['y']\n",
        "y_pred = data['pred']['y']\n",
        "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred):0.2f}')\n",
        "print(f'Coefficient of determination: {r2_score(y_test, y_pred):0.2f}')"
      ],
      "metadata": {
        "id": "cwExqZLtfw7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_classifier(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a classification model on the input DataFrame, returning the\n",
        "    classifier, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame for regression\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Copy the DataFrame since we will be modifying it\n",
        "    df = orig_df.copy()\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Turn the label into a binary variable\n",
        "    df['target'] = df['target'] > 150\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    clf = DecisionTreeClassifier()\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = clf.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return clf, feature_names, data"
      ],
      "metadata": {
        "id": "NdmtZ4niHtVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, data = generate_classifier(df)\n",
        "y_test = data['test']['y']\n",
        "y_pred = data['pred']['y']\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):0.2f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):0.2f}\")"
      ],
      "metadata": {
        "id": "v9Oogpa0gQnh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}