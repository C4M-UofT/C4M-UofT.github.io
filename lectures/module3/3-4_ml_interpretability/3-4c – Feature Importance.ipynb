{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DekAnzq1b82b",
        "K6PhsvK1NuDg",
        "y2buTYo2jNIF",
        "qaN8HJdVjQTN",
        "IX2o4Qs4jYoo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to talk about some of the ways we can inspect the internals of a machine learning model to learn more about how it makes predictions."
      ],
      "metadata": {
        "id": "BDaO9cuMWe83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important: Run this code cell each time you start a new session!"
      ],
      "metadata": {
        "id": "DekAnzq1b82b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install os\n",
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "jrO0X1ZMxMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "diabetes_dataset = datasets.load_diabetes(as_frame=True)\n",
        "df = diabetes_dataset.frame\n",
        "\n",
        "# Rename the features for clarity\n",
        "df = df.rename(columns={'s1': 'total serum cholesterol',\n",
        "                        's2': 'low-density lipoproteins',\n",
        "                        's3': 'high-density lipoproteins',\n",
        "                        's4': 'total cholesterol',\n",
        "                        's5': 'log of serum triglycerides',\n",
        "                        's6': 'blood sugar'})"
      ],
      "metadata": {
        "id": "bkHkGrOMm2KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_regressor(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a regression model on the input DataFrame, returning the\n",
        "    regressor, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(orig_df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    regr = LinearRegression()\n",
        "    regr.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = regr.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return regr, feature_names, data"
      ],
      "metadata": {
        "id": "dckQxUtmXekZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_classifier(orig_df):\n",
        "    \"\"\"\n",
        "    Train and test a classification model on the input DataFrame, returning the\n",
        "    classifier, the feature names, and a dictionary of all the relevant data\n",
        "    orig_df: the input DataFrame for regression\n",
        "    \"\"\"\n",
        "    # Set random number generator so the results are always the same\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Copy the DataFrame since we will be modifying it\n",
        "    df = orig_df.copy()\n",
        "\n",
        "    # Get the names of the features\n",
        "    feature_names = df.columns.tolist()\n",
        "    feature_names.remove('target')\n",
        "\n",
        "    # Turn the label into a binary variable\n",
        "    df['target'] = df['target'] > 150\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "    # Separate features from labels\n",
        "    x_train = train_df.drop('target', axis=1).values\n",
        "    y_train = train_df['target'].values\n",
        "    x_test = test_df.drop('target', axis=1).values\n",
        "    y_test = test_df['target'].values\n",
        "\n",
        "    # Create and train the model\n",
        "    clf = DecisionTreeClassifier()\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    # Use the model to predict on the test set\n",
        "    y_pred = clf.predict(x_test)\n",
        "\n",
        "    # Create a nested dictionary of all the data for easier retrieval\n",
        "    data = {'train': {'x': x_train, 'y': y_train},\n",
        "            'test': {'x': x_test, 'y': y_test},\n",
        "            'pred': {'x': x_test, 'y': y_pred}}\n",
        "    return clf, feature_names, data"
      ],
      "metadata": {
        "id": "NdmtZ4niHtVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Is Feature Importance?"
      ],
      "metadata": {
        "id": "K6PhsvK1NuDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Feature importance*** refers to the idea of quantifying the relevance of features that are used in a predictive model. Determining the importance of each feature can help us understand which features are most influential in determining the final predictions made by the model. By identifying important features, we can gain insights into the underlying patterns and relationships that drive the model's predictions, which can in turn help with feature selection, model interpretability, and improvements in the model's performance.  \n",
        "\n"
      ],
      "metadata": {
        "id": "C2XXNkrVKe_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each category of machine learning model architectures (e.g., linear models, tree-based models) have its own mathematical underpinnings. Therefore, each category may require a different technique to inspect its internals. We will go over relevant techniques for the two models we built on our Diabetes Dataset — a linear regression model and a decision tree classifier — but these methods will generalize to many other model architectures."
      ],
      "metadata": {
        "id": "afSy3_y4K9dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Weights and Coefficients"
      ],
      "metadata": {
        "id": "y2buTYo2jNIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models like linear regression and logistic regression use weighted sums of input features to make predictions. Therefore, the magnitude and direction of these coefficients can give us a rough idea about feature importance."
      ],
      "metadata": {
        "id": "OAePmUkgM3lo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assume that a feature has a positive value. It will influence the model's output in the following ways depending on the sign and magnitude of its coefficient:\n",
        "\n",
        "| Sign | Magnitude | Interpretation |\n",
        "|-----------|------|----------------|\n",
        "| Positive | Large | The feature is important and leads to larger outputs |\n",
        "| Positive | Small | The feature is less important, but still leads to larger outputs |\n",
        "| Negative | Large | The feature is important and leads to smaller outputs |\n",
        "| Negative | Small | The feature is less important, but still leads to smaller outputs |"
      ],
      "metadata": {
        "id": "A7jhQhn5VqIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important:** The table above assumes that all of the model's features have a similar scale. Imagine that we had two features: $F_1$, which ranges between 0 and 1; and $F_2$, which ranges between 0 and 100. If these features have the same coefficients, changing $F_2$ by a standard deviation of its scale is going to have a much larger impact than corresponding changes in $F_1$ on the final output. You may need to account for these differences in scale when interpreting your coefficients."
      ],
      "metadata": {
        "id": "11C580E4NbxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our dataset already has normalized features, we can directly compare our coefficients. Let's do that for our linear regression model:"
      ],
      "metadata": {
        "id": "xuwJalDQPne5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regr, feature_names, _ = generate_regressor(df)\n",
        "importances = regr.coef_\n",
        "for feature_name, importance in zip(feature_names, importances):\n",
        "    print(f'Feature: {feature_name}, Score: {importance}')"
      ],
      "metadata": {
        "id": "MbG_l_NzN-m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot these coefficients in a bar chart to visually compare them. Let's look at both the signed and unsigned value of the coefficients so that we can compare both their magnitude and direction:"
      ],
      "metadata": {
        "id": "oeSPzT7LQEcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(feature_names, np.abs(importances))\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Unsigned Coefficients')\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Linear Model Coefficient')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(feature_names, importances)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Signed Coefficients')\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Linear Model Coefficient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ch2MUYlINI0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to these graphs, we can see that total serum cholestorol has the strongest impact on the predictions from our linear regression model. Because it is negative, higher total serum cholesterol values will lead to lower predictions.\n",
        "\n",
        "Log of serum triglycerides, BMI, and low-density lipoproteins seem to be the next most significant features, and they are all positive. Therefore, higher values for those features will lead to higher predictions."
      ],
      "metadata": {
        "id": "Reh-tq_OQ2M0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Branches"
      ],
      "metadata": {
        "id": "qaN8HJdVjQTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree-based models like decision trees and random forests do not have coefficients. Instead, they use a series of learned rules to make predictions on input data. We can actually inspect these rules using the `plot_tree()` function in\n",
        "the `tree` module of `scikit-learn`. Note that we are saving this figure to as an image file rather than showing it in this notebook. That is because we need to make the image quite large in order to see all of the rules:"
      ],
      "metadata": {
        "id": "-cFgrjwsR3_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "clf, feature_names, _ = generate_classifier(df)\n",
        "plt.figure(figsize=(50, 50))\n",
        "tree.plot_tree(clf, feature_names=feature_names, filled=True, fontsize=10)\n",
        "plt.savefig('tree.jpg')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "GXLbxpPSN9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rows within a node of the decision tree can be interpreted as follows:\n",
        "1. **Rule:** The decision rule that dictates whether an input sample should go to the left or right.\n",
        "2. **Gini:** We'll talk about this number in a bit. For now, just know that lower numbers are better.\n",
        "3. **Samples:** The number of training samples that made it to this point in the tree.\n",
        "4. **Value:** The distribution of classes for training samples that have made it to this point in the tree."
      ],
      "metadata": {
        "id": "9-5XeOxeVje4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While visualizing the tree gives us definitive information about how the model is making decisions, it can be difficult to keep track of which feature values are relevant for a given input.\n",
        "\n",
        "Furthermore, looking at the tree doesn't necessarily give us a clear idea of which features are most important to the overall model. Just because a node is higher in the decision tree does not mean that the corresponding feature is automatically more important."
      ],
      "metadata": {
        "id": "sVfjnFpeVdqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purity-Based Importance Scores"
      ],
      "metadata": {
        "id": "IX2o4Qs4jYoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way we can rate the quality of a feature in a tree-based model is by seeing how effective it is at separating training samples belonging to different classes. To quantify that, we are going to take advantage of a measurement called ***Gini impurity***."
      ],
      "metadata": {
        "id": "1NARkW5mftUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine we zoom in on a particular chunk of our decision tree corresponding to a single rule. A good rule is able to sort through the mixture of training samples and separate them so that samples from different classes end up at different parts of the tree. When a node has training samples from multiple classes, we call it impure; when it only has training samples from a single class, we call it pure. The more uniform the distribution is across multiple classes, the more impure it is."
      ],
      "metadata": {
        "id": "fljbeObG73_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can measure the overall ***Gini importance score*** for a feature by averaging the total reduction in the Gini impurity across all the nodes in the tree that use the feature for splitting. Features that result in a significant reduction in the impurity are considered more important, as they contribute more to the overall purity and separation of the classes in the decision tree."
      ],
      "metadata": {
        "id": "Re8XgIo8SQTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini importance scores range from 0 to 1, with higher values indicating greater importance. Unlike with the linear model coefficients, we cannot look at the Gini importance scores to determine whether features have a positive or negative impact on predictions. This is because there may be multiple nodes with conflicting rules based on how the data gets separated within lower levels of the tree."
      ],
      "metadata": {
        "id": "tq8g6AucjnMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the Gini importance scores look like for our decision tree classifier:"
      ],
      "metadata": {
        "id": "rV6QJ-wiSujr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = clf.feature_importances_\n",
        "for feature_name, importance in zip(feature_names, importances):\n",
        "    print(f'Feature: {feature_name}, Score: {importance}')"
      ],
      "metadata": {
        "id": "H-L_9-_uSxhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.title('Gini Importance Scores')\n",
        "plt.bar(feature_names, importances)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.xlabel('Feature Name')\n",
        "plt.ylabel('Gini Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wHhmongPjeNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to this graph, we can see that the log of serum triglycerides is the most important feature in our model, followed by BMI and blood pressure."
      ],
      "metadata": {
        "id": "hEXWSVOvjcCc"
      }
    }
  ]
}