{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OBADxK39qJDx",
        "W-jsbV0Axsoh",
        "JqQYu6Io92hu",
        "d1xHZR6b_qCi",
        "9BFa3RD7_zRH",
        "43-5Vra6AcaA",
        "m8s6eiOkhLJY",
        "NTPFajKn94Ry",
        "9Skk8Xmh_3U6",
        "iS9PhgMCAB6z",
        "llxX8ByHAGKi",
        "b7YXfZRl99zL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Earlier, we talked about the importance of working with time-series data with a consistent sampling rate. In other words, it's easiest to work with data when all of the data points are evenly spaced in time.\n",
        "\n",
        "Another challenge you will run into is that you will encounter ***signal noise*** that impacts the measurements in your time-series data. In this notebook, we will cover some potential causes of signal noise and ways of addressing them."
      ],
      "metadata": {
        "id": "Vj8n7JvdwZ8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important: Run this code cell each time you start a new session!"
      ],
      "metadata": {
        "id": "OBADxK39qJDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "yuzcYs-MUBvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -Ncnp https://physionet.org/files/accelerometry-walk-climb-drive/1.0.0/raw_accelerometry_data/id00b70b13.csv"
      ],
      "metadata": {
        "id": "bASFcdZNZuGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('id00b70b13.csv')\n",
        "\n",
        "# Filter to only walking activity, which is given a code of 1\n",
        "df = df[df['activity'] == 1]\n",
        "\n",
        "# Process the time\n",
        "df.rename(columns={'time_s': 'Time'}, inplace=True)\n",
        "df = df[(df['Time']>=700) & (df['Time']<=710)]\n",
        "df['Time'] = df['Time'] - df['Time'].min()\n",
        "\n",
        "# Process the accel\n",
        "df['Accel'] = np.sqrt(df['la_x']**2 + df['la_y']**2 + df['la_z']**2)*9.8\n",
        "\n",
        "# Keep only crucial columns\n",
        "keep_cols = ['Time', 'Accel']\n",
        "df = df[keep_cols]\n",
        "df.to_csv('walking.csv',index=False)"
      ],
      "metadata": {
        "id": "2hP2UnPrcEdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Causes Signal Noise"
      ],
      "metadata": {
        "id": "W-jsbV0Axsoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources of signal noise can be highly dependent upon the kind of data that is being collected. Below are three broad categories of signal noise:\n",
        "* **Data entry:** People may manually enter information incorrectly into a survey or form, leading to outliers in your data (e.g., entering `600 bpm` instead of `60 bpm` for heart rate).\n",
        "* **Intrinsic causes:** A sensor can exhibit unwanted random variation due to factors like electrical interference, thermal deviation, and electronic component limitations.\n",
        "* **Extrinsic causes:** There may be disturbances outside of the sensor that get superimposed on our measurements. For instance, if we are trying to record someone's voice using a microphone, ambient background sounds can make it more difficult for us to analyze speech."
      ],
      "metadata": {
        "id": "QMlKMk-m6LU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, there is no one-size-fits-all solution to identifying signal noise. Some cases of noise might be obvious, while others may be more subtle. There might also be situations where you think something that looks like noise is actually an important indicator of the phenomenon you are trying to measure. Making these determinations will require inspecting your data closely and reflecting on how it was collected, and picking the correct technique for addressing signal noise can be equally complex."
      ],
      "metadata": {
        "id": "iMSZSF5t7Xb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we will cover two commonly-used techniques for dealing with sensor noise, but these are not comprehensive by any means."
      ],
      "metadata": {
        "id": "Wp00uw3W-Ph8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Detection"
      ],
      "metadata": {
        "id": "JqQYu6Io92hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An ***outlier*** is a data point that is significantly different from the other data points in a dataset. Identifying and removing outliers can help ensure that your analyses do not get thrown off by random perturbations that do not mean anything.\n",
        "\n",
        "To talk about different approaches to outlier detection, let's manually create a simple time-series of resting heart rate data, which we will assume should lie between 60â€“100 bpm."
      ],
      "metadata": {
        "id": "fbwed23LYl5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hr_data = [60, 66, 64, 73, 65, 200, 70, 64, 66, 60]\n",
        "hr_time = list(range(len(hr_data)))\n",
        "lower_bound = 60\n",
        "upper_bound = 100\n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(hr_time, hr_data, 'k-*')\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Heart Rate (bpm)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "souaHMQ_ZwOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a clear outlier at the 5th hour when a resting heart rate of 200 bpm was recorded. In fact, we can tell that this data point is an outlier without even knowing its timestamp, so the techniques we will cover below will primarily involve working with `hr_data` only."
      ],
      "metadata": {
        "id": "qHRZmcZMcrUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying Outliers: Manual Method"
      ],
      "metadata": {
        "id": "d1xHZR6b_qCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you know ahead of time that your data should fall within a specific range, you can use comparators and comparator-like functions to check if your data lies within that range. Below are three examples using `if` statements, `numpy`, and `pandas`:"
      ],
      "metadata": {
        "id": "NN_Dv9rj47bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual\n",
        "is_outlier = []\n",
        "for hr in hr_data:\n",
        "    out_of_range = hr < lower_bound or hr > upper_bound\n",
        "    is_outlier.append(out_of_range)\n",
        "print(is_outlier)"
      ],
      "metadata": {
        "id": "7MHTefzR-rBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy boolean mask\n",
        "hr_array = np.array(hr_data)\n",
        "(hr_array < lower_bound) | (hr_array > upper_bound)"
      ],
      "metadata": {
        "id": "Kr84ouAY8_M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas between\n",
        "hr_series = pd.Series(hr_data)\n",
        "~hr_series.between(lower_bound, upper_bound)"
      ],
      "metadata": {
        "id": "5ZDcCGyp8Gn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying Outliers: Z-Score"
      ],
      "metadata": {
        "id": "9BFa3RD7_zRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't know the range in which your data should fall ahead of time, you can manually calculate the distribution of the data and then check to see how far away each point is from that distribution."
      ],
      "metadata": {
        "id": "7lPtRJkM5B-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way of doing this is by using the ***Z-score***, which measures how many standard deviations a data point is from the mean. When you use a measure like Z-score, you will need to decide how far away a data point should be in order for it to be considered an outlier. Let's look at the Z-score for each data point in our heart rate data:"
      ],
      "metadata": {
        "id": "nhi5MvPia9FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "stats.zscore(hr_data)"
      ],
      "metadata": {
        "id": "STfMS1fMa_Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that most of our data points are within one standard deviation (i.e., their Z-score is between -1 and 1). However, our obviously incorrect data point has a Z-score that is close to 3. Therefore, we might consider that any data point with a Z-score greater than 2 or less than -2 should be considered an outlier in this case:"
      ],
      "metadata": {
        "id": "5jX16uLVbZxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.abs(stats.zscore(hr_data)) > 2"
      ],
      "metadata": {
        "id": "nuGwIs54b0HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Outliers: Placeholder Values\n"
      ],
      "metadata": {
        "id": "43-5Vra6AcaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling outliers can be just as tricky as handling missing data since you don't know what the real value should have been at that timestamp."
      ],
      "metadata": {
        "id": "ksgnnk3Ub8Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One common approach you might have seen for handling outliers and missing data is by using a pre-determined ***placeholder value***."
      ],
      "metadata": {
        "id": "WttOaheVcbFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some common placeholder values you might see people use as they are saving data in Excel include `-1` (if you know that the data is always going to be positive) and `np.nan` (short for \"not a number\"). However, these values actually introduces new trends that make it more difficult to process your data later."
      ],
      "metadata": {
        "id": "zzSamuaZhrBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A safer placeholder value is one that you will know falls within the distribution of your data. For our heart rate data, for example, we might consider picking the average heart rate in our time series:"
      ],
      "metadata": {
        "id": "xIU1gFPzhu1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the placeholder\n",
        "hr_series = pd.Series(hr_data)\n",
        "placeholder = hr_series.mean()\n",
        "\n",
        "# Find the outlier entries and replace them\n",
        "outlier_mask = ~hr_series.between(lower_bound, upper_bound)\n",
        "hr_series[outlier_mask] = placeholder\n",
        "print(hr_series)"
      ],
      "metadata": {
        "id": "o55AvBGKgRXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Outliers: Interpolation"
      ],
      "metadata": {
        "id": "m8s6eiOkhLJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Placeholder values are applied uniformly across an entire dataset whenever there is an outlier. If your data has lots of outliers, then the same placeholder value will appear many times in your new signal. This may not be ideal since it will heavily skew the distribution of your measurements."
      ],
      "metadata": {
        "id": "YKkyZPYdhbCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than replacing all outliers using the same value, we can replace outliers with values that resemble nearby measurements. We can do this using the same interpolation techniques we explored earlier, bearing in mind that we should remove out outliers before generating our interpolator:"
      ],
      "metadata": {
        "id": "9xFbpSk_jNdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy\n",
        "hr_time_arr = np.array(hr_time)\n",
        "hr_data_arr = np.array(hr_data)\n",
        "\n",
        "# Find the outlier entries and remove them\n",
        "outlier_mask = (hr_array > lower_bound) & (hr_array < upper_bound)\n",
        "hr_time_clean = hr_time_arr[outlier_mask]\n",
        "hr_data_clean = hr_data_arr[outlier_mask]\n",
        "\n",
        "# Interpolate\n",
        "hr_resampled = np.interp(hr_time_arr, hr_time_clean, hr_data_clean)\n",
        "print(hr_resampled)"
      ],
      "metadata": {
        "id": "BIPSHGArjixX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "b7YXfZRl99zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many techniques at our disposal in order to make real-world data conform to these assumptions. However, it will be important for you to consider the implications of applying these techniques to your data. If you preprocess your data too heavily, you may remove important information that will be useful down the road for other analyses."
      ],
      "metadata": {
        "id": "YlcSpabixQ5v"
      }
    }
  ]
}