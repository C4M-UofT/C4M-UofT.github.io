{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DekAnzq1b82b",
        "Ej2YskEli9vV",
        "HzuwW74sMY3p",
        "ZS_PN2Ttl6OX",
        "UTA06KW7gn0c",
        "mqaE1YWyMV5U",
        "KT9aakoz670M",
        "He4pDJU9mvnn",
        "QDj4g3Wbeepf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to talk about [`pandas`](https://pandas.pydata.org/) â€” a popular Python library for working specifically with tabular data. It is centered around two different data structures: ***Series*** and ***DataFrames***. `pandas` relies heavily on `numpy` to make operations on these data structures fast."
      ],
      "metadata": {
        "id": "OJrDln6VJPMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yuzcYs-MUBvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important: Run this code cell each time you start a new session!"
      ],
      "metadata": {
        "id": "DekAnzq1b82b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install os\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "jrO0X1ZMxMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://archive.physionet.org/users/shared/challenge-2019/training_setA.zip\n",
        "!unzip -n training_setA.zip"
      ],
      "metadata": {
        "id": "bASFcdZNZuGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_single_file(file_path):\n",
        "    df = pd.read_csv(file_path, sep=\"|\")\n",
        "    df['PatientID'] = file_path.split(os.sep)[-1][:-4]\n",
        "    df['Hour'] = df.index\n",
        "    keep_cols = ['PatientID', 'Age', 'Gender', 'SepsisLabel', 'Hour',\n",
        "                 'HR', 'O2Sat', 'SBP', 'DBP', 'Resp']\n",
        "    df = df[keep_cols]\n",
        "    df.rename(columns={'Gender': 'Sex', 'SepsisLabel': 'HasSepsis'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "def create_final_table():\n",
        "    final_df = pd.DataFrame()\n",
        "    patient_list = range(1, 51)\n",
        "    patient_list = [f'p{str(s).zfill(6)}.psv' for s in patient_list]\n",
        "    for f in patient_list:\n",
        "        df = load_single_file(os.path.join(\"training\", f))\n",
        "        final_df = pd.concat([final_df, df])\n",
        "    final_df = final_df[final_df['Hour'] == 12]\n",
        "    final_df.drop(columns=['Hour'], inplace=True)\n",
        "    final_df.to_csv('sepsis.csv',index=False)\n",
        "create_final_table()"
      ],
      "metadata": {
        "id": "2hP2UnPrcEdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Series"
      ],
      "metadata": {
        "id": "Ej2YskEli9vV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `Series` is essentially a 1D column array that can hold data of any type:\n",
        "\n"
      ],
      "metadata": {
        "id": "xTAzV95xMKAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series([67, 77, 75, 83, 64])"
      ],
      "metadata": {
        "id": "G_MQvL72f2YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that when we print out the `Series`, we actually see two columns of numbers. On the right, we see the numbers we used to initialize the `Series`. The numbers on the left are the labels we can use to access each element. `pandas` calls these labels the ***index*** of the `Series`, but bear in mind that they are different from the positional indices you've used for lists and arrays."
      ],
      "metadata": {
        "id": "yXo_d6FKhWlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, `pandas` automatically generates indices corresponding to the position of the element in the `Series`, but you can give your `Series` more informative indices as you see fit. For example, recall the patient data we looked at earlier:"
      ],
      "metadata": {
        "id": "JJ_ql3ed7_34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_data = [\"Alice\", \"Bob\", \"Carol\", \"Dan\", \"Eric\"]\n",
        "sex_data = [\"female\", \"male\", \"female\", \"male\", \"male\"]\n",
        "age_data = [67, 77, 75, 83, 64]"
      ],
      "metadata": {
        "id": "kurs4qt2kb4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can turn `sexes` and `ages` into their own `Series` and then index those elements by the patients' names:"
      ],
      "metadata": {
        "id": "0JWjQY_LkiBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sexes = pd.Series(sex_data, index=name_data)\n",
        "sexes"
      ],
      "metadata": {
        "id": "7OSxOyoMhwnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ages = pd.Series(age_data, index=name_data)\n",
        "ages"
      ],
      "metadata": {
        "id": "q26ILRpOy2cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a side note, we can also create a `Series` with indices using a `dict`:"
      ],
      "metadata": {
        "id": "BbPS3goylZQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Series using a dictionary\n",
        "pd.Series({\"Alice\": 67, \"Bob\": 77, \"Carol\": 75, \"Dan\": 83, \"Eric\": 64})"
      ],
      "metadata": {
        "id": "pGcoFCKXjAij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is strongly suggested that you keep your indices unique. Otherwise, you can run into weird issues as you merge, join, or resample your data."
      ],
      "metadata": {
        "id": "i3C3q39lj3Hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing Series Elements"
      ],
      "metadata": {
        "id": "HzuwW74sMY3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to grab elements within a `Series`, we can access them by their assigned label using `.loc[]`. In our toy dataset, we use the patients' names as the labeling index."
      ],
      "metadata": {
        "id": "gbb2HnmG1UxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_data = [\"Alice\", \"Bob\", \"Carol\", \"Dan\", \"Eric\"]\n",
        "sex_data = [\"female\", \"male\", \"female\", \"male\", \"male\"]\n",
        "age_data = [67, 77, 75, 83, 64]\n",
        "ages = pd.Series(age_data, index=name_data)"
      ],
      "metadata": {
        "id": "8nnADyIhNJjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab Alice and Bob's ages\n",
        "ages.loc[['Alice', 'Bob']]"
      ],
      "metadata": {
        "id": "DGNxHN4J1Ubm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want this data to look like a `numpy` array, you can simple look at the `.values` attribute."
      ],
      "metadata": {
        "id": "RxufZifWODoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ages.loc[['Alice', 'Bob']].values"
      ],
      "metadata": {
        "id": "EFSgIDzCOJ0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did not create an index for your `Series`, remember that `pandas` will create numerical indices for you. When this happens, it will ***seem like*** you can access rows by their position using `.loc[]`."
      ],
      "metadata": {
        "id": "CwIehhgH23q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ages_noindex = pd.Series(age_data)\n",
        "ages_noindex"
      ],
      "metadata": {
        "id": "8H4IlYDQOdOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the rows with the assigned labels 0 thru 4\n",
        "ages_noindex.loc[:2]"
      ],
      "metadata": {
        "id": "QipjcCSJQJ32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, you'll find that this is not actually the case because you won't be able to use negative indices."
      ],
      "metadata": {
        "id": "YXMzjgOFOjb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Grab the row with the label -1, which doesn't exist\n",
        "try:\n",
        "    ages_noindex.loc[-1]\n",
        "except KeyError:\n",
        "    print('Index does not exist')"
      ],
      "metadata": {
        "id": "5XM8AUQ13AmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you really want to access values by their position in the series, you can use the `.iloc[]` operator."
      ],
      "metadata": {
        "id": "QI3TxCEs6l1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the last row\n",
        "ages_noindex.iloc[-1]"
      ],
      "metadata": {
        "id": "pYq2l8m19LtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That being said, using the labels built into the `Series` will give you more power in the long run."
      ],
      "metadata": {
        "id": "v-zwwFYS9MP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataFrames"
      ],
      "metadata": {
        "id": "ZS_PN2Ttl6OX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `DataFrame` is a 2D array composed of multiple `Series`, which finally brings us to a structure that closely resembles tabular data. This is just one of many ways to create a `DataFrame`; you can also create them from lists, dictionaries, etc."
      ],
      "metadata": {
        "id": "3eTBP4WoOqHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = [\"Alice\", \"Bob\", \"Carol\", \"Dan\", \"Eric\"]\n",
        "sexes = [\"female\", \"male\", \"female\", \"male\", \"male\"]\n",
        "ages = [67, 77, 75, 83, 64]\n",
        "\n",
        "sex_series = pd.Series(sexes, index=names)\n",
        "age_series = pd.Series(ages, index=names)\n",
        "patients_df = pd.DataFrame({'Sex': sex_series, 'Age': age_series})\n",
        "patients_df"
      ],
      "metadata": {
        "id": "BlEU9Ue2omL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chances are that you won't be creating `DataFrames` from scratch. Instead, you'll be loading data from a file. Let's take a look at a subset of a real-world dataset from the [2019 PhysioNet Challenge](https://physionet.org/content/challenge-2019/1.0.0/), which was aimed at predicting sepsis from physiological data. The `DataFrame` we will load is going to be have fewer patients and fewer columns for the sake of simplicity."
      ],
      "metadata": {
        "id": "mNPEE3LkMLTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"sepsis.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "F_AwoB79UB34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All we did was give this function a filename, and `pandas` was automatically able to do the following for us:\n",
        "* Assume that the first row was the header of the `.csv` file and therefore named the columns accordingly\n",
        "* Assume the type of data in each column based on their syntax and make the conversions for us\n",
        "* Fill in missing data with placeholders (`np.nan`)\n",
        "\n",
        "This is a really powerful function with over 20 different optional arguments that you can investigate on your own."
      ],
      "metadata": {
        "id": "dHzcpJuprQbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will play with this `DataFrame` for the rest of this notebook."
      ],
      "metadata": {
        "id": "d_KCNAzQQQlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing DataFrame Columns\n",
        "\n"
      ],
      "metadata": {
        "id": "UTA06KW7gn0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can grab one or many columns by indexing into the `DataFrame` according to the column name(s):"
      ],
      "metadata": {
        "id": "Mkwo6vaI1BMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['PatientID'] # Provides a Series"
      ],
      "metadata": {
        "id": "ZSP3D6JW1N5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['PatientID', 'Age']] # Provides a DataFrame"
      ],
      "metadata": {
        "id": "O8mYVxbY1xEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing DataFrame Rows"
      ],
      "metadata": {
        "id": "mqaE1YWyMV5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can grab one or many rows within a `DataFrame` using either the `.loc[]`\n",
        "or `.iloc[]` operator that we used earlier for `Series`."
      ],
      "metadata": {
        "id": "1HZLJ5RF0xla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the rows with the assigned labels 0 thru 4\n",
        "df.loc[0:4]"
      ],
      "metadata": {
        "id": "-zlaXgUf08aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the last row\n",
        "df.iloc[-1]"
      ],
      "metadata": {
        "id": "dhg-AHd6Pl8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where `pandas` becomes very powerful is the fact that you can grab rows that satisfy a set of criteria specified as boolean statements. Here are some examples:"
      ],
      "metadata": {
        "id": "2VccNAA1Qeo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab all the rows from male patients\n",
        "df[df['Sex'] == 1]"
      ],
      "metadata": {
        "id": "tlrIJEqgQs-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab all the rows with heart rate values greater than 100\n",
        "df[df['HR'] > 100]"
      ],
      "metadata": {
        "id": "vfqnuK0dQ6ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab all of the rows with O2Sat values between 95 and 98\n",
        "df[(df['O2Sat'] >= 95) & (df['O2Sat'] <= 98)]"
      ],
      "metadata": {
        "id": "eFj_qDPiRBj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab all the rows from patient p000001 or p000005\n",
        "df[(df['PatientID'] == 'p000001') | (df['PatientID'] == 'p000005')]"
      ],
      "metadata": {
        "id": "lzZ4glEhRwq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that when we want to combine boolean conditions together, we use the `&` instead of `and` and  `|` instead of `or`. This is because we need to do an element-wise logical comparison between two `Series` of boolean values rather than two boolean values:"
      ],
      "metadata": {
        "id": "kQfG3HMTRSkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['O2Sat'] >= 95)\n",
        "print(df['O2Sat'] <= 98)"
      ],
      "metadata": {
        "id": "ARXOwCR3TnbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and Removing DataFrame Columns"
      ],
      "metadata": {
        "id": "KT9aakoz670M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a column requires accessing the column and then setting its value with an assignment statement. If you assign a single value, it will be propagated to all of the rows. If you assign as many values as there are rows, `pandas` will put the values in the correct rows. If you assign an arbitrary number of values, your code won't work."
      ],
      "metadata": {
        "id": "si9IbDGTCuXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume a default value\n",
        "df['Ethnicity'] = 'Caucasian'\n",
        "df"
      ],
      "metadata": {
        "id": "8a26ajsQDRkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use an empty placeholder\n",
        "df['Address'] = np.nan\n",
        "df"
      ],
      "metadata": {
        "id": "OqWUxyFuEA-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column based on the values in another column\n",
        "df['Age in days'] = df['Age'] * 365\n",
        "df"
      ],
      "metadata": {
        "id": "aS1S_nJLoqIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove columns, you can use the `.drop()` method:"
      ],
      "metadata": {
        "id": "4QePI3PUC-fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.drop(columns=['Address', 'Age in days'])\n",
        "df2"
      ],
      "metadata": {
        "id": "YBjBFU-yWDQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a useful aside, many methods provided by `pandas` have a handy argument called `inplace` that will allow you to modify the original object rather than needing to assign the output to a new variable:"
      ],
      "metadata": {
        "id": "CunXKNYlWeRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Address', 'Age in days'], inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "HVe8nh3NXLRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and Removing DataFrame Rows"
      ],
      "metadata": {
        "id": "JbxkaSDwUP0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best way to add one or many rows to a `DataFrame` is by using the `.append()` method. Here are two different ways to use it:"
      ],
      "metadata": {
        "id": "GVV4A28Sopd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a single patient from a dictionary\n",
        "new_patient = {'PatientID': 'Alex', 'Age': 31, 'Sex': 1, 'HasSepsis': 0,\n",
        "               'HR': 75, 'O2Sat': 98, 'SBP': 96, 'DBP': 60, 'Resp': 18}\n",
        "pd.concat([df, pd.DataFrame([new_patient])], ignore_index=True)"
      ],
      "metadata": {
        "id": "Y3HVvu3D9zUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a group of patients from another DataFrame\n",
        "df2 = df.loc[:5]\n",
        "df1and2 = pd.concat([df, df2], ignore_index=True)\n",
        "df1and2"
      ],
      "metadata": {
        "id": "EtXkJrt9B2LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll see that in both cases, we set the `ignore_index` argument to `True` so that `pandas` could automatically generate new indices for us. Had we not done this, `pandas` would have either used the indices from the data being added or thrown an error."
      ],
      "metadata": {
        "id": "jeavBwm_CPKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can drop rows by their assigned index lable using the `.drop()` method that we used for columns."
      ],
      "metadata": {
        "id": "gxV3e5FpC9af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1and2.drop(index=[11, 12, 13, 14, 15], inplace=True)"
      ],
      "metadata": {
        "id": "fzln2pI3ZKEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to drop rows that satisfy a set of criteria, you can use the same technique we used earlier to access rows by boolean conditions and then grab the index of the final result."
      ],
      "metadata": {
        "id": "7p1n4vkSaQBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all data from patient p004893\n",
        "df1and2.drop(index=df1and2[(df1and2['PatientID'] == 'p000050')].index, inplace=True)\n",
        "df1and2"
      ],
      "metadata": {
        "id": "xMWRTZ13adoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging DataFrames"
      ],
      "metadata": {
        "id": "He4pDJU9mvnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There may be situations when we have two `DataFrames` with information that we want to combine into a single `DataFrame`. For instance, we might have one `DataFrame` with patients' vital signs and another `DataFrame` with patients' blood test results, and we would want to combine those in a single `DataFrame` so that we can see the data side-by-side for each patient.\n",
        "\n",
        "We can do this using the function `pd.merge()`, which combines the information across two `pandas` structures according to a shared index or column."
      ],
      "metadata": {
        "id": "o3X2B4qVY32p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function requires a subset of the following parameters:\n",
        "\n",
        "* **left:** The first `DataFrame` we want to merge\n",
        "* **right:** The second `DataFrame` we want to merge\n",
        "* **on:** The column or index being used to decide which rows should be merged\n",
        "*  **left_on:** If `on` is not specified, the column or index in the left `DataFrame` that should be used for merging\n",
        "*  **right_on:** If `on` is not specified, the column or index in the right `DataFrame` that should be used for merging\n",
        "* **left_index:** If `True`, then the index from the left `DataFrame` will be used for merging\n",
        "* **right_index:** If `True`, then the index from the right `DataFrame` will be used for merging\n",
        "* **how:** The way that we want to merge the two `DataFrames` according to the merging key(s) defined by `on`, `left_on`, `right_on`, `left_index`, and/or `right_index`:\n",
        "\n",
        "| Threshold Type | Outcome |\n",
        "|:-------:|:---------:|\n",
        "| `'inner'` | Only keeps the keys that are present in both `left` and `right`  |\n",
        "| `'outer'` | Uses keys that are present in either `left` or `right` |\n",
        "| `'left'` | Only keeps the keys that are present in `left` |\n",
        "| `'right'` | Only keeps the keys that are present in `right` |"
      ],
      "metadata": {
        "id": "SnBtbSvxprkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image below illustrates how these different merging methods work:"
      ],
      "metadata": {
        "id": "gXt6hYW8qwpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1XmnLby8UB_Ad6jTj8xWQf-KyIrKZ7myT\" width=500px/>"
      ],
      "metadata": {
        "id": "j3EguyRVo6Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with fake information\n",
        "glucose_df = pd.DataFrame(index=df.index)\n",
        "glucose_df['PatientID'] = df['PatientID']\n",
        "glucose_df['Glucose'] = np.random.rand(len(glucose_df))"
      ],
      "metadata": {
        "id": "OasGapQLdGIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "nl8s7AIjeh24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glucose_df"
      ],
      "metadata": {
        "id": "UGU_FFnEel8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(df, glucose_df, how='inner', on='PatientID')\n",
        "merged_df"
      ],
      "metadata": {
        "id": "IGmw_RJsZXqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Operations"
      ],
      "metadata": {
        "id": "QDj4g3Wbeepf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like with `numpy`, `pandas` provides a number of methods and functions you can use to get the most use out of your `Series` and `DataFrames`."
      ],
      "metadata": {
        "id": "5NeaGFLJeges"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the min, max, mean\n",
        "print(f\"Min: {df['Age'].min()}\")\n",
        "print(f\"Mean: {df['Age'].mean()}\")\n",
        "print(f\"Max: {df['Age'].max()}\")"
      ],
      "metadata": {
        "id": "k2yT-9mMhqYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the positional index of the min and max values\n",
        "print(f\"Argmin: {df['Age'].argmin()}\")\n",
        "print(f\"Argmax: {df['Age'].argmax()}\")"
      ],
      "metadata": {
        "id": "MwhME3OWiOU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting by age, then by HR value\n",
        "df.sort_values(by=['Age', 'HR'], inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "LyabQ6D4hIl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the frequency of each value in a Series\n",
        "df['Sex'].value_counts()"
      ],
      "metadata": {
        "id": "N5Z5eJykhn8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pandas` also has a helpful method called `.describe()` that will compute all of the summary statistics for every column of your `DataFrame` and put those results in a new `DataFrame`."
      ],
      "metadata": {
        "id": "C07hsx0sgtO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "Z-2EdHC5gPJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with `numpy` arrays, you can perform element-wise operations on columns."
      ],
      "metadata": {
        "id": "BmO1F2ARsUgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age in Days'] = df['Age'] * 365\n",
        "df"
      ],
      "metadata": {
        "id": "9POFiqqGsl5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some operations may require you to perform element-wise operations that cannot be written for an entire `Series`. Another option for this situations is to use the `apply()` function, which applies a specified function to each element along a `Series` or an axis of a `DataFrame`. A few examples are shown below:"
      ],
      "metadata": {
        "id": "pEQp7S3usxnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a regular function on a Series\n",
        "def sex_to_text(sex):\n",
        "    if sex == 0:\n",
        "        return 'Male'\n",
        "    elif sex == 1:\n",
        "        return 'Female'\n",
        "    else:\n",
        "        return 'Non-binary'\n",
        "\n",
        "df['Sex Label'] = df['Sex'].apply(sex_to_text)\n",
        "df"
      ],
      "metadata": {
        "id": "rhxqQ5OqtBFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a lambda function\n",
        "df['HR Check'] = df['HR'].apply(lambda hr: hr > 60 and hr < 100)\n",
        "df"
      ],
      "metadata": {
        "id": "EDGXXh6qxS21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a lambda function on multiple columns\n",
        "def check_bp(sbp, dbp):\n",
        "    return (sbp < 130) and (dbp < 85)\n",
        "\n",
        "df['BP Check'] = df.apply(lambda row: check_bp(row.SBP, row.DBP), axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "dMhJIhcBucIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}